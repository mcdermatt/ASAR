{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#need to have these two lines to work on my ancient 1060 3gb\n",
    "#  https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# for device in physical_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 4*1024\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "\n",
    "from utils import *\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "import os\n",
    "\n",
    "#comment out for laptop use\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "\n",
    "import pykitti\n",
    "# from numba import cuda #gpu library used to clear gpu memory after each trial\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICET3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ICET3D import ICET3D\n",
    "\n",
    "# settings.embedWindow(backend='ipyvtk', verbose = True)\n",
    "\n",
    "# plt = Plotter(N=1, axes=1, bg = (0.1,0.1,0.1), bg2 = (0.3,0.3,0.3),  interactive=True)\n",
    "plt = Plotter(N = 1, axes = 4, bg = (1, 1, 1), interactive = True)\n",
    "\n",
    "\n",
    "##KITTI------\n",
    "# # basedir = 'C:/kitti/'\n",
    "# # date = '2011_09_26'\n",
    "# # drive = '0005'\n",
    "# basedir = '/media/derm/06EF-127D3/KITTI'\n",
    "# date = '2011_09_26'\n",
    "# # urban dataset used in 3D-ICET paper \n",
    "# drive = '0005' #life in the big city\n",
    "\n",
    "# # scan = 118\n",
    "# scan = 37\n",
    "# dataset = pykitti.raw(basedir, date, drive)\n",
    "# velo1 = dataset.get_velo(scan) # Each scan is a Nx4 array of [x,y,z,reflectance]\n",
    "# cloud1 = velo1[:,:3]\n",
    "# cloud1_tensor = tf.convert_to_tensor(cloud1, np.float32)\n",
    "# velo2 = dataset.get_velo(scan+1) # Each scan is a Nx4 array of [x,y,z,reflectance]\n",
    "# cloud2 = velo2[:,:3]\n",
    "# cloud2_tensor = tf.convert_to_tensor(cloud2, np.float32)\n",
    "# print(tf.shape(cloud1_tensor))\n",
    "\n",
    "## Custom point clouds generated for spherical paper #was 2,3\n",
    "cloud1 = np.loadtxt(\"../v3/spherical_paper/MC_trajectories/scene1_scan10.txt\", dtype = float)\n",
    "cloud2 = np.loadtxt(\"../v3/spherical_paper/MC_trajectories/scene1_scan12.txt\", dtype = float)\n",
    "cloud1_tensor = tf.convert_to_tensor(cloud1, np.float32)\n",
    "cloud2_tensor = tf.convert_to_tensor(cloud2, np.float32)\n",
    "\n",
    "nc = 2\n",
    "mnp = 10\n",
    "npts = 100000\n",
    "D = True #draw sim\n",
    "DG = False #draw grid\n",
    "DE = True #draw ellipsoids\n",
    "DC = False #draw correspondences\n",
    "TD = False #use test dataset\n",
    "FG = False #fast gaussian (turn off for fine voxels)\n",
    "CM = \"voxel\" #correspondence method, \"voxel\" or \"NN\"\n",
    "\n",
    "xHat0 = tf.constant([1.0, 0., 0., 0,0,0])\n",
    "\n",
    "start = time.time()\n",
    "# #use whole point set\n",
    "#---------------------------------------------------------------------------------\n",
    "f = tf.constant([100,20,6]) #fidelity in x, y, z\n",
    "lim = tf.constant([-100.,100.,-20.,20.,-3.,3.]) #needs to encompass every point\n",
    "# f = tf.constant([20,20,2])\n",
    "# lim = tf.constant([-100.,100.,-100.,100.,-10.,10.]) #needs to encompass every point\n",
    "Q, x_hist = ICET3D(cloud1_tensor, cloud2_tensor, plt, bounds = lim, \n",
    "           fid = f, num_cycles = nc , min_num_pts = mnp, draw = D, draw_grid = DG, \n",
    "           draw_ell = DE, draw_corr = DC, FG=FG, xHat0 = xHat0)\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "#just consider small section of image where there are easily identifiable features:\n",
    "# #----------------------------------------------------------------------------------\n",
    "# limtest = tf.constant([-20.,0.,-20.,0.,-1.5,1.5])\n",
    "# # f = tf.constant([35,35,35])\n",
    "# # f = tf.constant([17,17,17])\n",
    "# # f = tf.constant([21,21,21])\n",
    "# # f = tf.constant([100,100,4])\n",
    "# # cloud1_tensor = tf.squeeze(tf.gather(cloud1_tensor, tf.where( (cloud1_tensor[:,0] > limtest[0]))))\t#only works one cond at a time\n",
    "\n",
    "# Q, x_hist, actual = ICET3D(cloud1_tensor, cloud2_tensor, plt, bounds = limtest, \n",
    "#            fid = f, num_cycles = nc , min_num_pts = mnp, draw = D, draw_grid = DG,\n",
    "#            draw_ell = DE, draw_corr = DC, test_dataset = TD, CM = CM)\n",
    "# #----------------------------------------------------------------------------------\n",
    "#NOTE: Out of Memory Error comes from too high fidelity/ pts in cloud tensor --> 100x100x2x120,000 > 2gb\n",
    "\n",
    "print(\"took\", time.time() - start, \"seconds total\")\n",
    "\n",
    "# print(tf.sqrt(tf.math.abs(Q)))\n",
    "ans = np.sqrt(abs(Q.numpy()))\n",
    "print(\"\\n predicted solution standard deviation of error: \\n\",ans[0,0], ans[1,1], ans[2,2], ans[3,3], ans[4,4], ans[5,5])\n",
    "# print(\"\\n Q \\n\", Q)\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error\n",
    "# x_hist[-1] + tf.constant([1., 3., 2., -0.12, -0.1, -0.2]) #T intersection\n",
    "# x_hist[-1] + tf.constant([0., 1., 0., 0., 0., -0.1])      # simple wall\n",
    "print(x_hist[-1] + actual)\n",
    "\n",
    "# randy = tf.constant([tf.random.normal([1]).numpy(), tf.random.normal([1]).numpy(), \n",
    "#                   tf.random.normal([1]).numpy(), 0.03*tf.random.normal([1]).numpy(), \n",
    "#                   0.03*tf.random.normal([1]).numpy(), 0.1*tf.random.normal([1]).numpy()])\n",
    "# randy = tf.squeeze(randy)\n",
    "# print(randy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display progression of solution values\n",
    "from matplotlib import pyplot as plt\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot()\n",
    "ax1.set_xlabel(\"iteration\")\n",
    "ax1.set_ylabel(\"estimated rotation (rad)\")\n",
    "\n",
    "# ax1.plot(x_hist.numpy())\n",
    "# ax1.legend(['x','y','z','phi','theta','psi'])\n",
    "ax1.plot(x_hist[:,3:].numpy())\n",
    "ax1.legend(['phi','theta','psi'])\n",
    "\n",
    "# ax1.plot(np.linspace(1,15,15), 0.02*np.ones(15), 'g--')\n",
    "# ax1.plot(np.linspace(1,15,15), 0.03*np.ones(15), 'r--')\n",
    "# ax1.plot(np.linspace(1,15,15), 0.01*np.ones(15), 'b--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot()\n",
    "ax2.set_xlabel(\"iteration\")\n",
    "ax2.set_ylabel(\"estimated translation (cm)\")\n",
    "\n",
    "# ax1.plot(x_hist.numpy())\n",
    "# ax1.legend(['x','y','z','phi','theta','psi'])\n",
    "ax2.plot(x_hist[:,:3].numpy())\n",
    "ax2.legend(['x','y','z'])\n",
    "\n",
    "# ax2.plot(np.linspace(1,15,15), -.1*np.ones(15), 'g--')\n",
    "# ax2.plot(np.linspace(1,15,15), -3*np.ones(15), 'r--')\n",
    "# ax2.plot(np.linspace(1,15,15), -.2*np.ones(15), 'b--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICET Monte-Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ICET3D import ICET3D\n",
    "plt = Plotter(N=1, axes=1, bg = (0.1,0.1,0.1), bg2 = (0.3,0.3,0.3),  interactive=True)\n",
    "\n",
    "cloud1_tensor = None\n",
    "cloud2_tensor = None\n",
    "nc = 10\n",
    "mnp = 50\n",
    "npts = 100000\n",
    "D = False #draw sim\n",
    "DG = False #draw grid\n",
    "DE = True #draw ellipsoids\n",
    "DC = True #draw correspondences\n",
    "TD = False #use test dataset\n",
    "CM = \"voxel\" #correspondence method, \"voxel\" or \"NN\"\n",
    "epochs = 30\n",
    "\n",
    "# total_error = tf.zeros([1,6])\n",
    "Q_total = tf.zeros([6,6])\n",
    "errors = tf.zeros([1,6])\n",
    "\n",
    "###old\n",
    "# limtest = tf.constant([-20.,0.,-20.,0.,-1.5,1.5])\n",
    "# # f = tf.constant([35,35,35])\n",
    "# f = tf.constant([17,17,17])\n",
    "# # f = tf.constant([20,20,20])\n",
    "\n",
    "#for comparison w/ matlab pc data\n",
    "limtest = tf.constant([-60., 60., -60., 60., -3.,3.])\n",
    "f = tf.constant([40,40,2])\n",
    "\n",
    "## load custom point cloud geneated in matlab------------------------------------------\n",
    "cloud1 = np.loadtxt(r\"C:\\Users\\Derm\\vaRLnt\\v3\\scene1_scan1.txt\", dtype = float)\n",
    "cloud2 = np.loadtxt(r\"C:\\Users\\Derm\\vaRLnt\\v3\\scene1_scan2.txt\", dtype = float)\n",
    "cloud1 = cloud1[cloud1[:,2] > -1.55] #ignore ground plane\n",
    "cloud2 = cloud2[cloud2[:,2] > -1.55] #ignore ground plane\n",
    "## ------------------------------------------------------------------------------------\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"\\n ~~~~~~~~~ Epoch \", i, \" ~~~~~~~~~~~~~ \\n\")    \n",
    "    #add noise (if not generated when point clouds were created)\n",
    "    c1 = cloud1 + 0.01*np.random.randn(np.shape(cloud1)[0], 3)\n",
    "    c2 = cloud2 + 0.01*np.random.randn(np.shape(cloud2)[0], 3)\n",
    "    \n",
    "    cloud1_tensor = tf.convert_to_tensor(c1, dtype = tf.float32)\n",
    "    cloud2_tensor = tf.convert_to_tensor(c2, dtype = tf.float32)\n",
    "    \n",
    "    actual = tf.constant([-0.5, 0., 0., 0., 0., -0.035])\n",
    "    \n",
    "    Q, x_hist = ICET3D(cloud1_tensor, cloud2_tensor, plt, bounds = limtest, \n",
    "               fid = f, num_cycles = nc , min_num_pts = mnp, draw = D, draw_grid = DG,\n",
    "               draw_ell = DE, draw_corr = DC, test_dataset = TD, CM = CM)\n",
    "\n",
    "#     error_i = x_hist[-1] + tf.constant([1., 3., 2., -0.12, -0.1, -0.2]) #3D\n",
    "#     error_i = x_hist[-1] + tf.constant([1., 4., 2., -0., 0.0, -0.1]) # 2D\n",
    "#     error_i = x_hist[-1] +  tf.constant([0., 1., 0., 0., 0., 0.1]) #1D\n",
    "    error_i = x_hist[-1] - actual\n",
    "    \n",
    "    # print(\"\\n error_i \\n\", error_i)\n",
    "    Q_total += Q\n",
    "#     total_error += error_i\n",
    "    errors = tf.concat((errors, error_i[None,:]), axis = 0)\n",
    "errors = errors[1:] #get rid of placeholder value\n",
    "    \n",
    "# avg_error = total_error/epochs\n",
    "# print(\"\\n average solution errors \\n\", avg_error)\n",
    "print(\"\\n actual error std: \\n\", tf.math.reduce_std(errors, axis = 0))\n",
    "\n",
    "ans = np.sqrt(abs((Q_total/epochs).numpy()))\n",
    "print(\"\\n predicted solution standard deviation of error: \\n\", ans[0,0], ans[1,1], ans[2,2], ans[3,3], ans[4,4], ans[5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n mean estimates \\n \", tf.math.reduce_mean(x_hist[1:], axis = 0))\n",
    "print(\"\\n actual mean errors \\n\", tf.math.reduce_mean(errors, axis = 0))\n",
    "\n",
    "print(\"\\n actual error std: \\n\", tf.math.reduce_std(errors, axis = 0))\n",
    "\n",
    "ans = np.sqrt(abs((Q_total/epochs).numpy()))\n",
    "print(\"\\n predicted solution standard deviation of error: \\n\", ans[0,0], ans[1,1], ans[2,2], ans[3,3], ans[4,4], ans[5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get ground truth movement from pykitti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykitti\n",
    "from metpy.calc import lat_lon_grid_deltas\n",
    "import pyproj #package for cartographic projections\n",
    "\n",
    "id1 = 0\n",
    "id2 = 1\n",
    "basedir = 'C:/kitti/'\n",
    "date = '2011_09_26'\n",
    "# drive = '0005'\n",
    "drive = '0009'\n",
    "dataset = pykitti.raw(basedir, date, drive)\n",
    "velo1 = dataset.get_velo(0)\n",
    "poses0 = dataset.oxts[id1] #<- ID of 1st scan\n",
    "poses1 = dataset.oxts[id2] #<- ID of 2nd scan\n",
    "print(poses0)\n",
    "\n",
    "#get transformations in frame of OXTS inertial(?) sensor\n",
    "lat0 = poses0.packet.lat\n",
    "lon0 = poses0.packet.lon\n",
    "alt0 = poses0.packet.alt\n",
    "lat1 = poses1.packet.lat\n",
    "lon1 = poses1.packet.lon\n",
    "alt1 = poses1.packet.alt\n",
    "# print(lat0, lon0, alt0)\n",
    "\n",
    "#convert to dx/ dy in meters\n",
    "#from metpy ------------------------------------------------\n",
    "#these are \"pint\" objects which hold on to units\n",
    "dx_oxts, dy_oxts = lat_lon_grid_deltas(np.array([lon0,lon1]), np.array([lat0, lat1]))\n",
    "# print(dx_oxts, dy_oxts) \n",
    "dx_oxts = dx_oxts[0,0].magnitude\n",
    "dy_oxts = dy_oxts[0,0].magnitude\n",
    "# #---------------------------------------------------------\n",
    "# #manual dxdy calculations (not sure if correct) ----------\n",
    "# dx_oxts = (lon0-lon1)*40000*np.cos((lat0+lat1)*np.pi/360)/360 * 1000 \n",
    "# dy_oxts = (lat0-lat1)*40000/360 * 1000\n",
    "# print(dx_oxts, dy_oxts)\n",
    "# #---------------------------------------------------------\n",
    "dz_oxts = (alt0-alt1)\n",
    "droll_oxts = (poses0.packet.roll - poses1.packet.roll)\n",
    "dpitch_oxts = (poses0.packet.pitch - poses1.packet.pitch)\n",
    "dyaw_oxts = (poses0.packet.yaw - poses1.packet.yaw)\n",
    "# print(dx_oxts,dy_oxts,dz_oxts,droll_oxts, dpitch_oxts, dyaw_oxts)\n",
    "\n",
    "#get rotation matrix to bring things into frame of lidar unit\n",
    "rot = poses0.T_w_imu[:3,:3] #ignore reflectance componenet\n",
    "# print(rot)\n",
    "dxyz_oxts = np.array([[dx_oxts, dy_oxts, dz_oxts]])\n",
    "dxyz_lidar = dxyz_oxts.dot(rot)\n",
    "# print(dxyz_lidar, droll_oxts, dpitch_oxts, dyaw_oxts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.oxts[0]) #look at 100th datapoint\n",
    "print(\"\\n\",dataset.oxts[0].packet.vf/10) #forard velocity at time 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# print(type(dataset.timestamps[50]))\n",
    "# print(dataset.timestamps[50].time())\n",
    "\n",
    "t = 0\n",
    "for n in range(150):\n",
    "#     print((dataset.timestamps[n+1] - dataset.timestamps[n]).microseconds/(10e5))\n",
    "    t += (dataset.timestamps[n+1] - dataset.timestamps[n]).microseconds/(10e5)\n",
    "\n",
    "print(t/150)\n",
    "\n",
    "# dif = datetime.timedelta()\n",
    "# print(datetime.timedelta(dataset.timestamps[50],dataset.timestamps[51]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results of ICET estimates on KITTI lidar point clouds vs GPS/INS baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "font = {'fontname':'Times New Roman'}\n",
    "\n",
    "## usiung raw data -----------------------\n",
    "OXTS_baseline = np.loadtxt(\"OXTS_baseline_926_test2.txt\") #best baseline\n",
    "# OXTS_baseline = np.loadtxt(\"OXTS_baseline_926_0005.txt\") #best baseline\n",
    "# OXTS_baseline = np.loadtxt(\"OXTS_baseline_926_0005_test2.txt\") #best baseline\n",
    "#compenate for slight difference in sampling rate for absolute angle measurements\n",
    "# OXTS_baseline[:,3:] = OXTS_baseline[:,3:]/0.1*0.10327\n",
    "print(OXTS_baseline[118,0])\n",
    "\n",
    "ICET_estimates = np.loadtxt(\"ICET_estimates_926_test3.txt\") #[20,20,2]\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_926_test4.txt\") #[50,50,2]\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_926_0005.txt\")\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_926_0005_test2.txt\")\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_926_0005_test3.txt\")\n",
    "\n",
    "# # ICET_estimates = np.loadtxt(\"ICET_estimates_926_0018.txt\")\n",
    "# # # ICET_estimates[:,1] = -ICET_estimates[:,1]\n",
    "# # OXTS_baseline = np.loadtxt(\"OXTS_baseline_926_0018.txt\")\n",
    "# # # OXTS_baseline[:,1] = -OXTS_baseline[:,1]\n",
    "#----------------------------------------\n",
    "\n",
    "# OXTS_baseline = np.loadtxt(\"true_pose_00.txt\")\n",
    "# # print(OXTS_baseline[37,0])\n",
    "# ICET_estimates = np.loadtxt(\"ICET_benchmark_00.txt\")\n",
    "\n",
    "n = 3\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "#fix sign errors...\n",
    "ICET_estimates[:,1] = -ICET_estimates[:,1]\n",
    "ICET_estimates[:,3:] = -ICET_estimates[:,3:]\n",
    "style1 = 'b-'\n",
    "style2 = 'r-'\n",
    "\n",
    "fig, ax = plt.subplots(3,2, constrained_layout = True)\n",
    "ax[0,0].plot(ICET_estimates_no_ground[:,0], style1, label = 'ICET')\n",
    "ax[0,0].plot(OXTS_baseline[:,0], style2, label = 'GPS/INS Baseline')\n",
    "# ax[0,0].plot(np.arange(n//2, np.shape(ICET_estimates)[0] - n//2 ), moving_avg(OXTS_baseline[:,0], n),  style2, label = 'GPS/INS Baseline')\n",
    "# ax[0,0].plot(np.arange(n//2, np.shape(ICET_estimates)[0] - n//2 ), moving_avg(ICET_estimates[:,0], n),  style1, label = 'GPS/INS Baseline')\n",
    "ax[0,0].set_title(\"change in x per frame\", **font)\n",
    "ax[0,0].set_ylabel(\"dx (m)\", **font)\n",
    "ax[0,0].legend(loc = 'upper left')\n",
    "ax[0,0].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[1,0].plot(ICET_estimates[:,1], style1, lw = 1)\n",
    "ax[1,0].plot(-OXTS_baseline[:,1], style2, lw = 1)\n",
    "# ax[1,0].plot(np.arange(n//2, np.shape(ICET_estimates)[0] - n//2 ), moving_avg(OXTS_baseline[:,1], n),  style2, lw = 1)\n",
    "ax[1,0].set_title(\"change in y per frame\", **font)\n",
    "ax[1,0].set_ylabel(\"dy (m)\", **font)\n",
    "ax[1,0].set_xlabel(\"frame\", **font)\n",
    "\n",
    "\n",
    "ax[2,0].plot(ICET_estimates[:,2], style1, lw = 1)\n",
    "ax[2,0].plot(OXTS_baseline[:,2], style2, lw = 1)\n",
    "# ax[2,0].plot(np.arange(n//2, np.shape(ICET_estimates)[0] - n//2 ), moving_avg(OXTS_baseline[:,2], n),  style2, lw = 1)\n",
    "ax[2,0].set_title(\"change in z per frame\", **font)\n",
    "ax[2,0].set_ylabel(\"dz (m)\", **font)\n",
    "ax[2,0].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[0,1].plot(ICET_estimates[:,3], style1, lw = 1)\n",
    "ax[0,1].plot(OXTS_baseline[:,3], style2, lw = 1)\n",
    "ax[0,1].set_title(\"change in roll per frame\", **font)\n",
    "ax[0,1].set_ylabel(\"droll (rad)\", **font)\n",
    "ax[0,1].set_xlabel(\"frame\", **font)\n",
    "\n",
    "\n",
    "ax[1,1].plot(ICET_estimates[:,4], style1, lw = 1)\n",
    "ax[1,1].plot(OXTS_baseline[:,4], style2, lw = 1)\n",
    "ax[1,1].set_title(\"change in pitch per frame\", **font)\n",
    "ax[1,1].set_ylabel(\"dpitch (rad)\", **font)\n",
    "ax[1,1].set_xlabel(\"frame\", **font)\n",
    "\n",
    "\n",
    "ax[2,1].plot(ICET_estimates[:,5], style1, lw = 1)\n",
    "ax[2,1].plot(OXTS_baseline[:,5], style2, lw = 1)\n",
    "ax[2,1].set_title(\"change in yaw per frame\", **font)\n",
    "ax[2,1].set_ylabel(\"dyaw (rad)\", **font)\n",
    "ax[2,1].set_xlabel(\"frame\", **font)\n",
    "\n",
    "# fig.tight_layout(h_pad = 0.1)\n",
    "plt.show()\n",
    "\n",
    "# fname = \"assets/005.png\"\n",
    "# plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean difference between OXTS baseline and ICET estimates\n",
    "term = 0 #0 == forward movement, \n",
    "# term = 5 #5 = yaw\n",
    "\n",
    "ICET_estimates_with_ground = np.loadtxt(\"ICET_estimates_926_test3.txt\")\n",
    "OXTS_baseline_with_ground = np.loadtxt(\"OXTS_baseline_926_test2.txt\")\n",
    "OXTS_baseline_with_ground[:,3:] = -OXTS_baseline_with_ground[:,3:] #/0.1*0.10327\n",
    "d1 = (OXTS_baseline_with_ground[:,term] - ICET_estimates_with_ground[:,term])/OXTS_baseline_with_ground[:,term]\n",
    "\n",
    "OXTS_baseline_no_ground = np.loadtxt(\"OXTS_baseline_926_0005.txt\")\n",
    "# ICET_estimates_no_ground = np.loadtxt(\"ICET_estimates_926_0005.txt\")\n",
    "ICET_estimates_no_ground = np.loadtxt(\"ICET_estimates_926_0005_test2.txt\")\n",
    "# ICET_estimates_no_ground = np.loadtxt(\"ICET_estimates_926_0005_test3.txt\")\n",
    "OXTS_baseline_no_ground[:,3:] = -OXTS_baseline_no_ground[:,3:]*0.10327142/0.10\n",
    "d2 = (OXTS_baseline_no_ground[:,term] - ICET_estimates_no_ground[:,term])/OXTS_baseline_no_ground[:,term]\n",
    "\n",
    "# P = 90\n",
    "# print(np.mean(d[d < np.percentile(d,P)])*100) #mean error of bottom P% of scan pairs\n",
    "\n",
    "## plot on seperate graphs--------\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0].hist(d1, bins = np.linspace(-0.20,0.20,30))\n",
    "# ax[0].set_ylim([0,45])\n",
    "ax[0].set_ylabel(\"frequency\")\n",
    "# ax[0].set_xlabel(\"Error in forward translation estimate (m)\")\n",
    "ax[0].set_title(\"Difference between GPS/INS and ICET on raw dataset 005\")\n",
    "\n",
    "ax[1].hist(d2, bins = np.linspace(-0.20,0.20,30))\n",
    "# ax[1].set_ylim([0,45])\n",
    "ax[1].set_ylabel(\"frequency\")\n",
    "ax[1].set_xlabel(\"Error in forward translation estimate (m)\")\n",
    "ax[1].set_title(\"Difference between GPS/INS and ICET on dataset 005 without ground plane\")\n",
    "## ---------\n",
    "\n",
    "## single graph\n",
    "# fig, ax = plt.subplots(1)\n",
    "# bins = np.linspace(-0.30,0.30,20)\n",
    "# ax.hist([d1, d2], bins, density=True, histtype='bar')\n",
    "# ax.legend(prop={'size': 10})\n",
    "# ax.set_title('bars with legend')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot error between ICET and absolute position\n",
    "plt.rc('font',family='Times New Roman')\n",
    "fig3, ax3 = plt.subplots(1,1)\n",
    "\n",
    "# #load data on estimated standard deviations of each component\n",
    "# # ICET_pred_stds = np.loadtxt(\"ICET_pred_stds_926_test3.txt\")\n",
    "# # ICET_pred_stds = np.loadtxt(\"ICET_pred_stds_926_test4.txt\")\n",
    "ICET_pred_stds = np.loadtxt(\"ICET_pred_stds_926_0005_test2.txt\")\n",
    "\n",
    "# ICET_pred_stds = np.loadtxt(\"ICET_pred_stds_00.txt\")\n",
    "\n",
    "#which component to look at\n",
    "c = 5 #yaw\n",
    "# c = 0 # x (forward movement)\n",
    "# window = 5\n",
    "\n",
    "#raw - not that useful since GPS/INS is only an ESTIMATE, errors are +/- 2cm\n",
    "diffx_with_ground = OXTS_baseline_with_ground[:,c] - ICET_estimates_with_ground[:,c]\n",
    "diffx_no_ground = OXTS_baseline_no_ground[:,c] - ICET_estimates_no_ground[:,c]\n",
    "    \n",
    "#flip sign when looking at yaw\n",
    "if c ==5:\n",
    "    diffx_with_ground = -diffx_with_ground  \n",
    "    diffx_no_ground = -diffx_no_ground\n",
    "    \n",
    "cum_err = np.zeros(np.shape(ICET_pred_stds))\n",
    "cum_diffx_with_ground = np.zeros(np.shape(diffx_no_ground))\n",
    "cum_diffx_no_ground = np.zeros(np.shape(diffx_with_ground))\n",
    "\n",
    "for i in range(np.shape(ICET_pred_stds)[0]):\n",
    "    cum_err[i,:] = np.sum(ICET_pred_stds[:i,:]**2, axis = 0)\n",
    "    #add in baseline OXTS 1-sigma errors\n",
    "    cum_err[i,:] += np.sqrt(2)*np.array([0.05,0.05,0.1,0.0005,0.0005,0.001])**2\n",
    "    cum_err[i,:] = np.sqrt(cum_err[i,:]) \n",
    "    \n",
    "for j in range(np.shape(diffx_no_ground)[0]):\n",
    "    cum_diffx_with_ground[j] = np.sum(diffx_with_ground[:j]) \n",
    "    cum_diffx_no_ground[j] = np.sum(diffx_no_ground[:j]) \n",
    "    \n",
    " # ----------------------\n",
    "\n",
    "\n",
    "# #old (error for each individual timestep)------------------------\n",
    "# ax3.plot(diffx_with_ground, label = 'GPS/INS - ICET')\n",
    "# # ax3.plot(diffx_no_ground, label = 'GPS/INS - ICET (no ground plane)')\n",
    "# ax3.fill_between(np.linspace(0,150,np.shape(ICET_pred_stds)[0]), -2*ICET_pred_stds[:,c], 2*ICET_pred_stds[:,c], \n",
    "#                  color = (0,0,1,0.2), label = 'ICET Predicted 2σ Error Bounds')\n",
    "# #-------------------------------------------------------------------\n",
    "\n",
    "# #new (accumulated differences in error)--------------------------\n",
    "# ax3.plot(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), cum_diffx_with_ground, label = 'GPS/INS - ICET')\n",
    "ax3.plot(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), cum_diffx_no_ground, label = 'GPS/INS - ICET')\n",
    "ax3.fill_between(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), -2*cum_err[:,c], 2*cum_err[:,c], \n",
    "                 color = (0,0,1,0.2), label = 'Predicted 2σ Error Bounds')\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "ax3.legend(loc = 'lower left')\n",
    "ax3.set_title(\"Predicted vs Actual Error in yaw\")\n",
    "ax3.set_xlabel(\"time (s)\", **font)\n",
    "ax3.set_ylabel(\"GPS/INS Baseline yaw - Odometry Estimate yaw (rad)\", **font)\n",
    "# ax3.set_ylim([-0.07,0.07])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ICET_pred_stds[0,:])\n",
    "test = tf.zeros([6,1])\n",
    "print(test)\n",
    "# print(np.sqrt(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO- get r^2 value of relationship between Q[i,0] and actual error of estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot xy total positions\n",
    "fig2, ax2 = plt.subplots(1,1)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_926_0018.txt\")\n",
    "# OXTS_baseline = np.loadtxt(\"OXTS_baseline_926_0018.txt\")\n",
    "\n",
    "OXTS_total = np.zeros(np.shape(OXTS_baseline))\n",
    "ICET_total = np.zeros(np.shape(ICET_estimates))\n",
    "\n",
    "for i in range(np.shape(OXTS_baseline)[0]):\n",
    "    OXTS_total[i] = np.sum(OXTS_baseline[:i], axis = 0)\n",
    "    ICET_total[i] = np.sum(ICET_estimates[:i], axis = 0)\n",
    "# ax2.plot(ICET_total[:,5])\n",
    "# ax2.plot(OXTS_total[:,5])\n",
    "\n",
    "#need to combine TOTAL heading with incremental changes in x and y (really should be ignoring y...)\n",
    "ICET_traj = np.zeros([np.shape(ICET_estimates)[0], 2])\n",
    "OXTS_traj = np.zeros([np.shape(ICET_estimates)[0], 2])\n",
    "for j in range(1, np.shape(ICET_estimates)[0]):\n",
    "    #x[i] = x[i-1] + step_dist*cos(heading)\n",
    "    ICET_traj[j,0] = ICET_traj[j-1,0] + ICET_estimates[j,0]*np.cos(ICET_total[j,5]) #+ ICET_estimates[j,1]*np.sin(ICET_total[j,5])\n",
    "    OXTS_traj[j,0] = OXTS_traj[j-1,0] + OXTS_baseline[j,0]*np.cos(OXTS_total[j,5]) #+ OXTS_baseline[j,1]*np.sin(OXTS_total[j,5])\n",
    "    #y[i] = y[i-1] + step_dist*sin(heading)\n",
    "    ICET_traj[j,1] = ICET_traj[j-1,1] + ICET_estimates[j,0]*np.sin(ICET_total[j,5]) #+ ICET_estimates[j,1] * np.cos(ICET_total[j,5])\n",
    "    OXTS_traj[j,1] = OXTS_traj[j-1,1] + OXTS_baseline[j,0]*np.sin(OXTS_total[j,5]) #+ OXTS_baseline[j,1] * np.cos(OXTS_total[j,5])\n",
    "\n",
    "    \n",
    "ax2.plot(-ICET_traj[:,0], ICET_traj[:,1], 'b-', label = \"ICET Lidar Odometry\")\n",
    "ax2.plot(-OXTS_traj[:,0], OXTS_traj[:,1], 'r-', label = 'GPS/INS baseline')\n",
    "ax2.set_xlabel('North (m)', **font)\n",
    "ax2.set_ylabel('East (m)', **font)\n",
    "ax2.legend(loc = 'upper left')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get ground truth poses from text file??\n",
    "path = r\"C:\\kitti\\2011_09_26\\2011_09_26_drive_0005_sync\\poses\\05.txt\"\n",
    "gt = np.loadtxt(path)\n",
    "print(gt[0, :6])\n",
    "print(gt[2, :6])\n",
    "print(np.shape(gt))\n",
    "# print(gt[-1, :6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse XML file containing ground truth KITTI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Reading the data inside the xml\n",
    "# file to a variable under the name\n",
    "# data\n",
    "file = r'C:\\kitti\\2011_09_26\\2011_09_26_drive_0005_sync\\tracklet_labels.xml'\n",
    "with open(file, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# the beautifulsoup parser, storing\n",
    "# the returned object\n",
    "Bs_data = BeautifulSoup(data, \"xml\")\n",
    " \n",
    "# Finding all instances of tag\n",
    "# `tx`\n",
    "x = Bs_data.find_all('tx')\n",
    "# print(x[150])\n",
    "print(x)\n",
    "\n",
    "# Using find() to extract attributes\n",
    "# of the first instance of the tag\n",
    "b_name = Bs_data.find('child', {'name':'Frank'})\n",
    " \n",
    "print(b_name)\n",
    " \n",
    "# Extracting the data stored in a\n",
    "# specific attribute of the\n",
    "# `child` tag\n",
    "value = b_name.get('test')\n",
    " \n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.reduce_mean(errors, axis = 0))\n",
    "\n",
    "#save to external file---\n",
    "# fn = \"2d_errors.txt\"\n",
    "# np.savetxt(fn, errors.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.linspace(1.,10.,10)\n",
    "print(a)\n",
    "mask = tf.cast(tf.math.less(a, 4), tf.float32)\n",
    "print(mask)\n",
    "a = a*mask\n",
    "print(a)\n",
    "print(tf.math.reduce_max(tf.constant([1.,2.,9.])))\n",
    "print(tf.math.abs([-1.,2.,3.,-0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOAL: Keep distribution axis for neighboring ellipses on the same surface pointing in the same direction\n",
    "#      if I don't do this, it will mess up my z-error??\n",
    "\n",
    "# if largest absolute value element in 3rd column of U[i] is less than 0, multiply U[i] by -1\n",
    "U = tf.constant([[[1., 2.,  4.],\n",
    "                  [0., 3., -1.],\n",
    "                  [1., 3., -6.]],\n",
    "                  \n",
    "                 [[3., 2.,  1.],\n",
    "                  [0., 2.,  -9.],\n",
    "                  [9., 1.,  6.]],\n",
    "                \n",
    "                 [[1., 1.,  -1.],\n",
    "                  [1., 1.,  3.],\n",
    "                  [1., 1.,  5.]]])\n",
    "\n",
    "print(U, \"\\n\")\n",
    "# #slow loopy way\n",
    "# print(\"slower way ----------------------------\")\n",
    "# for i in range(tf.shape(U)[0]):\n",
    "#     a = tf.math.reduce_max(tf.abs(U[i,:,2]))\n",
    "#     print(a)\n",
    "#     b = tf.where(tf.equal(tf.abs(U[i,:,2] ), a ))[0,0]\n",
    "#     print(b) \n",
    "#     print(U[i,b,2])\n",
    "#     if U[i,b,2] < 0:\n",
    "#         print(\"gotta reverse U \\n\")\n",
    "\n",
    "#faster vectorized way\n",
    "print(\"\\n faster way ---------------------\")\n",
    "a = tf.math.reduce_max(tf.abs(U[:,:,2]), axis = 1)[:,None]\n",
    "# print(a)\n",
    "b = tf.where( tf.math.equal(tf.abs(U[:,:,2]), a) )\n",
    "# print(b)\n",
    "# print(\"U[:,:,2] \\n\", U[:,:,2])\n",
    "absmax = tf.gather_nd(U[:,:,2], b)\n",
    "print(\"absmax: \\n\", absmax)\n",
    "\n",
    "#test: only look at one comonent of direction of largest ditribution axis\n",
    "# absmax = U[:,0,2]\n",
    "# print(absmax)\n",
    "\n",
    "mask = (-2.*tf.cast(tf.math.less(absmax, 0), tf.float32) + tf.ones(tf.shape(absmax)))[:,None][:,None]\n",
    "print(\"mask: \\n\", mask)\n",
    "\n",
    "# print(\"output: \", mask*U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = tf.tile(tf.eye(3)[None,:,:], (2, 1, 1))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test out multiplying voxel means by either ragged tensor or by zero padded (3x3) tensor\n",
    "# make sure results are the same\n",
    "#BAD WAY...\n",
    "\n",
    "L1 = tf.constant([[[1., 0., 0.],\n",
    "                 [0., 1., 0.],\n",
    "                 [0., 0., 1.]],\n",
    "\n",
    "                [[0., 0., 0.],\n",
    "                 [0., 0., 0.],\n",
    "                 [0., 0., 0.]],\n",
    "                \n",
    "                [[1., 0., 0.],\n",
    "                 [0., 1., 0.],\n",
    "                 [0., 0., 0.]]])\n",
    "\n",
    "values = tf.tile(tf.eye(3), (3,1))\n",
    "useful = tf.constant([0,1,2,6,7]) #useful directions for ICET\n",
    "uv = tf.gather(values, useful)\n",
    "print(\"uv: \\n\", uv)\n",
    "\n",
    "# starts = [0, 3, 4]\n",
    "# L2 = tf.RaggedTensor.from_row_starts(values, starts)\n",
    "\n",
    "#find where first element of each row is equal to 1\n",
    "# w = tf.transpose(tf.where(uv[:,0] == 1))\n",
    "# print(\"w \\n\", w)\n",
    "\n",
    "#find where every first row is NOT 1 so we can add those to row_limits to create placeholder zero tensors\n",
    "everyThird = tf.range(0,tf.shape(values)[0],3, dtype = tf.int64)[None,:]\n",
    "print(\"\\n every third \\n\", everyThird)\n",
    "#find elements of everyThird that are not in <useful>\n",
    "both = tf.sets.intersection(everyThird, tf.cast(useful, tf.int64)[None,:])\n",
    "both = tf.sparse.to_dense(both)\n",
    "print(\"\\n both \\n \", both)\n",
    "placeholers = tf.where(everyThird == both)\n",
    "\n",
    "row_limits = [3,3,5] #temp -> write code to gen this automatically\n",
    "\n",
    "L2 = tf.RaggedTensor.from_row_limits(uv, row_limits)\n",
    "print(\"\\n L2 \\n\", L2.to_tensor())\n",
    "\n",
    "test1 = L1 @ U\n",
    "# print(\"\\n test1 \\n\", test1)\n",
    "\n",
    "# test2 = L2.to_tensor() @ U\n",
    "# print(\"\\n test2 \\n\", test2)\n",
    "# print(L2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Better way...\n",
    "I = tf.tile(tf.eye(3), (4,1))\n",
    "print(\"eye \\n\", I)\n",
    "short = tf.constant([0,1,2,6,7,9]) #useful directions for ICET\n",
    "print(\"\\n useful \\n\", useful)\n",
    "\n",
    "data = tf.ones((tf.shape(short)[0],3))\n",
    "print(\"\\n data \\n\", data)\n",
    "\n",
    "mask = tf.scatter_nd(indices = short[:,None], updates = data, shape = tf.shape(I))\n",
    "print(\"\\n mask \\n\", mask)\n",
    "\n",
    "ans = mask * I\n",
    "ans = tf.reshape(ans, (tf.shape(ans)[0]//3,3,3))\n",
    "print(\"\\n ans \\n\",ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.range(0,tf.shape(values)[0],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof that extended axis truncation == etended axis zeroing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = tf.constant([[1., 0., 0.],\n",
    "                  [0., 1., 0.],\n",
    "                  [0., 0., 0.]])\n",
    "print(\"\\n L1 \\n\", L1)\n",
    "\n",
    "L2 = tf.constant([[1., 0., 0.],\n",
    "                  [0., 1., 0.]])\n",
    "print(\"\\n L2 \\n\", L2)\n",
    "\n",
    "U = R(tf.constant([1.,2.,3.]))\n",
    "\n",
    "sigma = tf.random.uniform((3,3))\n",
    "\n",
    "#fake calculation of R_noise\n",
    "test1 = L1 @ tf.transpose(U) @ sigma @ U @ tf.transpose(L1)\n",
    "print(\"\\n test1 \\n\", test1)\n",
    "\n",
    "test2 = L2 @ tf.transpose(U) @ sigma @ U @ tf.transpose(L2)\n",
    "print(\"\\n test1 \\n\", test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truncating L matrix does the same thing as zeroing out end rows...\n",
    "L3 = tf.constant([[1.,0.,0.],\n",
    "                  [0., 1., 0.]])\n",
    "print(L3 @ U[0])\n",
    "print(L1[2] @ U[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test - multiply a [6,6] tensor with a [N, 6, 3] tensor\n",
    "# A = tf.eye(6)        #[6, 6]\n",
    "# B = tf.ones([2,6,3]) #[2, 6, 3]\n",
    "# print(tf.shape(tf.matmul(A,B))) #[2, 6, 3]\n",
    "\n",
    "C = tf.ones([164, 6,3])\n",
    "D = tf.ones([164,3])[:,:,None]\n",
    "print(tf.shape(tf.matmul(C,D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = tf.ones([2,3,3])\n",
    "b = tf.ones([2,3,3])\n",
    "# print(a, b)\n",
    "print(a * b)\n",
    "print(a @ b)\n",
    "print(tf.matmul(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.random.uniform([10,3])\n",
    "print(t)\n",
    "lims = [tf.constant([1]), tf.constant([2])]\n",
    "rag = tf.RaggedTensor.from_row_splits(t, lims)\n",
    "print(rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subdivide scan using NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## from vedo import *\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import os\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "import pykitti\n",
    "\n",
    "settings.embedWindow(backend='ipyvtk', verbose = True) #was this\n",
    "\n",
    "try:\n",
    "    plt1.closeWindow()\n",
    "    print(\"closed\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt1 = Plotter(N=1, axes=1, bg = (0.1,0.1,0.1), bg2 = (0.3,0.3,0.3),  interactive=True)\n",
    "# settings.useParallelProjection = True #makes view orthographic\n",
    "\n",
    "## uncomment to use VOLPE dataset -----------------------------------------------------\n",
    "# location = 'C:/Users/Derm/2021-03-10-16-43-50_Velodyne-VLP-16-Data_garminSignage.txt'\n",
    "# cloud = np.loadtxt(open(location, \"rb\"), delimiter=\",\")\n",
    "# cloud = cloud[~np.isnan(cloud).any(axis=1)] #remove all rows with NaN elements\n",
    "## ------------------------------------------------------------------------------------\n",
    "\n",
    "## uncomment to use KITTI dataset -----------------------------------------------------\n",
    "basedir = 'C:/kitti/'\n",
    "date = '2011_09_26'\n",
    "drive = '0005'\n",
    "frame_range = range(150, 151, 1)\n",
    "dataset = pykitti.raw(basedir, date, drive)\n",
    "velo1 = dataset.get_velo(0) # Each scan is a Nx4 array of [x,y,z,reflectance]\n",
    "cloud = velo1[:,:3]\n",
    "## ------------------------------------------------------------------------------------\n",
    "\n",
    "# make 2D sinusioal pattern (for debug) ------------------------------------------------\n",
    "# cloud = np.random.randn(10000,3)\n",
    "# cloud[:,0] += -50*cloud[:,1] + np.random.randn()*5\n",
    "# cloud[:,1] += 5\n",
    "# cloud[:,1] = cloud[:,1] * 3 + 10*np.random.randn() + np.sin(cloud[:,1]*5)*10\n",
    "# cloud[:,2] += 0.5*cloud[:,1] - 10\n",
    "##-------------------------------------------------------------------------------------\n",
    "\n",
    "# f =np.array([200,200,40]) #fidelity in x, y, z #takes ~30s on my dsektop\n",
    "# lim = np.array([-50,50,-50,50,-10,10])\n",
    "\n",
    "f =np.array([100,100,2]) #fidelity in x, y, z # < 5s\n",
    "lim = np.array([-100,100,-100,100,-10,10])\n",
    "\n",
    "for _ in range(1):\n",
    "#     cloud_partial = tf.gather(cloud,tf.cast((tf.linspace(1,100000,100)), tf.int32))\n",
    "    cloud_partial = cloud\n",
    "    mus, sigmas, sizes = subdivide_scan(cloud_partial,plt1, bounds = lim, fid = f, draw_grid = False, show_pc = True) \n",
    "\n",
    "# print(\"\\n mus: \\n\", mus)\n",
    "# print(\"\\n sigmas: \\n\", sigmas, np.shape(sigmas))\n",
    "    \n",
    "ViewInteractiveWidget(plt1.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subdivide scan using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## from vedo import *\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import os\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "import pykitti\n",
    "\n",
    "settings.embedWindow(backend='ipyvtk', verbose = True) #was this\n",
    "\n",
    "plt2 = Plotter(N=1, axes=1, bg = (0.1,0.1,0.1), bg2 = (0.3,0.3,0.3),  interactive=True)\n",
    "\n",
    "## uncomment to use KITTI dataset -----------------------------------------------------\n",
    "basedir = 'C:/kitti/'\n",
    "date = '2011_09_26'\n",
    "drive = '0005'\n",
    "frame_range = range(150, 151, 1)\n",
    "dataset = pykitti.raw(basedir, date, drive)\n",
    "velo1 = dataset.get_velo(0) # Each scan is a Nx4 array of [x,y,z,reflectance]\n",
    "cloud = velo1[:,:3]\n",
    "cloud_tensor = tf.convert_to_tensor(cloud, np.float32)\n",
    "# print(tf.shape(cloud))\n",
    "## ------------------------------------------------------------------------------------\n",
    "\n",
    "# # make 2D sinusioal motion (for debug) ------------------------------------------------\n",
    "# cloud = np.random.randn(10000,3)\n",
    "# cloud[:,0] += -50*cloud[:,1] + np.random.randn()*5\n",
    "# cloud[:,1] += 5\n",
    "# cloud[:,1] = cloud[:,1] * 3 + 10*np.random.randn() + np.sin(cloud[:,1]*5)*10\n",
    "# cloud[:,2] += 0.5*cloud[:,1] - 10\n",
    "# #-------------------------------------------------------------------------------------\n",
    "\n",
    "f = tf.constant([100,100,2]) #fidelity in x, y, z # < 5s\n",
    "lim = tf.constant([-100.,100.,-100.,100.,-10.,10.])\n",
    "DRAW = True\n",
    "\n",
    "for _ in range(1):\n",
    "#     cloud_partial = tf.gather(cloud,tf.cast((tf.linspace(1,100000,30)), tf.int32))\n",
    "    cloud_partial = cloud\n",
    "    E = subdivide_scan_tf(cloud_partial, plt2, bounds = lim, fid = f, draw=DRAW, draw_grid = False, show_pc = 1) \n",
    "# print(\"\\n points: \\n\", cloud_partial)\n",
    "\n",
    "# mu = E[0]\n",
    "# print(\"\\n mu: \\n\",mu)\n",
    "\n",
    "# sigma = E[1]\n",
    "# print(\"\\n sigma: \\n\", sigma)\n",
    "\n",
    "# print(tf.transpose(sigma))\n",
    "# print(\"\\n sigma[:,:,1] \\n\", sigma[:,:,0])\n",
    "\n",
    "# sig2 = tf.reshape(tf.transpose(sigma), (tf.shape(sigma)[1] ,3,3))\n",
    "# print(\"reshaped sigma \\n\", sig2)\n",
    "\n",
    "# shapes = E[2]\n",
    "# print(\"\\n shapes: \\n\", shapes)\n",
    "\n",
    "# sigma = E[1]\n",
    "# print(tf.shape(sigma),sigma[:,:,1])\n",
    "\n",
    "# if DRAW:\n",
    "ViewInteractiveWidget(plt2.window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.constant([1.,1.,1.]) #works ------------------------------\n",
    "# print(\"\\n a \\n\", a)\n",
    "\n",
    "# b = tf.random.normal([10,3])\n",
    "# print(\"\\n b \\n\", b)\n",
    "\n",
    "# dist = tf.reduce_sum(tf.math.squared_difference(a,b), axis = 1)\n",
    "# ans = tf.where(dist == tf.math.reduce_min(dist))[0,0]\n",
    "# print(\"\\n ans \\n\", b[ans])\n",
    "## -----------------------------------------------------------------\n",
    "\n",
    "# batch input - not working yet ------------------------------------\n",
    "a = tf.constant([[[1., 1., 1.]],\n",
    "                 [[0., 0., 0.]],\n",
    "                 [[0., 0., 0.]]])  \n",
    "print(\"\\n a \\n\", a)\n",
    "\n",
    "b = tf.random.normal([10,3])\n",
    "print(\"\\n b \\n\", b)\n",
    "\n",
    "# print(tf.gather(a,(0,1)))\n",
    "# print(tf.math.subtract(a, b))\n",
    "# print(tf.square(a-b))\n",
    "dist = tf.math.reduce_sum( (tf.square( tf.math.subtract(a, b) ))  , axis = 2)\n",
    "print(\"\\n dist \\n\", dist)\n",
    "\n",
    "ans = tf.where( tf.transpose(dist) ==tf.math.reduce_min(dist, axis = 1))\n",
    "print(\"\\n shortest dist \\n\", ans)\n",
    "\n",
    "reordered = tf.argsort(ans[:,1], axis = 0)\n",
    "print(\"\\n reordered \\n\", tf.gather(ans,reordered))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ragged tensor given row sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = tf.constant([1,2,8,4,5])\n",
    "\n",
    "# dummy_vals would be (points_x - mu_x)\n",
    "v = tf.random.uniform([tf.math.reduce_sum(sizes)])\n",
    "# print(\"v \\n\", v, \"\\n\")\n",
    "rag = tf.RaggedTensor.from_row_lengths(v ,sizes) \n",
    "print(\"as a normal tensor: \\n\", rag.to_tensor(), \"\\n\")\n",
    "print(\"nth ragged element of v \\n\", rag[0], \"\\n\")\n",
    "mu = tf.math.reduce_mean(rag, axis=1)\n",
    "print(\"mu \\n\", mu, \"\\n\")\n",
    "\n",
    "#test calculating std for each ragged element in rag\n",
    "for i in range(rag.bounding_shape()[0]):\n",
    "    #init stdx on first loop, append on each after\n",
    "    if i == 0:\n",
    "        stdx = tf.reduce_sum(tf.math.square( rag[i][:] - mu[i] ))[None]\n",
    "    else:\n",
    "        new = tf.reduce_sum(tf.math.square( rag[i][:] - mu[i] ))[None]\n",
    "        stdx = tf.concat([stdx, new], 0)\n",
    "print(\"stdx \\n\", stdx, \"\\n\")\n",
    "\n",
    "# mask_test = tf.RaggedTensor.from_row_lengths(tf.ones(tf.math.reduce_sum(sizes)) ,sizes)\n",
    "# print(tf.transpose(mask_test.to_tensor()))\n",
    "# print((dummy_vals.to_tensor() -1) * mask_test.to_tensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.constant([10,1], tf.int32)\n",
    "a = tf.zeros([1])[None]\n",
    "test = tf.tile(a, s)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Test R() and Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hat = np.array([1,0,0])\n",
    "theta =  0.1 #np.pi/6 #rad\n",
    "\n",
    "rot_mat_simp = R_simp(n_hat, theta)\n",
    "print(rot_mat_simp)\n",
    "angs = np.array([theta,0 ,0])\n",
    "rot_mat = R(angs)\n",
    "print(rot_mat)\n",
    "\n",
    "print(R2Euler(rot_mat))\n",
    "\n",
    "p_point = np.array([1,1,1]).T\n",
    "\n",
    "J = jacobian(angs, p_point)\n",
    "\n",
    "d_rot_mat_simp = dR_simp(n_hat,theta)\n",
    "# print(d_rot_mat_simp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit test R2Euler_tf and R_tf\n",
    "#### Works with single axis roation\n",
    "#### Works with vectoried input\n",
    "#### Solution becomes ambiguious with mutliple axis inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angs = tf.random.normal((3,1)) * tf.constant([[0.], [0.], [1.] ]) #single axis angle input\n",
    "# angs = tf.Variable([[0.],[ np.pi/2],[ 0.]]) #vector input, multiple axis rotation\n",
    "print(\"Input angs: \\n\", angs.numpy())\n",
    "# print(\"R(angs): \\n\", R_tf(angs).numpy())\n",
    "test1 = R2Euler_tf(R_tf(angs))\n",
    "print(\"R2Euler_tf(R(angs)): \\n\", test1.numpy())\n",
    "# test2 = R2Euler(R(angs[:,:2]))\n",
    "# print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit test tfp find bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.convert_to_tensor(cloud, np.float32)#[:100]\n",
    "# print(\"c: \\n\", c.numpy())\n",
    "startx = -100.\n",
    "stopx = 100.\n",
    "numx = 10\n",
    "edgesx = tf.linspace(startx, stopx, numx)\n",
    "xbins = tfp.stats.find_bins(c[:,0], edgesx)\n",
    "print(xbins)\n",
    "starty = -100.\n",
    "stopy = 100.\n",
    "numy = 10\n",
    "edgesy = tf.linspace(starty, stopy, numy)\n",
    "ybins = tfp.stats.find_bins(c[:,1], edgesy)\n",
    "print(ybins)\n",
    "\n",
    "min_num_pts = 1000\n",
    "\n",
    "count = 0\n",
    "E = []\n",
    "\n",
    "for x in range(numx):\n",
    "    for y in range(numy):\n",
    "        #only do calculations if there are a sufficicently high number of points in the bin\n",
    "        xin = tf.where(xbins == x)\n",
    "        if tf.shape(xin)[0] > min_num_pts:\n",
    "            if tf.shape(tf.where(tf.gather(ybins, xin) == y))[0] > min_num_pts: #repeat for y points at x coord\n",
    "#                 print(\"working\", x, y)\n",
    "                count += 1\n",
    "# print(xin)\n",
    "# print(ybins)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.linspace(1,5,5)\n",
    "print(test)\n",
    "ans = tf.where(test < 4)\n",
    "print(\"ans: \\n\",ans)\n",
    "print(\"\\n\", tf.gather(test, ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inline volumetric rendering using ipyvolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple demo\n",
    "import ipyvolume\n",
    "ds = ipyvolume.datasets.aquariusA2.fetch()\n",
    "short = ds.data[:,:,:]\n",
    "ipyvolume.quickvolshow(short, lighting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = np.linspace(1,12,8)\n",
    "ans[2] = 0\n",
    "print(ans)\n",
    "\n",
    "test = ans[ans < 10]\n",
    "print(test)\n",
    "np.shape(test)[0]\n",
    "\n",
    "print(np.median(test[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = tf.random.normal([3,3])\n",
    "print(tes)\n",
    "print(tf.reverse(tes, axis = [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.reshape([10.,10.,2.], (3,1))\n",
    "# print(type(a))\n",
    "# t = tf.Tensor(a, dtype = \"float32\")\n",
    "\n",
    "a = tf.constant([2.1,2.,3.])\n",
    "b = tf.constant([1.,2.,3.])\n",
    "tf.tensordot(a,b, axes = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye = tf.eye(3)\n",
    "# print(j)\n",
    "Jx = tf.constant([[1.], [2.], [3.]])\n",
    "J = tf.concat([eye, Jx], axis = 1)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare jacobian() and jacobian_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.sin(1.))\n",
    "start = time.time()\n",
    "numiter = 1000\n",
    "angs  = np.array([1.,0.1,0.1])\n",
    "p_point = np.array([1.,2.,3.])\n",
    "for _ in range(numiter):\n",
    "    J = jacobian(angs, p_point);\n",
    "print(\"took\", time.time()-start, \"seconds on CPU\")\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF is slower if we do them one by one BUT is waaay faster if we send them in all at once\n",
    "start = time.time()\n",
    "numiter = 1000\n",
    "# angs = tf.random.normal((3,numiter))\n",
    "angs = tf.constant([1.,2.,3.])\n",
    "p_point = tf.random.normal((3,numiter))\n",
    "J = jacobian_tf(p_point, angs);\n",
    "print(\"took\", time.time()-start, \"seconds on GPU\")\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing subdividing cells without loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal: given input tensor \"cloud\" and \"bins\" which contains all og and binned coordinates\n",
    "#      subdivide and perform ops on \"cloud\" wihtout using any loops \n",
    "\n",
    "bins = tf.transpose(tf.constant([[2., 1., 3., 0., 2., 2., 3., 1., 0., 2.],\n",
    "                                 [0., 1., 0., 0., 0., 0., 3., 1., 0., 0.],\n",
    "                                 [1., 2., 3., 0., 1., 1., 3., 2., 0., 1.]]))\n",
    "cloud = bins + tf.random.normal(tf.shape(bins))*0.1\n",
    "print(\"binned coordinate values: \\n\", bins)\n",
    "# print(cloud)\n",
    "\n",
    "#1d case (easy)\n",
    "# q = tf.constant([1., 1., 2.]) \n",
    "# print(tf.squeeze(tf.gather(bins, ans))) #works for 1d, unsure of utility in 2d\n",
    "#2d case (hard)\n",
    "q = tf.constant([[[1., 1., 2.]],\n",
    "                 [[0., 9., 9.]],\n",
    "                 [[2., 0., 1.]],\n",
    "                 [[7., 8., 9.]]])\n",
    "print(\"\\n cells of interest: \\n\",q)\n",
    "\n",
    "idx = tf.equal(bins, q)\n",
    "print(idx)\n",
    "#ans outputs tensor of shape [N,2], where:\n",
    "#  [[voxel number, index of [x,y,z] in cloud that corresponds to bin #],\n",
    "#   [voxel number,index of [x,y,z] in cloud that corresponds to bin #]] ... \n",
    "loc = tf.where(tf.math.reduce_all(idx, axis = 2) == True)\n",
    "print(\"\\n loc: \\n\", loc)\n",
    "\n",
    "#Need to \"ungroup\" so that we can fit_gaussian_tf() to each individual voxel...\n",
    "s = tf.shape(loc)\n",
    "group_ids, group_idx = tf.unique(loc[:, 0], out_idx=s.dtype)\n",
    "num_groups = tf.reduce_max(group_idx) + 1\n",
    "# print(group_ids, group_idx, num_groups)\n",
    "sizes = tf.math.bincount(group_idx)\n",
    "# print(sizes)\n",
    "\n",
    "#replace <bins> here with <cloud> when done debugging\n",
    "rag = tf.RaggedTensor.from_row_lengths(tf.gather(cloud, loc[:,1]), sizes) \n",
    "# print(\"ragged: \\n\", rag)\n",
    "\n",
    "#Run on GPU as vectorized operation (WAAAAAY Faster) --\n",
    "reg = tf.RaggedTensor.to_tensor(rag)\n",
    "print(\"\\n regular tensor: \\n\", reg)\n",
    "mu, sigma = fit_gaussian_tf(reg)\n",
    "print(\"mu: \\n\", mu)\n",
    "print(\"sigma: \\n\", sigma)\n",
    "#------------------------------------------------------- \n",
    "\n",
    "# # works but uses loop (runs on CPU -> slow) -----------\n",
    "# A =  tf.data.Dataset.from_tensor_slices(rag)\n",
    "# mus = []\n",
    "# sigmas = []\n",
    "# for i in range(len(A)):\n",
    "#     mu, sigma = fit_gaussian_tf(rag[i])\n",
    "#     mus.append(mu)\n",
    "#     sigmas.append(sigma)\n",
    "# print(mus, sigmas)\n",
    "# #------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find \"loc\" more efficiently than using tf.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"binned points: \\n\", bins)\n",
    "print(\"\\nbins to place them: \\n\",q)\n",
    "\n",
    "# #SUPER inefficient \n",
    "# for i in range(tf.shape(bins)[0]):\n",
    "#     for j in range(tf.shape(q)[0]):\n",
    "#         if tf.reduce_all(bins[i] == q[j]):\n",
    "#             print(j,i)\n",
    "#             try:\n",
    "#                 loc2 = tf.concat((loc2, tf.constant([[j,i]])), axis = 0)\n",
    "#             except:\n",
    "#                 loc2 = tf.constant([[j,i]])\n",
    "# print(\"\\n loc \\n\",loc2)\n",
    "# loc2 = None\n",
    "\n",
    "testidx = tf.where(bins == q[1])\n",
    "print(testidx)\n",
    "    \n",
    "print(\"\\n goal is to get this: \\n\",loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove zero rows from 3d tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins2 = tf.transpose(tf.constant([[[2., 9., 3., 0., 2., 2., 3., 1., 0., 2.], #x\n",
    "                                  [0., 0., 0., 0., 0., 0., 3., 1., 0., 0.],\n",
    "                                  [0., 1., 0., 0., 0., 0., 3., 1., 0., 0.],\n",
    "                                  [1., 2., 3., 0., 1., 1., 3., 2., 0., 1.]], \n",
    "                                  \n",
    "                                  [[2., 1., 3., 0., 2., 2., 3., 1., 0., 2.], #y\n",
    "                                  [0., 0., 0., 0., 0., 0., 3., 1., 0., 0.],\n",
    "                                  [0., 1., 0., 0., 0., 0., 3., 1., 0., 0.],\n",
    "                                  [1., 2., 3., 0., 1., 1., 3., 2., 0., 1.]],\n",
    "                                  \n",
    "                                  [[2., 1., 3., 1., 2., 2., 3., 1., 0., 2.], #z\n",
    "                                  [0., 0., 0., 0., 0., 0., 3., 1., 0., 0.],\n",
    "                                  [0., 1., 0., 0., 0., 0., 3., 1., 0., 0.],\n",
    "                                  [1., 2., 3., 0., 1., 1., 3., 2., 0., 1.]]]))\n",
    "\n",
    "print(bins2[:2])\n",
    "print(bins2[:,:,0])\n",
    "#need to AVERAGE point locations PER AXIS, PER BIN\n",
    "#   Ignore SPECIFIC POINTS where XYZ are ALL ZERO\n",
    "idx = tf.math.not_equal(bins2[:,:,0], tf.constant([0.,0.,0.]))\n",
    "print(idx[:2])\n",
    "mask = tf.where(tf.math.reduce_any(idx, axis = 2) == True)\n",
    "print(\"\\n mask: \\n\", mask[:6]) #correct(?)\n",
    "# print(tf.gather(bins,mask))\n",
    "\n",
    "nonzero = tf.gather(bins,[0,0,0])\n",
    "print(\"\\n nonzero elements: \\n\", nonzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 2D tensor with all permutations (:n1, :n2, :n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fida = 2 \n",
    "fidb = 4\n",
    "fidc = 1\n",
    "\n",
    "a = tf.linspace(0,fida-1,fida)[:,None]\n",
    "b = tf.linspace(0,fidb-1,fidb)[:,None]\n",
    "c = tf.linspace(0,fidc-1,fidc)[:,None]\n",
    "\n",
    "ansa = tf.tile(a, [fidb*fidc, 1])\n",
    "ansb = tf.tile(tf.reshape(tf.tile(b, [1,fida]), [-1,1] ), [(fidc), 1])\n",
    "ansc = tf.reshape(tf.tile(c, [1,fida*fidb]), [-1,1] )\n",
    "\n",
    "q = tf.squeeze(tf.transpose(tf.Variable([ansa,ansb,ansc])))\n",
    "print(q)\n",
    "\n",
    "#GOAL- determine which voxel pt belongs in based on its coords\n",
    "# pt = tf.constant([2,1,0])\n",
    "pt = bins\n",
    "print(\"\\n pt:\", pt, \"\\n\")\n",
    "num = tf.cast( ( pt[:,0] + fida*pt[:,1] + (fida*fidb)*pt[:,2] ), tf.int32)\n",
    "print(\"\\n\", num, \"\\n\")\n",
    "# print(q[num])\n",
    "\n",
    "ans = tf.concat((num[:,None], tf.cast(tf.linspace(0, tf.shape(pt)[0], tf.shape(pt)[0]  )[:,None],\n",
    "                                      dtype = tf.int32) ), axis = 1 )\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate covariance of 3d tensors with multiple voxels. Ignore zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(10, 2, 1))\n",
    "b = tf.random.normal(shape=(10, 2, 1))*4\n",
    "c = tf.random.normal(shape=(10, 2, 1))\n",
    "\n",
    "d = tf.concat((a,b,c), axis = 2)\n",
    "d = tf.concat((d, tf.zeros((10,2,3))), axis = 0)\n",
    "print(d[:,1])\n",
    "\n",
    "I = tf.sparse.eye(10,3)\n",
    "print(\"\\n Sparse Identity: \\n\", I)\n",
    "\n",
    "cov = tfp.stats.covariance(d, sample_axis = 0, event_axis = 2)\n",
    "print(\"\\n covariance matrices: \\n\", cov[1])\n",
    "\n",
    "print(sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.cast((tf.linspace(1,1000,100)), tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.linspace(1,100,100)\n",
    "print(test[1:100:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
