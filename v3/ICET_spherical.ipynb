{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spherical ICET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vedo import *\n",
    "import os\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "import pykitti\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import sin, cos, tan\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ICET_spherical import ICET\n",
    "\n",
    "# init KITTI dataset -----------------------------------------------------------------\n",
    "# basedir = 'C:/kitti/'\n",
    "basedir = '/media/derm/06EF-127D1/KITTI'\n",
    "date = '2011_09_26'\n",
    "drive = '0005'\n",
    "idx = 0\n",
    "# drive = '0093'\n",
    "# idx = 220\n",
    "dataset = pykitti.raw(basedir, date, drive)\n",
    "velo1 = dataset.get_velo(idx) # Each scan is a Nx4 array of [x,y,z,reflectance]\n",
    "c1 = velo1[:,:3]\n",
    "velo2 = dataset.get_velo(idx+1) # Each scan is a Nx4 array of [x,y,z,reflectance]\n",
    "c2 = velo2[:,:3]\n",
    "# c1 = c1[c1[:,2] > -1.5] #ignore ground plane\n",
    "# c2 = c2[c2[:,2] > -1.5] #ignore ground plane\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# # ## load custom point cloud geneated in matlab------------------------------------------\n",
    "# c1 = np.loadtxt(\"verify_geometry_scan1.txt\", dtype = float) #for debugging DNN filter\n",
    "# c2 = np.loadtxt(\"verify_geometry_scan2.txt\", dtype = float)\n",
    "# #debug: get rid of half of the points in scan 2 (testing outlier rejection indexing)\n",
    "# # c2 = c2[c2[:,1] > 0 ]\n",
    "# #add noise (if not generated when point clouds were created)\n",
    "# # c1 += 0.02*np.random.randn(np.shape(c1)[0], 3)\n",
    "# # c2 += 0.02*np.random.randn(np.shape(c2)[0], 3) \n",
    "# c1 = c1[c1[:,2] > -1.55] #ignore ground plane\n",
    "# c2 = c2[c2[:,2] > -1.55] #ignore ground plane\n",
    "# # ## ------------------------------------------------------------------------------------\n",
    "\n",
    "# #single distinct cluster---------------------------------------------------------------\n",
    "# c1 = np.random.randn(3000,3)*tf.constant([0.04,0.3,0.3]) + tf.constant([6.,0.,0.])\n",
    "# c2 = np.random.randn(3000,3)*tf.constant([0.04,0.3,0.3]) + tf.constant([6.,0.,0.]) - np.array([0., 0.25, 0.0])\n",
    "# # # c2 = c1 - np.array([0.1, 0.3, 0.0])\n",
    "# # -------------------------------------------------------------------------------------\n",
    "\n",
    "D = True\n",
    "# D = False\n",
    "X = tf.constant([0., 0., 0., 0., 0., 0.])\n",
    "it1 = ICET(cloud1 = c1, cloud2 = c2,  fid = 50, draw = D, \n",
    "           x0 = X, niter = 5, group= 2, RM = False, DNN_filter = False)\n",
    "\n",
    "ViewInteractiveWidget(it1.plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test rejection\n",
    "# print(tf.shape(it1.residuals_full))\n",
    "# print(tf.shape(it1.U_i))\n",
    "\n",
    "res = it1.residuals_full\n",
    "U = it1.U_i\n",
    "L = it1.L_i\n",
    "\n",
    "res_compact = L @ tf.transpose(U, [0,2,1]) @ res[:,:,None]\n",
    "# print(res_compact)\n",
    "bad = tf.where(tf.math.abs(res_compact) > 0.05)\n",
    "print(bad[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pre-process points from ICET to feed to DNN\n",
    "\n",
    "#Get ragged tensor containing all points from each scan inside each sufficient voxel\n",
    "in1 = it1.inside1\n",
    "npts1 = it1.npts1\n",
    "in2 = it1.inside2\n",
    "npts2 = it1.npts2\n",
    "corr = it1.corr #indices of bins that have enough points from scan1 and scan2\n",
    "\n",
    "# print(tf.shape(in2.to_tensor()))\n",
    "\n",
    "#get indices of rag with >= 25 elements\n",
    "ncells = tf.shape(corr)[0].numpy() #num of voxels with sufficent number of points\n",
    "# print(tf.gather(npts2, corr))\n",
    "enough1 = tf.gather(in1, corr)\n",
    "enough2 = tf.gather(in2, corr)\n",
    "print(tf.shape(enough2.to_tensor())[0].numpy())\n",
    "# print(npts2)\n",
    "# print(corr)\n",
    "\n",
    "#init array to store indices\n",
    "idx1 = np.zeros([ncells ,25])\n",
    "idx2 = np.zeros([ncells ,25])\n",
    "\n",
    "#loop through each element of ragged tensor\n",
    "for i in range(ncells):\n",
    "    idx1[i,:] = tf.random.shuffle(enough1[i])[:25].numpy() #shuffle order and take first 25 elements\n",
    "    idx2[i,:] = tf.random.shuffle(enough2[i])[:25].numpy() #shuffle order and take first 25 elements\n",
    "\n",
    "idx1 = tf.cast(tf.convert_to_tensor(idx1), tf.int32)\n",
    "idx2 = tf.cast(tf.convert_to_tensor(idx2), tf.int32)\n",
    "\n",
    "# print(it1.cloud1_tensor)\n",
    "from1 = tf.gather(it1.cloud1_tensor, idx1)\n",
    "# from2 = tf.gather(it1.cloud2_tensor_OG, idx2)\n",
    "from2 = tf.gather(it1.cloud2_tensor, idx2)\n",
    "# print(from1)\n",
    "\n",
    "x_test = tf.concat((from1, from2), axis = 1)\n",
    "# np.savetxt('perspective_shift/ICET_KITTI_frame0.txt', tf.reshape(from1, [-1, 3]).numpy())\n",
    "# np.savetxt('perspective_shift/ICET_KITTI_frame1.txt', tf.reshape(from2, [-1, 3]).numpy())\n",
    "\n",
    "model = tf.keras.models.load_model(\"perspective_shift/KITTInet.kmod\")\n",
    "from_DNN = model.predict(x_test)\n",
    "# print(from_DNN)\n",
    "# print(np.shape(from_DNN))\n",
    "print(tf.math.reduce_mean(from_DNN, axis = 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify compact directions where ICET and DNN disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare per cell translation estimates between ICET and DNN on converged results\n",
    "dnnsoln = tf.convert_to_tensor(from_DNN)\n",
    "# n = 0 #cell idx\n",
    "# print(dnnsoln[n])\n",
    "# print(it1.residuals[n])\n",
    "\n",
    "# icetsoln = tf.gather(it1.residuals_full, it1.corr)\n",
    "icetsoln = tf.gather(it1.residuals, it1.corr) #test\n",
    "\n",
    "#align differences between solutions with the principal axis of ICET scan1\n",
    "L = it1.L #only from \"mu1_enough\", \"sigma1_enough\" -> why it's too small rn???\n",
    "U = it1.U\n",
    "print(tf.shape(U))\n",
    "# print(U)\n",
    "\n",
    "# print(it1.enough1) #voxel IDs from scan1 with enough points\n",
    "# print(it1.corr)    # voxel IDs from BOTH with enough points\n",
    "\n",
    "#TODO: \n",
    "#  1) get IDX of elements that are in both enough1 and corr\n",
    "#  2) use this to index U and L to get U_i and L_i\n",
    "# print(it1.enough1)\n",
    "both = tf.sets.intersection(it1.enough1[None,:], it1.corr[None,:]).values\n",
    "ans = tf.where(it1.enough1[:,None] == both)[:,0]\n",
    "# print(ans)\n",
    "\n",
    "#project into frame of principal axis of distribution from scan1, prune extended axis\n",
    "LUT = tf.matmul(L, tf.transpose(U, [0,2,1]))\n",
    "it_compact = tf.matmul(LUT, icetsoln[:,:,None])\n",
    "dnn_compact = tf.matmul(LUT, dnnsoln[:,:,None])\n",
    "# print(it_compact)\n",
    "# print(dnn_compact)\n",
    "\n",
    "#find where the largest difference in residuals are\n",
    "thresh = 0.1\n",
    "#be careful- not sure what this index corresponds to (may not be voxel ID)\n",
    "problem_voxels = tf.where(tf.math.abs(it_compact - dnn_compact) > thresh)[:,0]\n",
    "print(problem_voxels)\n",
    "problem_voxels = tf.unique(problem_voxels)[0] #get rid of repeated indices\n",
    "print(problem_voxels)\n",
    "\n",
    "\n",
    "#remove extended axis\n",
    "# print(dnnsoln - icetsoln)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test loading raw KITTI point clouds (not compensated for rolling shutter)\n",
    "fn = \"C:/kitti/2011_09_26/2011_09_26_drive_0005_raw/velodyne_points/data/0000000000.txt\"\n",
    "\n",
    "test = np.loadtxt(fn)[:,:3]\n",
    "print(np.shape(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn1 = \"C:/kitti/2011_09_26/2011_09_26_drive_0005_raw/velodyne_points/data/0000000115.txt\"\n",
    "fn1 = \"C:/kitti/2011_09_26/2011_09_26_drive_0005_raw/velodyne_points/data/%010d.txt\" %(1)\n",
    "print(fn1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#generate plots from radial distance measurements\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "purple = np.load(\"figure_dist_measurements2.npy\")[:,0].T\n",
    "green = np.load(\"figure_dist_measurements1.npy\")[:,0].T\n",
    "orange = np.load(\"figure_dist_measurements3.npy\")[:,0].T\n",
    "blue = np.load(\"figure_dist_measurements4.npy\")[:,0].T\n",
    "\n",
    "scale = 3\n",
    "\n",
    "# ax.set_xlim([0,20])\n",
    "ax.set_ylim([-1,3*scale + 1])\n",
    "ax.set_aspect(\"equal\")\n",
    "# ax.set_yticks(ticks = [])\n",
    "ax.set_yticks(labels = ['Case a', 'Case b', 'Case c', 'Case d'], ticks = [3*scale, 2*scale, 1*scale, 0.])\n",
    "ax.set_xlabel(\"Distance from sensor (m)\")\n",
    "# ax.set_title(\"Spread of points per bin\") # $\\it{n}$\")\n",
    "# ax.set_ylabel(\"bin\")\n",
    "\n",
    "ptSize = 15\n",
    "ax.scatter(purple, scale*3*np.ones(len(purple)), marker = '.', s = ptSize, c = [0.5,0.,0.5], label = 'a')\n",
    "ax.scatter(orange, scale*2*np.ones(len(orange)), marker = '.', s = ptSize, c = [0.8,0.5,0.1], label = 'b')\n",
    "ax.scatter(green, scale*1*np.ones(len(green)), marker = '.', s = ptSize, c = [0,0.5,0], label = 'c')\n",
    "ax.scatter(blue, np.zeros(len(blue)), marker = '.', s = ptSize, c = [0.2,0.2,0.8], label = 'd')\n",
    "\n",
    "patches = []\n",
    "rectb = Rectangle([25.65, 5.5], 7.5, 1, fill = False) #(xy, w, h)\n",
    "patches.append(rectb)\n",
    "rectc = Rectangle([10.65, 2.5], 1.1, 1, fill = False) #(xy, w, h)\n",
    "patches.append(rectc)\n",
    "rectd = Rectangle([4.6, -0.5], 1.1, 1, fill = False) #(xy, w, h)\n",
    "patches.append(rectd)\n",
    "\n",
    "pc = PatchCollection(patches)\n",
    "pc.set_edgecolor('black')\n",
    "pc.set_facecolor([0,0,0,0])\n",
    "ax.add_collection(pc)\n",
    "\n",
    "\n",
    "# from utils import get_cluster\n",
    "# test = tf.convert_to_tensor(green)[:,None]\n",
    "# # print(tf.shape(test))\n",
    "# bounds = get_cluster(test, mnp = 50, thresh = 0.3)\n",
    "# # print(\"bounds:\", bounds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify outlier cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2,1)\n",
    "nbins = 25\n",
    "nstd = 2\n",
    "\n",
    "# print(it.dx_i[:,0])\n",
    "# print(tf.math.reduce_sum(it.dx_i, axis = 0))\n",
    "# print(it.W)\n",
    "# print(it.H)\n",
    "# print(it.residuals[:,0])\n",
    "\n",
    "component = it1.residuals_full[:,1]\n",
    "print(tf.shape(component))\n",
    "# print(component)\n",
    "\n",
    "print(\"\\n before:\")\n",
    "mu = tf.math.reduce_mean(component)\n",
    "print(\"mean\", mu)\n",
    "sigma = tf.math.reduce_std(component)\n",
    "print(\"standard deviation\", sigma)\n",
    "bad_idx = tf.where( tf.math.abs(component) > mu + nstd*sigma )\n",
    "# print(\"bad idx\", bad_idx)\n",
    "good_idx = tf.where( tf.math.abs(component) < mu + nstd*sigma )\n",
    "# print(tf.gather(component, bad_idx))\n",
    "# ax.hist(it.dx_i[:,0], nbins);\n",
    "ax[0].hist(component, nbins);\n",
    "ax[0].set_xlabel(\"y_i - y0_i (forward translation error)\")\n",
    "ax[0].set_ylabel(\"frequency\")\n",
    "ax[0].set_title(\"All Distributions\")\n",
    "\n",
    "#test to make sure outliers are being removed correctly\n",
    "component = it1.residuals[:,1]\n",
    "print(tf.shape(component))\n",
    "# print(component)\n",
    "\n",
    "print(\"\\n after:\")\n",
    "mu = tf.math.reduce_mean(component)\n",
    "print(\"mean\", mu)\n",
    "sigma = tf.math.reduce_std(component)\n",
    "print(\"standard deviation\", sigma)\n",
    "\n",
    "ax[1].hist(component, nbins);\n",
    "ax[1].set_xlabel(\"y_i - y0_i (forward translation error)\")\n",
    "ax[1].set_ylabel(\"frequency\")\n",
    "ax[1].set_title(\"Best Fitting Distributions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "# print(it.residuals_full[:,0])\n",
    "edges = tf.linspace(-1.,1.,30)\n",
    "# print(edges)\n",
    "print(edges)\n",
    "\n",
    "bins_soln = tfp.stats.find_bins(it.residuals_full[:,0], edges)\n",
    "# print(bins_soln)\n",
    "\n",
    "good_idx = tf.where(bins_soln == 14)\n",
    "bad_idx = tf.where(bins_soln == 14)\n",
    "# print(bad_idx)\n",
    "# print(good_idx)\n",
    "# print(tf.gather(it.residuals_full[:,0], good_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[5, 6, 7, 8]])\n",
    "b = tf.constant([[8, 7, 10]])\n",
    "print(tf.sets.difference(a,b).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Useful Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import get_cluster\n",
    "\n",
    "#index of spike that each of the points from cloud 1 is occupying\n",
    "print(it1.bins_spike)\n",
    "\n",
    "occupied_spikes, idxs = tf.unique(it1.bins_spike)\n",
    "print(\"\\n occupied_spikes \\n\", occupied_spikes)\n",
    "temp =  tf.where(it1.bins_spike == occupied_spikes[:,None])\n",
    "rag = tf.RaggedTensor.from_value_rowids(temp[:,1], temp[:,0])\n",
    "idx_by_rag = tf.gather(it1.cloud1_tensor_spherical[:,0], rag)\n",
    "\n",
    "# rads = idx_by_rag[50,:] #single element from ragged tensor\n",
    "rads = tf.transpose(idx_by_rag.to_tensor()[:3,:])\n",
    "# rads = tf.transpose(idx_by_rag.to_tensor())\n",
    "# print(rads) #starts out unordered\n",
    "\n",
    "# #_________________________________________________________________\n",
    "fig, ax = plt.subplots(2,1)\n",
    "nbins = 25\n",
    "ax[0].hist(rads.numpy(), nbins, histtype = 'step');\n",
    "yax = tf.ones(tf.shape(rads), tf.float32) #plots everything on top of eachother\n",
    "yax = yax * tf.cast(tf.linspace(1, 0, tf.shape(rads)[1]), tf.float32)\n",
    "# print(tf.linspace(0, 1, tf.shape(rads)[1])[:,None] )\n",
    "ax[1].plot(rads,yax, 'b.', markersize = 3)\n",
    "# #_________________________________________________________________\n",
    "\"\"\n",
    "bounds = get_cluster(rads)\n",
    "print(\"\\n Bounds \\n\", bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros([1,2])\n",
    "b = np.ones([3,2])\n",
    "print(np.append(b, a, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_r = bounds[6,:]\n",
    "print(edges_r)\n",
    "pts = tf.cast(tf.convert_to_tensor(c1[:,1]), tf.float64)\n",
    "print(pts)\n",
    "\n",
    "bins_r = tfp.stats.find_bins(pts, edges_r)\n",
    "print(bins_r)\n",
    "#get rid of NaNs\n",
    "nonnan = 1 - tf.cast(tf.math.is_nan(bins_r), tf.float32)\n",
    "idxnonan = tf.where(nonnan == 1)\n",
    "print(tf.gather(bins_r, idxnonan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test load Ford Campus Dataset 1 sample\n",
    "# import h5py\n",
    "import mat4py\n",
    "fn1 = 'E:/Ford/IJRR-Dataset-1-subset/SCANS/Scan1000.mat'\n",
    "dat1 = mat4py.loadmat(fn1)\n",
    "\n",
    "SCAN1 = dat1['SCAN']\n",
    "pc1 = np.transpose(np.array(SCAN1['XYZ']))\n",
    "\n",
    "print(np.shape(pc1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ground truth transformation from KITTI poses file (full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load transformation matrices\n",
    "# filename = \"E:/KITTI/dataset/2011_09_26/2011_09_26_drive_00_sync/poses/00.txt\"\n",
    "filename ='/media/derm/06EF-127D1/KITTI/dataset/2011_09_26/2011_09_26_drive_00_sync/poses/00.txt'\n",
    "full_poses = np.loadtxt(filename)\n",
    "mat_full = np.reshape(full_poses, [-1,3,4])\n",
    "\n",
    "frame = 4500 #300 good\n",
    "t_i = mat_full[frame, :, :]\n",
    "t_next = mat_full[frame+1, :, :]\n",
    "print(t_i)\n",
    "# print(t_next - t_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rotation matrices\n",
    "from utils import R2Euler\n",
    "Rmat = tf.convert_to_tensor(mat_full[:,:,:3])\n",
    "euls = R2Euler(Rmat)\n",
    "drot = euls[:,frame+1] - euls[:,frame]\n",
    "#re-order to match ICET output\n",
    "drot = np.array([-drot[2], drot[0], drot[1] ])\n",
    "# print(\"change in rotation:\", drot)\n",
    "\n",
    "#get translation in vehicle body frame \n",
    "dpos_xyz = mat_full[frame+1,:,3] - mat_full[frame,:,3]\n",
    "# print(dpos_xyz)\n",
    "dpos_bf = np.array([np.sqrt(dpos_xyz[0]**2 + dpos_xyz[2]**2), 0, dpos_xyz[1]])\n",
    "# print(\"translation in vehicle body frame:\", dpos_bf)\n",
    "\n",
    "X = np.append(dpos_bf, drot)\n",
    "#assume zero vertical movement between frames???\n",
    "X[2] = 0\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot trajectory\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mat_full[0,2,3], mat_full[0,0,3], 'ro') #mark start in red\n",
    "for i in range(frame):\n",
    "    ax.plot(mat_full[i,2,3], mat_full[i,0,3], 'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot body frame velocity\n",
    "v_xyz = np.diff(mat_full[:,:,3], axis = 0)\n",
    "v_xyz = np.array([v_xyz[:,2], v_xyz[:,0], v_xyz[:,1] ]).T #need to reorder\n",
    "# print(v_xyz[10])\n",
    "vf = np.sqrt(v_xyz[:,0]**2  + v_xyz[:,1]**2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True transformation between frames (for mini KITTI datsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get true transformation between frames (for mini KITTI datsets)\n",
    "from metpy.calc import lat_lon_grid_deltas\n",
    "idx = 51\n",
    "poses0 = dataset.oxts[idx] #<- ID of 1st scan\n",
    "poses1 = dataset.oxts[idx+1] #<- ID of 2nd scan\n",
    "lat0 = poses0.packet.lat\n",
    "lon0 = poses0.packet.lon\n",
    "alt0 = poses0.packet.alt\n",
    "lat1 = poses1.packet.lat\n",
    "lon1 = poses1.packet.lon\n",
    "alt1 = poses1.packet.alt\n",
    "\n",
    "# print(lat0)\n",
    "# print(lon0)\n",
    "\n",
    "dx_oxts, dy_oxts = lat_lon_grid_deltas(np.array([lon0,lon1]), np.array([lat0, lat1]))\n",
    "# print(dx_oxts, dy_oxts) \n",
    "dx_oxts = dx_oxts[0,0].magnitude\n",
    "dy_oxts = dy_oxts[0,0].magnitude\n",
    "dz_oxts = (alt0-alt1)\n",
    "droll_oxts = (poses0.packet.roll - poses1.packet.roll)\n",
    "dpitch_oxts = (poses0.packet.pitch - poses1.packet.pitch)\n",
    "dyaw_oxts = (poses0.packet.yaw - poses1.packet.yaw)\n",
    "\n",
    "rot = poses1.T_w_imu[:3,:3] #trying this\n",
    "\n",
    "dxyz_oxts = np.array([[dx_oxts, dy_oxts, dz_oxts]])\n",
    "dxyz_lidar = dxyz_oxts.dot(rot)\n",
    "print(dxyz_lidar)\n",
    "\n",
    "dt = 0.10\n",
    "# dt = 0.1037 #mean time between lidar samples\n",
    "from_vel = np.array([[poses1.packet.vf*dt, poses1.packet.vl*dt, poses1.packet.vu*dt, -poses1.packet.wf*dt, -poses1.packet.wl*dt, -poses1.packet.wu*dt]])\n",
    "print(from_vel)\n",
    "\n",
    "# print(poses1.packet.vel_accuracy)\n",
    "# print((dataset.timestamps[idx+1] - dataset.timestamps[idx]).microseconds/(10e5))\n",
    "\n",
    "# # print(np.shape(dataset.timestamps)[0])\n",
    "# # tvec = np.zeros(np.shape(dataset.timestamps)[0])\n",
    "tvec = np.zeros(149)\n",
    "# # for i in range(np.shape(dataset.timestamps)[0] - 1):\n",
    "for i in range(149):\n",
    "#     print((dataset.timestamps[i+1] - dataset.timestamps[i]).microseconds/(10e5))\n",
    "    tvec[i] = (dataset.timestamps[i+1] - dataset.timestamps[i]).microseconds/(10e5)\n",
    "# print(tvec)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[1,2,3,4]])\n",
    "test = np.append(test,np.array([[0,2,3,4]]),axis = 0)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test batch rotation matrix conversions\n",
    "from utils import R_tf\n",
    "\n",
    "print(R_tf(tf.Variable([[0., 0., 1.]])), \"\\n\")\n",
    "\n",
    "angs = tf.Variable([[0., 0., 1.], [0., 0., 1.]])\n",
    "# angs = tf.Variable([[0., 0., 1.]])\n",
    "print(angs)\n",
    "\n",
    "rots = R_tf(angs) \n",
    "print(rots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad tensors to get them to the same length\n",
    "#to fix bug in get_U_and_L()\n",
    "\n",
    "t1 = tf.ones([3,8], tf.int32)\n",
    "print(t1)\n",
    "t2 = tf.ones([3,7], tf.int32)\n",
    "print(t2)\n",
    "\n",
    "bofa = tf.sets.intersection(t1, t2).values\n",
    "print(bofa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test- workaround for in place tensor operations\n",
    "indices = tf.cast(tf.constant([1, 2, 3, 5]), tf.int32)[:,None]\n",
    "print(\"indices\", indices)\n",
    "updates = tf.ones(tf.shape(indices))\n",
    "print(\"updates\", updates)\n",
    "shape = tf.constant([7, 1])\n",
    "print(\"shape\", shape)\n",
    "\n",
    "b = tf.scatter_nd(indices, updates, shape)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test incramenting strings for file names\n",
    "for a in range(10):\n",
    "    test = \"Scan%04d.mat\" %(a+1000)\n",
    "    test =  'E:/Ford/IJRR-Dataset-1-subset/SCANS/' + test\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results of ICET estimates on KITTI lidar point clouds vs GPS/INS baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "font = {'fontname':'Times New Roman'}\n",
    "\n",
    "#v8 is the best so far...\n",
    "ICET_estimates = np.loadtxt(\"ICET_estimates_v8.txt\")\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_v10.txt\")\n",
    "# OXTS_baseline = np.loadtxt(\"OXTS_baseline_v10.txt\")\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_v13.txt\")\n",
    "# OXTS_baseline = np.loadtxt(\"OXTS_baseline_v13.txt\")\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_v18.txt\")\n",
    "OXTS_baseline = np.loadtxt(\"OXTS_baseline_v17.txt\")\n",
    "BC = np.loadtxt(\"Before_correction_v18.txt\")\n",
    "\n",
    "# vf_from_matlab = np.loadtxt(\"vf.txt\")\n",
    "# vf_from_matlab = np.append(vf_from_matlab, 0)\n",
    "# # print(vf_from_matlab)\n",
    "# OXTS_baseline[:,0] = vf_from_matlab\n",
    "\n",
    "# OXTS_baseline = np.loadtxt(\"OXTS_baseline_gps.txt\")\n",
    "\n",
    "# OXTS_baseline[:,3:] = OXTS_baseline[:,3:]/0.1*0.1037\n",
    "# OXTS_baseline = OXTS_baseline/0.1*0.1037\n",
    "OXTS_baseline = (OXTS_baseline/0.1*0.1037 + OXTS_baseline)/2 #test\n",
    "\n",
    "# ICET_estimates[:,0] = ICET_estimates[:,0]/tvec*0.1\n",
    "\n",
    "#fix sign errors\n",
    "ICET_estimates[:,1] = -ICET_estimates[:,1]\n",
    "ICET_estimates[:,3:] = -ICET_estimates[:,3:]\n",
    "BC[:,1] = -BC[:,1]\n",
    "BC[:,3:] = -BC[:,3:]\n",
    "\n",
    "style1 = 'b-'\n",
    "style2 = 'r-'\n",
    "style3 = 'b--'\n",
    "\n",
    "fig, ax = plt.subplots(3,2, constrained_layout = True)\n",
    "ax[0,0].plot(BC[:,0], style3, label = 'ICET- before', alpha = 0.3)\n",
    "ax[0,0].plot(ICET_estimates[:,0], style1, label = 'ICET- after')\n",
    "ax[0,0].plot(OXTS_baseline[:,0], style2, label = 'GPS/INS Baseline')\n",
    "# ax[0,0].plot(np.sqrt(OXTS_baseline[:,0]**2 + OXTS_baseline[:,1]**2), style2, label = 'GPS/INS Baseline')\n",
    "ax[0,0].set_title(\"change in x per frame\", **font)\n",
    "ax[0,0].set_ylabel(\"dx (m)\", **font)\n",
    "ax[0,0].legend(loc = 'best')\n",
    "ax[0,0].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[1,0].plot(BC[:,1], style3, alpha = 0.3)\n",
    "ax[1,0].plot(ICET_estimates[:,1], style1, lw = 1)\n",
    "ax[1,0].plot(-OXTS_baseline[:,1], style2, lw = 1)\n",
    "# ax[1,0].plot(np.arange(n//2, np.shape(ICET_estimates)[0] - n//2 ), moving_avg(OXTS_baseline[:,1], n),  style2, lw = 1)\n",
    "ax[1,0].set_title(\"change in y per frame\", **font)\n",
    "ax[1,0].set_ylabel(\"dy (m)\", **font)\n",
    "ax[1,0].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[2,0].plot(BC[:,2], style3, alpha = 0.3)\n",
    "ax[2,0].plot(ICET_estimates[:,2], style1, lw = 1)\n",
    "ax[2,0].plot(OXTS_baseline[:,2], style2, lw = 1)\n",
    "# ax[2,0].plot(np.arange(n//2, np.shape(ICET_estimates)[0] - n//2 ), moving_avg(OXTS_baseline[:,2], n),  style2, lw = 1)\n",
    "ax[2,0].set_title(\"change in z per frame\", **font)\n",
    "ax[2,0].set_ylabel(\"dz (m)\", **font)\n",
    "ax[2,0].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[0,1].plot(BC[:,3], style3, alpha = 0.3)\n",
    "ax[0,1].plot(ICET_estimates[:,3], style1, lw = 1)\n",
    "ax[0,1].plot(OXTS_baseline[:,3], style2, lw = 1)\n",
    "ax[0,1].set_title(\"change in roll per frame\", **font)\n",
    "ax[0,1].set_ylabel(\"droll (rad)\", **font)\n",
    "ax[0,1].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[1,1].plot(BC[:,4], style3, alpha = 0.3)\n",
    "ax[1,1].plot(ICET_estimates[:,4], style1, lw = 1)\n",
    "ax[1,1].plot(OXTS_baseline[:,4], style2, lw = 1)\n",
    "ax[1,1].set_title(\"change in pitch per frame\", **font)\n",
    "ax[1,1].set_ylabel(\"dpitch (rad)\", **font)\n",
    "ax[1,1].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[2,1].plot(BC[:,5], style3, alpha = 0.3)\n",
    "ax[2,1].plot(ICET_estimates[:,5], style1, lw = 1)\n",
    "ax[2,1].plot(OXTS_baseline[:,5], style2, lw = 1)\n",
    "ax[2,1].set_title(\"change in yaw per frame\", **font)\n",
    "ax[2,1].set_ylabel(\"dyaw (rad)\", **font)\n",
    "ax[2,1].set_xlabel(\"frame\", **font)\n",
    "\n",
    "# fig.tight_layout(h_pad = 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot error between ICET and absolute position\n",
    "plt.rc('font',family='Times New Roman')\n",
    "fig3, ax3 = plt.subplots(1,1)\n",
    "\n",
    "# ICET_pred_stds = np.loadtxt(\"ICET_pred_stds_v8.txt\")\n",
    "ICET_pred_stds = np.loadtxt(\"ICET_pred_stds_v13.txt\")\n",
    "\n",
    "# ICET_pred_stds = (2*np.sqrt(ICET_pred_stds))**2\n",
    "# ICET_pred_stds = np.sqrt(2*(ICET_pred_stds**2))\n",
    "\n",
    "\n",
    "#which component to look at\n",
    "# c = 3 #roll\n",
    "# c = 4 #pitch\n",
    "# c = 5 #yaw\n",
    "c = 0 # x (forward movement)\n",
    "\n",
    "diffx = OXTS_baseline[:,c] - ICET_estimates[:,c]\n",
    "diffx_BC = OXTS_baseline[:,c] - BC[:,c]\n",
    "\n",
    "# print(abs(diffx))\n",
    "print(\"correlation coefficient \\n\", np.corrcoef(abs(diffx), ICET_pred_stds[:,0]))\n",
    "\n",
    "#flip sign when looking at yaw\n",
    "if c ==5:\n",
    "    diffx = -diffx \n",
    "    \n",
    "cum_err = np.zeros(np.shape(ICET_pred_stds))\n",
    "cum_diffx = np.zeros(np.shape(diffx))\n",
    "cum_diffx_BC = np.zeros(np.shape(diffx_BC))\n",
    "\n",
    "for i in range(np.shape(ICET_pred_stds)[0]):\n",
    "    cum_err[i,:] = np.sum(ICET_pred_stds[:i,:]**2, axis = 0)\n",
    "    #add in baseline OXTS 1-sigma errors\n",
    "#     cum_err[i,:] += np.sqrt(2)*np.array([0.05,0.05,0.1,0.0005,0.0005,0.001])**2\n",
    "    cum_err[i,:] += np.sqrt(2)*np.array([0.08,0.08,0.1,0.0005,0.0005,0.001745])**2\n",
    "    cum_err[i,:] = np.sqrt(cum_err[i,:]) \n",
    "    \n",
    "for j in range(np.shape(diffx)[0]):\n",
    "    cum_diffx[j] = np.sum(diffx[:j]) \n",
    "    cum_diffx_BC[j] = np.sum(diffx_BC[:j]) \n",
    "\n",
    "# #old (error for each individual timestep)------------------------\n",
    "ax3.plot(diffx, label = 'GPS/INS - ICET (after bias reduction)')\n",
    "ax3.plot(diffx_BC, 'b--', label = 'GPS/INS - ICET (before bias reduction)', alpha = 0.3)\n",
    "ax3.fill_between(np.linspace(0,150,np.shape(ICET_pred_stds)[0]), -2*ICET_pred_stds[:,c], 2*ICET_pred_stds[:,c], \n",
    "                 color = (0.5,0.5,0.5,0.4), label = 'ICET Predicted 2σ Error Bounds')\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "# # #new (accumulated differences in error)--------------------------\n",
    "# # ax3.plot(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), cum_diffx_with_ground, label = 'GPS/INS - ICET')\n",
    "# ax3.plot(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), cum_diffx, label = 'GPS/INS - ICET (after correction)')\n",
    "# ax3.plot(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), cum_diffx_BC, 'b--', alpha = 0.3, label = 'GPS/INS - ICET (before correction)')\n",
    "# ax3.fill_between(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), -2*cum_err[:,c], 2*cum_err[:,c], \n",
    "#                  color = (0,0,1,0.2), label = 'Predicted 2σ Error Bounds')\n",
    "# # # --------------------------------------------------------------------\n",
    "\n",
    "# ax3.legend(loc = 'lower left')\n",
    "ax3.legend(loc = 'best')\n",
    "ax3.set_title(\"Predicted vs Actual Error in x\")\n",
    "ax3.set_xlabel(\"time (s)\", **font)\n",
    "ax3.set_ylabel(\"GPS/INS Baseline x - Odometry Estimate x (m)\", **font)\n",
    "# ax3.set_title(\"Predicted vs Actual Error in yaw\")\n",
    "# ax3.set_xlabel(\"time (s)\", **font)\n",
    "# ax3.set_ylabel(\"GPS/INS Baseline yaw - Odometry Estimate yaw (rad)\", **font)\n",
    "# ax3.set_ylim(-0.032,0.045)\n",
    "# ax3.set_ylim([-0.07,0.07])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test where points are inside spherical cell...\n",
    "# print(it.cloud1_tensor_spherical)\n",
    "maxtheta = tf.constant([[0.2],[0.7]])\n",
    "maxr = tf.constant([[0.5],[2.]])\n",
    "\n",
    "ans1 = tf.greater(it.cloud1_tensor_spherical[:,1], maxtheta)\n",
    "# print(ans1)\n",
    "ans2 = tf.less(it.cloud1_tensor_spherical[:,0], maxr)\n",
    "# print(ans2)\n",
    "combined = tf.Variable([ans1, ans2])\n",
    "# print(combined)\n",
    "ans3 = tf.math.reduce_all(combined, axis = 1)\n",
    "\n",
    "print(ans3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate each element of an n*1 vector 3 times\n",
    "t = tf.linspace(0,5,6)[:,None]\n",
    "print(t)\n",
    "\n",
    "test  = tf.tile(t, [3,1])\n",
    "# print(test)\n",
    "test2 = tf.reshape(tf.transpose(tf.reshape(test, [3,-1])), [-1,1])\n",
    "print(test2)\n",
    "test3 = tf.reshape(tf.transpose(tf.reshape(test, [3,-1])), [-1,3])\n",
    "print(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate each element of an n*3 vector 3 times\n",
    "t = tf.linspace(1,5,5)\n",
    "t = tf.transpose(tf.Variable([t, 2*t, 3*t]))\n",
    "print(t)\n",
    "\n",
    "test  = tf.tile(t, [4,1])\n",
    "# print(test)\n",
    "\n",
    "test = tf.reshape(tf.transpose(test), [3, 4, -1])\n",
    "# print(test)\n",
    "\n",
    "test = tf.transpose(test, [2,1,0])\n",
    "# print(test)\n",
    "\n",
    "test = tf.reshape(test, [-1,3])\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MC sim to compare performance estimation in \"realistic\" scenes with flat vs curved surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "c1_OG = np.loadtxt(\"scene2_scan1.txt\", dtype = float) #thin cylinders\n",
    "c2_OG = np.loadtxt(\"scene2_scan2.txt\", dtype = float)\n",
    "# c1_OG = np.loadtxt(\"scene3_scan1.txt\", dtype = float) #rectangles\n",
    "# c2_OG = np.loadtxt(\"scene3_scan2.txt\", dtype = float)\n",
    "# c1 = np.loadtxt(\"scene4_scan1.txt\", dtype = float) #cylinders\n",
    "# c2 = np.loadtxt(\"scene4_scan2.txt\", dtype = float)\n",
    "\n",
    "xvec = np.zeros([epochs, 6])\n",
    "pred_stds = np.zeros([epochs, 6])\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"\\n -------------- Epoch\", i, \"--------------------\")\n",
    "    #add noise (if not generated when point clouds were created)\n",
    "    c1 = c1_OG + 0.02*np.random.randn(np.shape(c1_OG)[0], 3)\n",
    "    c2 = c2_OG + 0.02*np.random.randn(np.shape(c2_OG)[0], 3)  \n",
    "\n",
    "    it = ICET(cloud1 = c1, cloud2 = c2, fid = 70, niter = 20, draw = False, group = 2, RM = True)\n",
    "    xvec[i] = it.X\n",
    "    pred_stds[i] = it.pred_stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_soln_err = np.array([-0.5, 0, 0, 0, 0, 0,]) - np.mean(xvec, axis = 0)\n",
    "\n",
    "print(\"Smaller Clylindrical features, no occlusion, no outlier rejection, n=10:\\n\")\n",
    "# print(\"Rectangular features, no occlusion, no outlier rejection, n=50:\\n\")\n",
    "\n",
    "print(\"mean solution error: \\n\", mean_soln_err)\n",
    "soln_std = np.std(xvec, axis = 0)\n",
    "print(\"\\n experimentally determined std: \\n\", soln_std)\n",
    "mean_pred_std = np.mean(pred_stds, axis = 0)\n",
    "print(\"\\n predicted std: \\n\", mean_pred_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "nbins = 10\n",
    "ax.hist(xvec[:,0], nbins)\n",
    "# ax.set_title(\"Clylindrical features, no occlusion, no outlier rejection, n=50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing bad indices from tensor (used in test where we replace dz with dnn soln)\n",
    "\n",
    "test = tf.random.normal([4,3])\n",
    "print(test)\n",
    "\n",
    "bad_idx = tf.constant([1,3])[None,:]\n",
    "print(bad_idx)\n",
    "all_idx = tf.cast(tf.linspace(0,tf.shape(test)[0].numpy()-1,tf.shape(test)[0].numpy()), tf.int32)[None,:]\n",
    "print(all_idx)\n",
    "\n",
    "good_idx = tf.sets.difference(all_idx, bad_idx).values\n",
    "print(good_idx)\n",
    "print(tf.gather(test, good_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
