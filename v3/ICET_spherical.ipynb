{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spherical ICET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 23:03:11.023942: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 23:03:11.132131: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 23:03:11.545205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/derm/anaconda3/envs/py39/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-11-15 23:03:11.545259: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/derm/anaconda3/envs/py39/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-11-15 23:03:11.545263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 23:03:12.080249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:03:12.099947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:03:12.100212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "from vedo import *\n",
    "import os\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "import pykitti\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import sin, cos, tan\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# for device in physical_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 4*1024\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 23:03:12.149737: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 23:03:12.150426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:03:12.150629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:03:12.150768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:03:12.485978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:03:12.486152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:03:12.486277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:03:12.486371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n",
      "2022-11-15 23:03:16.624192: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-11-15 23:03:16.908114: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x8426a90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 2.8583395e-01  1.8191051e-02  3.1497292e-03 -2.1548053e-04\n",
      "  4.3223388e-03  1.3837331e-02], shape=(6,), dtype=float32)\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 3.2559136e-01  1.6874645e-02  3.8070772e-03 -2.5382821e-04\n",
      "  4.4757212e-03  1.4040945e-02], shape=(6,), dtype=float32)\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 3.3207193e-01  1.5300351e-02  3.9010930e-03 -2.5943207e-04\n",
      "  4.4892821e-03  1.3999331e-02], shape=(6,), dtype=float32)\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 3.3309239e-01  1.4907254e-02  3.9138766e-03 -2.5960983e-04\n",
      "  4.4908281e-03  1.3994747e-02], shape=(6,), dtype=float32)\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 3.3306915e-01  1.4907764e-02  3.9172969e-03 -2.5931624e-04\n",
      "  4.4915411e-03  1.3994705e-02], shape=(6,), dtype=float32)\n",
      "pred_stds: \n",
      " tf.Tensor(\n",
      "[9.6863497e-04 1.0170989e-03 7.2491741e-05 1.2732616e-05 1.4471746e-05\n",
      " 9.2156988e-05], shape=(6,), dtype=float32)\n",
      " L2: \n",
      " tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]], shape=(6, 6), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f658782a5f40699fa2f35f3da869c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=1043, layout=Layout(height='auto', width='100%'), width=1280)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ICET_spherical import ICET\n",
    "\n",
    "# init KITTI dataset -----------------------------------------------------------------\n",
    "# basedir = 'C:/kitti/'\n",
    "basedir = '/media/derm/06EF-127D1/KITTI'\n",
    "date = '2011_09_26'\n",
    "drive = '0005'\n",
    "idx = 0\n",
    "# drive = '0093'\n",
    "# idx = 220\n",
    "dataset = pykitti.raw(basedir, date, drive)\n",
    "velo1 = dataset.get_velo(idx) # Each scan is a Nx4 array of [x,y,z,reflectance]\n",
    "c1 = velo1[:,:3]\n",
    "velo2 = dataset.get_velo(idx+1) # Each scan is a Nx4 array of [x,y,z,reflectance]\n",
    "c2 = velo2[:,:3]\n",
    "# c1 = c1[c1[:,2] > -1.5] #ignore ground plane\n",
    "# c2 = c2[c2[:,2] > -1.5] #ignore ground plane\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# # ## load custom point cloud geneated in matlab------------------------------------------\n",
    "# c1 = np.loadtxt(\"verify_geometry_scan1.txt\", dtype = float) #for debugging DNN filter\n",
    "# c2 = np.loadtxt(\"verify_geometry_scan2.txt\", dtype = float)\n",
    "# #debug: get rid of half of the points in scan 2 (testing outlier rejection indexing)\n",
    "# # c2 = c2[c2[:,1] > 0 ]\n",
    "# #add noise (if not generated when point clouds were created)\n",
    "# # c1 += 0.02*np.random.randn(np.shape(c1)[0], 3)\n",
    "# # c2 += 0.02*np.random.randn(np.shape(c2)[0], 3) \n",
    "# c1 = c1[c1[:,2] > -1.55] #ignore ground plane\n",
    "# c2 = c2[c2[:,2] > -1.55] #ignore ground plane\n",
    "# # ## ------------------------------------------------------------------------------------\n",
    "\n",
    "# #single distinct cluster---------------------------------------------------------------\n",
    "# c1 = np.random.randn(3000,3)*tf.constant([0.04,0.3,0.3]) + tf.constant([6.,0.,0.])\n",
    "# c2 = np.random.randn(3000,3)*tf.constant([0.04,0.3,0.3]) + tf.constant([6.,0.,0.]) - np.array([0., 0.25, 0.0])\n",
    "# # # c2 = c1 - np.array([0.1, 0.3, 0.0])\n",
    "# # -------------------------------------------------------------------------------------\n",
    "\n",
    "D = True\n",
    "# D = False\n",
    "X = tf.constant([0., 0., 0., 0., 0., 0.])\n",
    "it1 = ICET(cloud1 = c1, cloud2 = c2,  fid = 50, draw = D, \n",
    "           x0 = X, niter = 5, group= 2, RM = False, DNN_filter = False)\n",
    "\n",
    "ViewInteractiveWidget(it1.plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtimeit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "timeit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 23:04:17.997052: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 23:04:18.098959: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 23:04:18.494467: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/derm/anaconda3/envs/py39/lib/python3.9/site-packages/cv2/../../lib64:/home/derm/anaconda3/envs/py39/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-11-15 23:04:18.494523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/derm/anaconda3/envs/py39/lib/python3.9/site-packages/cv2/../../lib64:/home/derm/anaconda3/envs/py39/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-11-15 23:04:18.494528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-15 23:04:18.913378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:04:18.934460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:04:18.934632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:04:19.050780: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 23:04:19.051728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:04:19.052016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:04:19.052227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:04:19.385064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:04:19.385256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:04:19.385395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-15 23:04:19.385498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n",
      "2022-11-15 23:04:24.560086: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-11-15 23:04:24.813969: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x71d8380\n",
      "2022-11-15 23:04:27.847590: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n",
      "2022-11-15 23:04:28.206020: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Ground truth poses are not avaialble for sequence 09.\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 9.6676338e-01  9.2120767e-03  8.7613408e-03 -2.5694739e-04\n",
      "  2.3203035e-04  2.0662434e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 9.6305877e-01  1.0306334e-02  1.1121606e-02 -3.2966310e-04\n",
      "  6.0296693e-04  2.3027936e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 9.6203017e-01  1.2354067e-02  1.2188804e-02 -3.3440531e-04\n",
      "  8.4410451e-04  2.3175529e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 9.6189344e-01  1.2511434e-02  1.2300026e-02 -3.2540807e-04\n",
      "  9.3140936e-04  2.3238997e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 9.6186936e-01  1.1617874e-02  1.2019489e-02 -3.0541845e-04\n",
      "  9.4946974e-04  2.3612604e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 9.6176225e-01  1.2465493e-02  1.2178505e-02 -3.1211268e-04\n",
      "  9.5930009e-04  2.3342117e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " ---checking for moving objects---\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 9.6185702e-01  1.2471880e-02  1.2156363e-02 -3.1007943e-04\n",
      "  9.6141669e-04  2.3301356e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " ---checking for moving objects---\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 9.6187550e-01  1.2472168e-02  1.2141513e-02 -3.1064515e-04\n",
      "  9.5797487e-04  2.3280210e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " ---checking for moving objects---\n",
      "\n",
      " ---checking for perspective shift---\n",
      "6/6 [==============================] - 1s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 9.64163363e-01  1.21465195e-02  1.15701333e-02 -7.04580161e-05\n",
      "  1.01505651e-03  2.14505126e-03], shape=(6,), dtype=float32)\n",
      "pred_stds: \n",
      " tf.Tensor([0.00137136 0.00057491 0.00077217 0.00010863 0.00017974 0.0001235 ], shape=(6,), dtype=float32)\n",
      " L2: \n",
      " tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]], shape=(6, 6), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/derm/ASAR/v3/scan_match.py\", line 342, in <module>\n",
      "    print(\"\\n OXTS_ground_truth: \\n\", OXTS_ground_truth)\n",
      "NameError: name 'OXTS_ground_truth' is not defined\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'# python -m cProfile scan_match.py\\npython scan_match.py\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m# python -m cProfile scan_match.py\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpython scan_match.py\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2358\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2357\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2358\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'# python -m cProfile scan_match.py\\npython scan_match.py\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# python -m cProfile scan_match.py\n",
    "python scan_match.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test rejection\n",
    "# print(tf.shape(it1.residuals_full))\n",
    "# print(tf.shape(it1.U_i))\n",
    "\n",
    "res = it1.residuals_full\n",
    "U = it1.U_i\n",
    "L = it1.L_i\n",
    "\n",
    "res_compact = L @ tf.transpose(U, [0,2,1]) @ res[:,:,None]\n",
    "# print(res_compact)\n",
    "bad = tf.where(tf.math.abs(res_compact) > 0.05)\n",
    "print(bad[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pre-process points from ICET to feed to DNN\n",
    "\n",
    "#Get ragged tensor containing all points from each scan inside each sufficient voxel\n",
    "in1 = it1.inside1\n",
    "npts1 = it1.npts1\n",
    "in2 = it1.inside2\n",
    "npts2 = it1.npts2\n",
    "corr = it1.corr #indices of bins that have enough points from scan1 and scan2\n",
    "\n",
    "# print(tf.shape(in2.to_tensor()))\n",
    "\n",
    "#get indices of rag with >= 25 elements\n",
    "ncells = tf.shape(corr)[0].numpy() #num of voxels with sufficent number of points\n",
    "# print(tf.gather(npts2, corr))\n",
    "enough1 = tf.gather(in1, corr)\n",
    "enough2 = tf.gather(in2, corr)\n",
    "print(tf.shape(enough2.to_tensor())[0].numpy())\n",
    "# print(npts2)\n",
    "# print(corr)\n",
    "\n",
    "#init array to store indices\n",
    "idx1 = np.zeros([ncells ,25])\n",
    "idx2 = np.zeros([ncells ,25])\n",
    "\n",
    "#loop through each element of ragged tensor\n",
    "for i in range(ncells):\n",
    "    idx1[i,:] = tf.random.shuffle(enough1[i])[:25].numpy() #shuffle order and take first 25 elements\n",
    "    idx2[i,:] = tf.random.shuffle(enough2[i])[:25].numpy() #shuffle order and take first 25 elements\n",
    "\n",
    "idx1 = tf.cast(tf.convert_to_tensor(idx1), tf.int32)\n",
    "idx2 = tf.cast(tf.convert_to_tensor(idx2), tf.int32)\n",
    "\n",
    "# print(it1.cloud1_tensor)\n",
    "from1 = tf.gather(it1.cloud1_tensor, idx1)\n",
    "# from2 = tf.gather(it1.cloud2_tensor_OG, idx2)\n",
    "from2 = tf.gather(it1.cloud2_tensor, idx2)\n",
    "# print(from1)\n",
    "\n",
    "x_test = tf.concat((from1, from2), axis = 1)\n",
    "# np.savetxt('perspective_shift/ICET_KITTI_frame0.txt', tf.reshape(from1, [-1, 3]).numpy())\n",
    "# np.savetxt('perspective_shift/ICET_KITTI_frame1.txt', tf.reshape(from2, [-1, 3]).numpy())\n",
    "\n",
    "model = tf.keras.models.load_model(\"perspective_shift/KITTInet.kmod\")\n",
    "from_DNN = model.predict(x_test)\n",
    "# print(from_DNN)\n",
    "# print(np.shape(from_DNN))\n",
    "print(tf.math.reduce_mean(from_DNN, axis = 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify compact directions where ICET and DNN disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare per cell translation estimates between ICET and DNN on converged results\n",
    "dnnsoln = tf.convert_to_tensor(from_DNN)\n",
    "# n = 0 #cell idx\n",
    "# print(dnnsoln[n])\n",
    "# print(it1.residuals[n])\n",
    "\n",
    "# icetsoln = tf.gather(it1.residuals_full, it1.corr)\n",
    "icetsoln = tf.gather(it1.residuals, it1.corr) #test\n",
    "\n",
    "#align differences between solutions with the principal axis of ICET scan1\n",
    "L = it1.L #only from \"mu1_enough\", \"sigma1_enough\" -> why it's too small rn???\n",
    "U = it1.U\n",
    "print(tf.shape(U))\n",
    "# print(U)\n",
    "\n",
    "# print(it1.enough1) #voxel IDs from scan1 with enough points\n",
    "# print(it1.corr)    # voxel IDs from BOTH with enough points\n",
    "\n",
    "#TODO: \n",
    "#  1) get IDX of elements that are in both enough1 and corr\n",
    "#  2) use this to index U and L to get U_i and L_i\n",
    "# print(it1.enough1)\n",
    "both = tf.sets.intersection(it1.enough1[None,:], it1.corr[None,:]).values\n",
    "ans = tf.where(it1.enough1[:,None] == both)[:,0]\n",
    "# print(ans)\n",
    "\n",
    "#project into frame of principal axis of distribution from scan1, prune extended axis\n",
    "LUT = tf.matmul(L, tf.transpose(U, [0,2,1]))\n",
    "it_compact = tf.matmul(LUT, icetsoln[:,:,None])\n",
    "dnn_compact = tf.matmul(LUT, dnnsoln[:,:,None])\n",
    "# print(it_compact)\n",
    "# print(dnn_compact)\n",
    "\n",
    "#find where the largest difference in residuals are\n",
    "thresh = 0.1\n",
    "#be careful- not sure what this index corresponds to (may not be voxel ID)\n",
    "problem_voxels = tf.where(tf.math.abs(it_compact - dnn_compact) > thresh)[:,0]\n",
    "print(problem_voxels)\n",
    "problem_voxels = tf.unique(problem_voxels)[0] #get rid of repeated indices\n",
    "print(problem_voxels)\n",
    "\n",
    "\n",
    "#remove extended axis\n",
    "# print(dnnsoln - icetsoln)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test loading raw KITTI point clouds (not compensated for rolling shutter)\n",
    "fn = \"C:/kitti/2011_09_26/2011_09_26_drive_0005_raw/velodyne_points/data/0000000000.txt\"\n",
    "\n",
    "test = np.loadtxt(fn)[:,:3]\n",
    "print(np.shape(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn1 = \"C:/kitti/2011_09_26/2011_09_26_drive_0005_raw/velodyne_points/data/0000000115.txt\"\n",
    "fn1 = \"C:/kitti/2011_09_26/2011_09_26_drive_0005_raw/velodyne_points/data/%010d.txt\" %(1)\n",
    "print(fn1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#generate plots from radial distance measurements\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "purple = np.load(\"figure_dist_measurements2.npy\")[:,0].T\n",
    "green = np.load(\"figure_dist_measurements1.npy\")[:,0].T\n",
    "orange = np.load(\"figure_dist_measurements3.npy\")[:,0].T\n",
    "blue = np.load(\"figure_dist_measurements4.npy\")[:,0].T\n",
    "\n",
    "scale = 3\n",
    "\n",
    "# ax.set_xlim([0,20])\n",
    "ax.set_ylim([-1,3*scale + 1])\n",
    "ax.set_aspect(\"equal\")\n",
    "# ax.set_yticks(ticks = [])\n",
    "ax.set_yticks(labels = ['Case a', 'Case b', 'Case c', 'Case d'], ticks = [3*scale, 2*scale, 1*scale, 0.])\n",
    "ax.set_xlabel(\"Distance from sensor (m)\")\n",
    "# ax.set_title(\"Spread of points per bin\") # $\\it{n}$\")\n",
    "# ax.set_ylabel(\"bin\")\n",
    "\n",
    "ptSize = 15\n",
    "ax.scatter(purple, scale*3*np.ones(len(purple)), marker = '.', s = ptSize, c = [0.5,0.,0.5], label = 'a')\n",
    "ax.scatter(orange, scale*2*np.ones(len(orange)), marker = '.', s = ptSize, c = [0.8,0.5,0.1], label = 'b')\n",
    "ax.scatter(green, scale*1*np.ones(len(green)), marker = '.', s = ptSize, c = [0,0.5,0], label = 'c')\n",
    "ax.scatter(blue, np.zeros(len(blue)), marker = '.', s = ptSize, c = [0.2,0.2,0.8], label = 'd')\n",
    "\n",
    "patches = []\n",
    "rectb = Rectangle([25.65, 5.5], 7.5, 1, fill = False) #(xy, w, h)\n",
    "patches.append(rectb)\n",
    "rectc = Rectangle([10.65, 2.5], 1.1, 1, fill = False) #(xy, w, h)\n",
    "patches.append(rectc)\n",
    "rectd = Rectangle([4.6, -0.5], 1.1, 1, fill = False) #(xy, w, h)\n",
    "patches.append(rectd)\n",
    "\n",
    "pc = PatchCollection(patches)\n",
    "pc.set_edgecolor('black')\n",
    "pc.set_facecolor([0,0,0,0])\n",
    "ax.add_collection(pc)\n",
    "\n",
    "\n",
    "# from utils import get_cluster\n",
    "# test = tf.convert_to_tensor(green)[:,None]\n",
    "# # print(tf.shape(test))\n",
    "# bounds = get_cluster(test, mnp = 50, thresh = 0.3)\n",
    "# # print(\"bounds:\", bounds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify outlier cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2,1)\n",
    "nbins = 25\n",
    "nstd = 2\n",
    "\n",
    "# print(it.dx_i[:,0])\n",
    "# print(tf.math.reduce_sum(it.dx_i, axis = 0))\n",
    "# print(it.W)\n",
    "# print(it.H)\n",
    "# print(it.residuals[:,0])\n",
    "\n",
    "component = it1.residuals_full[:,1]\n",
    "print(tf.shape(component))\n",
    "# print(component)\n",
    "\n",
    "print(\"\\n before:\")\n",
    "mu = tf.math.reduce_mean(component)\n",
    "print(\"mean\", mu)\n",
    "sigma = tf.math.reduce_std(component)\n",
    "print(\"standard deviation\", sigma)\n",
    "bad_idx = tf.where( tf.math.abs(component) > mu + nstd*sigma )\n",
    "# print(\"bad idx\", bad_idx)\n",
    "good_idx = tf.where( tf.math.abs(component) < mu + nstd*sigma )\n",
    "# print(tf.gather(component, bad_idx))\n",
    "# ax.hist(it.dx_i[:,0], nbins);\n",
    "ax[0].hist(component, nbins);\n",
    "ax[0].set_xlabel(\"y_i - y0_i (forward translation error)\")\n",
    "ax[0].set_ylabel(\"frequency\")\n",
    "ax[0].set_title(\"All Distributions\")\n",
    "\n",
    "#test to make sure outliers are being removed correctly\n",
    "component = it1.residuals[:,1]\n",
    "print(tf.shape(component))\n",
    "# print(component)\n",
    "\n",
    "print(\"\\n after:\")\n",
    "mu = tf.math.reduce_mean(component)\n",
    "print(\"mean\", mu)\n",
    "sigma = tf.math.reduce_std(component)\n",
    "print(\"standard deviation\", sigma)\n",
    "\n",
    "ax[1].hist(component, nbins);\n",
    "ax[1].set_xlabel(\"y_i - y0_i (forward translation error)\")\n",
    "ax[1].set_ylabel(\"frequency\")\n",
    "ax[1].set_title(\"Best Fitting Distributions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "# print(it.residuals_full[:,0])\n",
    "edges = tf.linspace(-1.,1.,30)\n",
    "# print(edges)\n",
    "print(edges)\n",
    "\n",
    "bins_soln = tfp.stats.find_bins(it.residuals_full[:,0], edges)\n",
    "# print(bins_soln)\n",
    "\n",
    "good_idx = tf.where(bins_soln == 14)\n",
    "bad_idx = tf.where(bins_soln == 14)\n",
    "# print(bad_idx)\n",
    "# print(good_idx)\n",
    "# print(tf.gather(it.residuals_full[:,0], good_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[5, 6, 7, 8]])\n",
    "b = tf.constant([[8, 7, 10]])\n",
    "print(tf.sets.difference(a,b).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Useful Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import get_cluster\n",
    "\n",
    "#index of spike that each of the points from cloud 1 is occupying\n",
    "print(it1.bins_spike)\n",
    "\n",
    "occupied_spikes, idxs = tf.unique(it1.bins_spike)\n",
    "print(\"\\n occupied_spikes \\n\", occupied_spikes)\n",
    "temp =  tf.where(it1.bins_spike == occupied_spikes[:,None])\n",
    "rag = tf.RaggedTensor.from_value_rowids(temp[:,1], temp[:,0])\n",
    "idx_by_rag = tf.gather(it1.cloud1_tensor_spherical[:,0], rag)\n",
    "\n",
    "# rads = idx_by_rag[50,:] #single element from ragged tensor\n",
    "rads = tf.transpose(idx_by_rag.to_tensor()[:3,:])\n",
    "# rads = tf.transpose(idx_by_rag.to_tensor())\n",
    "# print(rads) #starts out unordered\n",
    "\n",
    "# #_________________________________________________________________\n",
    "fig, ax = plt.subplots(2,1)\n",
    "nbins = 25\n",
    "ax[0].hist(rads.numpy(), nbins, histtype = 'step');\n",
    "yax = tf.ones(tf.shape(rads), tf.float32) #plots everything on top of eachother\n",
    "yax = yax * tf.cast(tf.linspace(1, 0, tf.shape(rads)[1]), tf.float32)\n",
    "# print(tf.linspace(0, 1, tf.shape(rads)[1])[:,None] )\n",
    "ax[1].plot(rads,yax, 'b.', markersize = 3)\n",
    "# #_________________________________________________________________\n",
    "\"\"\n",
    "bounds = get_cluster(rads)\n",
    "print(\"\\n Bounds \\n\", bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros([1,2])\n",
    "b = np.ones([3,2])\n",
    "print(np.append(b, a, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_r = bounds[6,:]\n",
    "print(edges_r)\n",
    "pts = tf.cast(tf.convert_to_tensor(c1[:,1]), tf.float64)\n",
    "print(pts)\n",
    "\n",
    "bins_r = tfp.stats.find_bins(pts, edges_r)\n",
    "print(bins_r)\n",
    "#get rid of NaNs\n",
    "nonnan = 1 - tf.cast(tf.math.is_nan(bins_r), tf.float32)\n",
    "idxnonan = tf.where(nonnan == 1)\n",
    "print(tf.gather(bins_r, idxnonan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test load Ford Campus Dataset 1 sample\n",
    "# import h5py\n",
    "import mat4py\n",
    "fn1 = 'E:/Ford/IJRR-Dataset-1-subset/SCANS/Scan1000.mat'\n",
    "dat1 = mat4py.loadmat(fn1)\n",
    "\n",
    "SCAN1 = dat1['SCAN']\n",
    "pc1 = np.transpose(np.array(SCAN1['XYZ']))\n",
    "\n",
    "print(np.shape(pc1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ground truth transformation from KITTI poses file (full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load transformation matrices\n",
    "# filename = \"E:/KITTI/dataset/2011_09_26/2011_09_26_drive_00_sync/poses/00.txt\"\n",
    "filename ='/media/derm/06EF-127D1/KITTI/dataset/2011_09_26/2011_09_26_drive_00_sync/poses/00.txt'\n",
    "full_poses = np.loadtxt(filename)\n",
    "mat_full = np.reshape(full_poses, [-1,3,4])\n",
    "\n",
    "frame = 4500 #300 good\n",
    "t_i = mat_full[frame, :, :]\n",
    "t_next = mat_full[frame+1, :, :]\n",
    "print(t_i)\n",
    "# print(t_next - t_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rotation matrices\n",
    "from utils import R2Euler\n",
    "Rmat = tf.convert_to_tensor(mat_full[:,:,:3])\n",
    "euls = R2Euler(Rmat)\n",
    "drot = euls[:,frame+1] - euls[:,frame]\n",
    "#re-order to match ICET output\n",
    "drot = np.array([-drot[2], drot[0], drot[1] ])\n",
    "# print(\"change in rotation:\", drot)\n",
    "\n",
    "#get translation in vehicle body frame \n",
    "dpos_xyz = mat_full[frame+1,:,3] - mat_full[frame,:,3]\n",
    "# print(dpos_xyz)\n",
    "dpos_bf = np.array([np.sqrt(dpos_xyz[0]**2 + dpos_xyz[2]**2), 0, dpos_xyz[1]])\n",
    "# print(\"translation in vehicle body frame:\", dpos_bf)\n",
    "\n",
    "X = np.append(dpos_bf, drot)\n",
    "#assume zero vertical movement between frames???\n",
    "X[2] = 0\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot trajectory\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mat_full[0,2,3], mat_full[0,0,3], 'ro') #mark start in red\n",
    "for i in range(frame):\n",
    "    ax.plot(mat_full[i,2,3], mat_full[i,0,3], 'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot body frame velocity\n",
    "v_xyz = np.diff(mat_full[:,:,3], axis = 0)\n",
    "v_xyz = np.array([v_xyz[:,2], v_xyz[:,0], v_xyz[:,1] ]).T #need to reorder\n",
    "# print(v_xyz[10])\n",
    "vf = np.sqrt(v_xyz[:,0]**2  + v_xyz[:,1]**2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(vf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True transformation between frames (for mini KITTI datsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get true transformation between frames (for mini KITTI datsets)\n",
    "from metpy.calc import lat_lon_grid_deltas\n",
    "idx = 51\n",
    "poses0 = dataset.oxts[idx] #<- ID of 1st scan\n",
    "poses1 = dataset.oxts[idx+1] #<- ID of 2nd scan\n",
    "lat0 = poses0.packet.lat\n",
    "lon0 = poses0.packet.lon\n",
    "alt0 = poses0.packet.alt\n",
    "lat1 = poses1.packet.lat\n",
    "lon1 = poses1.packet.lon\n",
    "alt1 = poses1.packet.alt\n",
    "\n",
    "# print(lat0)\n",
    "# print(lon0)\n",
    "\n",
    "dx_oxts, dy_oxts = lat_lon_grid_deltas(np.array([lon0,lon1]), np.array([lat0, lat1]))\n",
    "# print(dx_oxts, dy_oxts) \n",
    "dx_oxts = dx_oxts[0,0].magnitude\n",
    "dy_oxts = dy_oxts[0,0].magnitude\n",
    "dz_oxts = (alt0-alt1)\n",
    "droll_oxts = (poses0.packet.roll - poses1.packet.roll)\n",
    "dpitch_oxts = (poses0.packet.pitch - poses1.packet.pitch)\n",
    "dyaw_oxts = (poses0.packet.yaw - poses1.packet.yaw)\n",
    "\n",
    "rot = poses1.T_w_imu[:3,:3] #trying this\n",
    "\n",
    "dxyz_oxts = np.array([[dx_oxts, dy_oxts, dz_oxts]])\n",
    "dxyz_lidar = dxyz_oxts.dot(rot)\n",
    "print(dxyz_lidar)\n",
    "\n",
    "dt = 0.10\n",
    "# dt = 0.1037 #mean time between lidar samples\n",
    "from_vel = np.array([[poses1.packet.vf*dt, poses1.packet.vl*dt, poses1.packet.vu*dt, -poses1.packet.wf*dt, -poses1.packet.wl*dt, -poses1.packet.wu*dt]])\n",
    "print(from_vel)\n",
    "\n",
    "# print(poses1.packet.vel_accuracy)\n",
    "# print((dataset.timestamps[idx+1] - dataset.timestamps[idx]).microseconds/(10e5))\n",
    "\n",
    "# # print(np.shape(dataset.timestamps)[0])\n",
    "# # tvec = np.zeros(np.shape(dataset.timestamps)[0])\n",
    "tvec = np.zeros(149)\n",
    "# # for i in range(np.shape(dataset.timestamps)[0] - 1):\n",
    "for i in range(149):\n",
    "#     print((dataset.timestamps[i+1] - dataset.timestamps[i]).microseconds/(10e5))\n",
    "    tvec[i] = (dataset.timestamps[i+1] - dataset.timestamps[i]).microseconds/(10e5)\n",
    "# print(tvec)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[1,2,3,4]])\n",
    "test = np.append(test,np.array([[0,2,3,4]]),axis = 0)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test batch rotation matrix conversions\n",
    "from utils import R_tf\n",
    "\n",
    "print(R_tf(tf.Variable([[0., 0., 1.]])), \"\\n\")\n",
    "\n",
    "angs = tf.Variable([[0., 0., 1.], [0., 0., 1.]])\n",
    "# angs = tf.Variable([[0., 0., 1.]])\n",
    "print(angs)\n",
    "\n",
    "rots = R_tf(angs) \n",
    "print(rots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad tensors to get them to the same length\n",
    "#to fix bug in get_U_and_L()\n",
    "\n",
    "t1 = tf.ones([3,8], tf.int32)\n",
    "print(t1)\n",
    "t2 = tf.ones([3,7], tf.int32)\n",
    "print(t2)\n",
    "\n",
    "bofa = tf.sets.intersection(t1, t2).values\n",
    "print(bofa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test- workaround for in place tensor operations\n",
    "indices = tf.cast(tf.constant([1, 2, 3, 5]), tf.int32)[:,None]\n",
    "print(\"indices\", indices)\n",
    "updates = tf.ones(tf.shape(indices))\n",
    "print(\"updates\", updates)\n",
    "shape = tf.constant([7, 1])\n",
    "print(\"shape\", shape)\n",
    "\n",
    "b = tf.scatter_nd(indices, updates, shape)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test incramenting strings for file names\n",
    "for a in range(10):\n",
    "    test = \"Scan%04d.mat\" %(a+1000)\n",
    "    test =  'E:/Ford/IJRR-Dataset-1-subset/SCANS/' + test\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results of ICET estimates on KITTI lidar point clouds vs GPS/INS baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "font = {'fontname':'Times New Roman'}\n",
    "\n",
    "#v8 is the best so far...\n",
    "ICET_estimates = np.loadtxt(\"ICET_estimates_v8.txt\")\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_v10.txt\")\n",
    "# OXTS_baseline = np.loadtxt(\"OXTS_baseline_v10.txt\")\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_v13.txt\")\n",
    "# OXTS_baseline = np.loadtxt(\"OXTS_baseline_v13.txt\")\n",
    "# ICET_estimates = np.loadtxt(\"ICET_estimates_v18.txt\")\n",
    "OXTS_baseline = np.loadtxt(\"OXTS_baseline_v17.txt\")\n",
    "BC = np.loadtxt(\"Before_correction_v18.txt\")\n",
    "\n",
    "# vf_from_matlab = np.loadtxt(\"vf.txt\")\n",
    "# vf_from_matlab = np.append(vf_from_matlab, 0)\n",
    "# # print(vf_from_matlab)\n",
    "# OXTS_baseline[:,0] = vf_from_matlab\n",
    "\n",
    "# OXTS_baseline = np.loadtxt(\"OXTS_baseline_gps.txt\")\n",
    "\n",
    "# OXTS_baseline[:,3:] = OXTS_baseline[:,3:]/0.1*0.1037\n",
    "# OXTS_baseline = OXTS_baseline/0.1*0.1037\n",
    "OXTS_baseline = (OXTS_baseline/0.1*0.1037 + OXTS_baseline)/2 #test\n",
    "\n",
    "# ICET_estimates[:,0] = ICET_estimates[:,0]/tvec*0.1\n",
    "\n",
    "#fix sign errors\n",
    "ICET_estimates[:,1] = -ICET_estimates[:,1]\n",
    "ICET_estimates[:,3:] = -ICET_estimates[:,3:]\n",
    "BC[:,1] = -BC[:,1]\n",
    "BC[:,3:] = -BC[:,3:]\n",
    "\n",
    "style1 = 'b-'\n",
    "style2 = 'r-'\n",
    "style3 = 'b--'\n",
    "\n",
    "fig, ax = plt.subplots(3,2, constrained_layout = True)\n",
    "ax[0,0].plot(BC[:,0], style3, label = 'ICET- before', alpha = 0.3)\n",
    "ax[0,0].plot(ICET_estimates[:,0], style1, label = 'ICET- after')\n",
    "ax[0,0].plot(OXTS_baseline[:,0], style2, label = 'GPS/INS Baseline')\n",
    "# ax[0,0].plot(np.sqrt(OXTS_baseline[:,0]**2 + OXTS_baseline[:,1]**2), style2, label = 'GPS/INS Baseline')\n",
    "ax[0,0].set_title(\"change in x per frame\", **font)\n",
    "ax[0,0].set_ylabel(\"dx (m)\", **font)\n",
    "ax[0,0].legend(loc = 'best')\n",
    "ax[0,0].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[1,0].plot(BC[:,1], style3, alpha = 0.3)\n",
    "ax[1,0].plot(ICET_estimates[:,1], style1, lw = 1)\n",
    "ax[1,0].plot(-OXTS_baseline[:,1], style2, lw = 1)\n",
    "# ax[1,0].plot(np.arange(n//2, np.shape(ICET_estimates)[0] - n//2 ), moving_avg(OXTS_baseline[:,1], n),  style2, lw = 1)\n",
    "ax[1,0].set_title(\"change in y per frame\", **font)\n",
    "ax[1,0].set_ylabel(\"dy (m)\", **font)\n",
    "ax[1,0].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[2,0].plot(BC[:,2], style3, alpha = 0.3)\n",
    "ax[2,0].plot(ICET_estimates[:,2], style1, lw = 1)\n",
    "ax[2,0].plot(OXTS_baseline[:,2], style2, lw = 1)\n",
    "# ax[2,0].plot(np.arange(n//2, np.shape(ICET_estimates)[0] - n//2 ), moving_avg(OXTS_baseline[:,2], n),  style2, lw = 1)\n",
    "ax[2,0].set_title(\"change in z per frame\", **font)\n",
    "ax[2,0].set_ylabel(\"dz (m)\", **font)\n",
    "ax[2,0].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[0,1].plot(BC[:,3], style3, alpha = 0.3)\n",
    "ax[0,1].plot(ICET_estimates[:,3], style1, lw = 1)\n",
    "ax[0,1].plot(OXTS_baseline[:,3], style2, lw = 1)\n",
    "ax[0,1].set_title(\"change in roll per frame\", **font)\n",
    "ax[0,1].set_ylabel(\"droll (rad)\", **font)\n",
    "ax[0,1].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[1,1].plot(BC[:,4], style3, alpha = 0.3)\n",
    "ax[1,1].plot(ICET_estimates[:,4], style1, lw = 1)\n",
    "ax[1,1].plot(OXTS_baseline[:,4], style2, lw = 1)\n",
    "ax[1,1].set_title(\"change in pitch per frame\", **font)\n",
    "ax[1,1].set_ylabel(\"dpitch (rad)\", **font)\n",
    "ax[1,1].set_xlabel(\"frame\", **font)\n",
    "\n",
    "ax[2,1].plot(BC[:,5], style3, alpha = 0.3)\n",
    "ax[2,1].plot(ICET_estimates[:,5], style1, lw = 1)\n",
    "ax[2,1].plot(OXTS_baseline[:,5], style2, lw = 1)\n",
    "ax[2,1].set_title(\"change in yaw per frame\", **font)\n",
    "ax[2,1].set_ylabel(\"dyaw (rad)\", **font)\n",
    "ax[2,1].set_xlabel(\"frame\", **font)\n",
    "\n",
    "# fig.tight_layout(h_pad = 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot error between ICET and absolute position\n",
    "plt.rc('font',family='Times New Roman')\n",
    "fig3, ax3 = plt.subplots(1,1)\n",
    "\n",
    "# ICET_pred_stds = np.loadtxt(\"ICET_pred_stds_v8.txt\")\n",
    "ICET_pred_stds = np.loadtxt(\"ICET_pred_stds_v13.txt\")\n",
    "\n",
    "# ICET_pred_stds = (2*np.sqrt(ICET_pred_stds))**2\n",
    "# ICET_pred_stds = np.sqrt(2*(ICET_pred_stds**2))\n",
    "\n",
    "\n",
    "#which component to look at\n",
    "# c = 3 #roll\n",
    "# c = 4 #pitch\n",
    "# c = 5 #yaw\n",
    "c = 0 # x (forward movement)\n",
    "\n",
    "diffx = OXTS_baseline[:,c] - ICET_estimates[:,c]\n",
    "diffx_BC = OXTS_baseline[:,c] - BC[:,c]\n",
    "\n",
    "# print(abs(diffx))\n",
    "print(\"correlation coefficient \\n\", np.corrcoef(abs(diffx), ICET_pred_stds[:,0]))\n",
    "\n",
    "#flip sign when looking at yaw\n",
    "if c ==5:\n",
    "    diffx = -diffx \n",
    "    \n",
    "cum_err = np.zeros(np.shape(ICET_pred_stds))\n",
    "cum_diffx = np.zeros(np.shape(diffx))\n",
    "cum_diffx_BC = np.zeros(np.shape(diffx_BC))\n",
    "\n",
    "for i in range(np.shape(ICET_pred_stds)[0]):\n",
    "    cum_err[i,:] = np.sum(ICET_pred_stds[:i,:]**2, axis = 0)\n",
    "    #add in baseline OXTS 1-sigma errors\n",
    "#     cum_err[i,:] += np.sqrt(2)*np.array([0.05,0.05,0.1,0.0005,0.0005,0.001])**2\n",
    "    cum_err[i,:] += np.sqrt(2)*np.array([0.08,0.08,0.1,0.0005,0.0005,0.001745])**2\n",
    "    cum_err[i,:] = np.sqrt(cum_err[i,:]) \n",
    "    \n",
    "for j in range(np.shape(diffx)[0]):\n",
    "    cum_diffx[j] = np.sum(diffx[:j]) \n",
    "    cum_diffx_BC[j] = np.sum(diffx_BC[:j]) \n",
    "\n",
    "# #old (error for each individual timestep)------------------------\n",
    "ax3.plot(diffx, label = 'GPS/INS - ICET (after bias reduction)')\n",
    "ax3.plot(diffx_BC, 'b--', label = 'GPS/INS - ICET (before bias reduction)', alpha = 0.3)\n",
    "ax3.fill_between(np.linspace(0,150,np.shape(ICET_pred_stds)[0]), -2*ICET_pred_stds[:,c], 2*ICET_pred_stds[:,c], \n",
    "                 color = (0.5,0.5,0.5,0.4), label = 'ICET Predicted 2σ Error Bounds')\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "# # #new (accumulated differences in error)--------------------------\n",
    "# # ax3.plot(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), cum_diffx_with_ground, label = 'GPS/INS - ICET')\n",
    "# ax3.plot(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), cum_diffx, label = 'GPS/INS - ICET (after correction)')\n",
    "# ax3.plot(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), cum_diffx_BC, 'b--', alpha = 0.3, label = 'GPS/INS - ICET (before correction)')\n",
    "# ax3.fill_between(np.linspace(0,15,np.shape(ICET_pred_stds)[0]), -2*cum_err[:,c], 2*cum_err[:,c], \n",
    "#                  color = (0,0,1,0.2), label = 'Predicted 2σ Error Bounds')\n",
    "# # # --------------------------------------------------------------------\n",
    "\n",
    "# ax3.legend(loc = 'lower left')\n",
    "ax3.legend(loc = 'best')\n",
    "ax3.set_title(\"Predicted vs Actual Error in x\")\n",
    "ax3.set_xlabel(\"time (s)\", **font)\n",
    "ax3.set_ylabel(\"GPS/INS Baseline x - Odometry Estimate x (m)\", **font)\n",
    "# ax3.set_title(\"Predicted vs Actual Error in yaw\")\n",
    "# ax3.set_xlabel(\"time (s)\", **font)\n",
    "# ax3.set_ylabel(\"GPS/INS Baseline yaw - Odometry Estimate yaw (rad)\", **font)\n",
    "# ax3.set_ylim(-0.032,0.045)\n",
    "# ax3.set_ylim([-0.07,0.07])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test where points are inside spherical cell...\n",
    "# print(it.cloud1_tensor_spherical)\n",
    "maxtheta = tf.constant([[0.2],[0.7]])\n",
    "maxr = tf.constant([[0.5],[2.]])\n",
    "\n",
    "ans1 = tf.greater(it.cloud1_tensor_spherical[:,1], maxtheta)\n",
    "# print(ans1)\n",
    "ans2 = tf.less(it.cloud1_tensor_spherical[:,0], maxr)\n",
    "# print(ans2)\n",
    "combined = tf.Variable([ans1, ans2])\n",
    "# print(combined)\n",
    "ans3 = tf.math.reduce_all(combined, axis = 1)\n",
    "\n",
    "print(ans3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate each element of an n*1 vector 3 times\n",
    "t = tf.linspace(0,5,6)[:,None]\n",
    "print(t)\n",
    "\n",
    "test  = tf.tile(t, [3,1])\n",
    "# print(test)\n",
    "test2 = tf.reshape(tf.transpose(tf.reshape(test, [3,-1])), [-1,1])\n",
    "print(test2)\n",
    "test3 = tf.reshape(tf.transpose(tf.reshape(test, [3,-1])), [-1,3])\n",
    "print(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate each element of an n*3 vector 3 times\n",
    "t = tf.linspace(1,5,5)\n",
    "t = tf.transpose(tf.Variable([t, 2*t, 3*t]))\n",
    "print(t)\n",
    "\n",
    "test  = tf.tile(t, [4,1])\n",
    "# print(test)\n",
    "\n",
    "test = tf.reshape(tf.transpose(test), [3, 4, -1])\n",
    "# print(test)\n",
    "\n",
    "test = tf.transpose(test, [2,1,0])\n",
    "# print(test)\n",
    "\n",
    "test = tf.reshape(test, [-1,3])\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MC sim to compare performance estimation in \"realistic\" scenes with flat vs curved surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "c1_OG = np.loadtxt(\"scene2_scan1.txt\", dtype = float) #thin cylinders\n",
    "c2_OG = np.loadtxt(\"scene2_scan2.txt\", dtype = float)\n",
    "# c1_OG = np.loadtxt(\"scene3_scan1.txt\", dtype = float) #rectangles\n",
    "# c2_OG = np.loadtxt(\"scene3_scan2.txt\", dtype = float)\n",
    "# c1 = np.loadtxt(\"scene4_scan1.txt\", dtype = float) #cylinders\n",
    "# c2 = np.loadtxt(\"scene4_scan2.txt\", dtype = float)\n",
    "\n",
    "xvec = np.zeros([epochs, 6])\n",
    "pred_stds = np.zeros([epochs, 6])\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"\\n -------------- Epoch\", i, \"--------------------\")\n",
    "    #add noise (if not generated when point clouds were created)\n",
    "    c1 = c1_OG + 0.02*np.random.randn(np.shape(c1_OG)[0], 3)\n",
    "    c2 = c2_OG + 0.02*np.random.randn(np.shape(c2_OG)[0], 3)  \n",
    "\n",
    "    it = ICET(cloud1 = c1, cloud2 = c2, fid = 70, niter = 20, draw = False, group = 2, RM = True)\n",
    "    xvec[i] = it.X\n",
    "    pred_stds[i] = it.pred_stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_soln_err = np.array([-0.5, 0, 0, 0, 0, 0,]) - np.mean(xvec, axis = 0)\n",
    "\n",
    "print(\"Smaller Clylindrical features, no occlusion, no outlier rejection, n=10:\\n\")\n",
    "# print(\"Rectangular features, no occlusion, no outlier rejection, n=50:\\n\")\n",
    "\n",
    "print(\"mean solution error: \\n\", mean_soln_err)\n",
    "soln_std = np.std(xvec, axis = 0)\n",
    "print(\"\\n experimentally determined std: \\n\", soln_std)\n",
    "mean_pred_std = np.mean(pred_stds, axis = 0)\n",
    "print(\"\\n predicted std: \\n\", mean_pred_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "nbins = 10\n",
    "ax.hist(xvec[:,0], nbins)\n",
    "# ax.set_title(\"Clylindrical features, no occlusion, no outlier rejection, n=50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing bad indices from tensor (used in test where we replace dz with dnn soln)\n",
    "\n",
    "test = tf.random.normal([4,3])\n",
    "print(test)\n",
    "\n",
    "bad_idx = tf.constant([1,3])[None,:]\n",
    "print(bad_idx)\n",
    "all_idx = tf.cast(tf.linspace(0,tf.shape(test)[0].numpy()-1,tf.shape(test)[0].numpy()), tf.int32)[None,:]\n",
    "print(all_idx)\n",
    "\n",
    "good_idx = tf.sets.difference(all_idx, bad_idx).values\n",
    "print(good_idx)\n",
    "print(tf.gather(test, good_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
