{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5359c9b",
   "metadata": {},
   "source": [
    "# Implementing LiDAR-NeRF with Ray Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load requirements for working with PCs\n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 20*1024\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "import sys\n",
    "import os\n",
    "current = os.getcwd()\n",
    "parent_directory = os.path.dirname(current)\n",
    "sys.path.append(parent_directory)\n",
    "sys.path.append(parent_directory+\"/point_cloud_rectification\")\n",
    "from ICET_spherical import ICET\n",
    "from linear_corrector import LC\n",
    "\n",
    "from utils import R_tf\n",
    "from metpy.calc import lat_lon_grid_deltas\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "import trimesh\n",
    "\n",
    "\n",
    "from pillow_heif import register_heif_opener\n",
    "from matplotlib import pyplot as p\n",
    "from colmapParsingUtils import *\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a9661",
   "metadata": {},
   "source": [
    "## Load Gazebo Point Clouds and Convert to image-like data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images =  17 #18 max for town, 11 max for old box, 16 for new box\n",
    "n_rots = 8 #8 #1 #number of side by side frames to add in\n",
    "# n_rots = 260 #130 for 1/16th #60 for 1/8th (needs to be much bigger when we are splitting up range images vertically)\n",
    "\n",
    "#Get LiDAR Intrinsics\n",
    "#Simulated LIDAR Sensor -- (look for <min_angle> in velodyne.world file in ROS package)\n",
    "phimin = -0.53529248 #rad\n",
    "phimax = 0.18622663 #rad\n",
    "#achieve roughly same horizonal and vertical fov\n",
    "vert_fov = np.rad2deg(phimax-phimin)\n",
    "# images = np.zeros([n_images, 64, image_width, 1])\n",
    "# images = np.zeros([n_images*n_rots, 64, 64, 1]) #squares\n",
    "# images = np.zeros([n_images*n_rots, 64, 2, 1]) #2 pixels wide\n",
    "poses = np.zeros([n_images*n_rots,4,4])\n",
    "# non_returns = np.zeros([n_images*n_rots, 64, 64, 1]) #squares\n",
    "images = np.ones([n_images*n_rots, 64, 64, 2]) #depth image and raydrop\n",
    "\n",
    "#focal length (in pixels) = Image Size / (2 tan(FOV/2)) #needs to be array!\n",
    "focal = np.array(np.shape(images)[1]/(2*np.tan((phimax-phimin)/2))) #default image size\n",
    "H, W = images.shape[1:3]\n",
    "print(focal, H, W)\n",
    "\n",
    "gtfn = \"gazebo_scene/ground_truth.npy\" #town\n",
    "# gtfn = \"gazebo_scene2/ground_truth.npy\" #box v1\n",
    "# gtfn = \"gazebo_scene2/box_ground_truth.npy\" #box v2\n",
    "# gtfn = \"gazebo_scene2/box2_ground_truth.npy\" #box v3\n",
    "sensor_pose = np.load(gtfn)\n",
    "# print(sensor_pose)\n",
    "print(np.shape(sensor_pose))\n",
    "\n",
    "for i in range(n_images):\n",
    "#     for j in range(n_rots):\n",
    "    #load point cloud file\n",
    "    pcfn = \"gazebo_scene/scan\" + str(i+2) + \".npy\" #town\n",
    "#     pcfn = \"gazebo_scene2/scan\" + str(i+2) + \".npy\" #box v1\n",
    "#         pcfn = \"gazebo_scene2/box_scan\" + str(i+2) + \".npy\" #box v2\n",
    "#     pcfn = \"gazebo_scene2/box2_scan\" + str(i+2) + \".npy\" #box v3\n",
    "    pc = np.load(pcfn)\n",
    "    \n",
    "    #cap inf values TEMP\n",
    "    pc[pc[:,0]>64] = 100 #64 #0\n",
    "    pc[pc[:,0]<-64] = 100 #64 #0\n",
    "    \n",
    "    #convert to image array\n",
    "    pc_spherical = LC.c2s(LC,pc).numpy() #[r, theta, phi]\n",
    "\n",
    "    for j in range(n_rots):\n",
    "\n",
    "        pcs = np.reshape(pc_spherical, [-1,64,3])\n",
    "        pcs = np.flip(pcs, axis = 1) #flip vertical\n",
    "        pcs = np.flip(pcs, axis = 0) #flip horizontal to look at first clockwise patch of scan sweep \n",
    "\n",
    "        #resize image to 64x64\n",
    "        image_width = int((vert_fov/360)*np.shape(pcs)[0])\n",
    "        \n",
    "        pcs = pcs[j*image_width:(j+1)*image_width,:,0].T ##square\n",
    "#         pcs = pcs[(j*image_width//32):((j+1)*image_width//32),:,0].T #32nds\n",
    "\n",
    "        pcs = cv2.resize(pcs, (64, 64), cv2.INTER_NEAREST) #keep square\n",
    "        #preserve aspect ratio and focal length but just take middle\n",
    "#         pcs = pcs[:,31:33] #uncomment for 32nds\n",
    "        images[j+(i*n_rots),:,:,0] = pcs #save depth information to first channel\n",
    "    \n",
    "\n",
    "        a = np.argwhere(pcs > 64)\n",
    "#         non_returns[j+(i*n_rots),a[:,0],a[:,1],0] = 1\n",
    "        images[j+(i*n_rots),a[:,0],a[:,1],1] = 0 #save raydrop mask to 2nd channel\n",
    "    \n",
    "        #get sensor transformation matrix\n",
    "        rotm = np.eye(4)\n",
    "        rotm[0,3] = sensor_pose[i+1,0] #x\n",
    "        rotm[2,3] = -sensor_pose[i+1,1] #y\n",
    "        rotm[1,3] = -sensor_pose[i+1,2] #z\n",
    "        rotm[:3,:3] = R.from_quat(sensor_pose[i+1,3:]).as_matrix() \n",
    "        \n",
    "        #account for image crop in rotation\n",
    "        crop_angle = -(phimax-phimin)/2 - j*(phimax-phimin) #square\n",
    "#         crop_angle = -(phimax-phimin)/64 - j*(phimax-phimin)/32 #2-pixels wide\n",
    "        rotm_crop = R.from_euler('xyz', [0,-crop_angle,0]).as_matrix()\n",
    "        rotm[:3,:3] = rotm[:3,:3] @ rotm_crop\n",
    "\n",
    "        #also need to account for the fact that the LIDAR beam isn't actually centered at horizon\n",
    "        sensor_elevation_zero_rotm = R.from_euler('xyz', [(phimin+phimax)/2,0,0]).as_matrix() #correct\n",
    "        rotm[:3,:3] = rotm[:3,:3] @ sensor_elevation_zero_rotm\n",
    "\n",
    "        #flip x and z axis\n",
    "        rotm[0,-1], rotm[2,-1] = rotm[2,-1], rotm[0,-1] \n",
    "\n",
    "        # flip sign of y and z axis\n",
    "        rotm[0:3,2] *= -1 \n",
    "        rotm[0:3,1] *= -1\n",
    "        rotm = rotm[[1,0,2,3],:]\n",
    "        rotm[2,:] *= -1 # flip whole world upside down\n",
    "#         #translate all frames above xy plane\n",
    "        rotm[2,-1] += 25 #for town  \n",
    "#         rotm[2,-1] += 5 #for box (old)\n",
    "#         rotm[2,-1] += 7 #for box (new)\n",
    "\n",
    "# #         scale translations\n",
    "#         rotm[:3,-1] *= 0.1\n",
    "#         #scale ranges measurements?\n",
    "#         images[j+(i*n_rots),:,:,0] *= 0.1\n",
    "        \n",
    "        poses[j+(i*n_rots)] = rotm\n",
    "\n",
    "fig, ax = p.subplots(1,4)\n",
    "ax[0].imshow(images[-4,:,:,0], cmap = \"gray\", norm='log')\n",
    "ax[1].imshow(images[-3,:,:,0], cmap = \"gray\", norm='log')\n",
    "ax[2].imshow(images[-2,:,:,0], cmap = \"gray\", norm='log')\n",
    "ax[3].imshow(images[-1,:,:,0], cmap = \"gray\", norm='log')\n",
    "images = images.astype(np.float32)\n",
    "poses = poses.astype(np.float32)\n",
    "\n",
    "# test on one only\n",
    "testimg, testpose = images[(n_images*n_rots)-1], poses[(n_images*n_rots)-1]\n",
    "images = images[:((n_images*n_rots)-1),...,:3]\n",
    "poses = poses[:((n_images*n_rots)-1)]\n",
    "\n",
    "# #90/10 split\n",
    "# cutoff = (n_images*9)//10\n",
    "# print(cutoff)\n",
    "# testimg, testpose = images[cutoff:], poses[cutoff:]\n",
    "# images = images[:cutoff,...,:3]\n",
    "# poses = poses[:cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b72844",
   "metadata": {},
   "source": [
    "##  Train Vanilla NeRF on depth image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92236a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posenc(x):\n",
    "  rets = [x]\n",
    "  for i in range(L_embed):\n",
    "    for fn in [tf.sin, tf.cos]:\n",
    "      rets.append(fn(2.**i * x))\n",
    "  return tf.concat(rets, -1)\n",
    "\n",
    "L_embed = 6 #6\n",
    "embed_fn = posenc\n",
    "# L_embed = 0\n",
    "# embed_fn = tf.identity\n",
    "\n",
    "def init_model(D=8, W=256): #8,256\n",
    "    relu = tf.keras.layers.ReLU()    \n",
    "    dense = lambda W=W, act=relu : tf.keras.layers.Dense(W, activation=act)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(3 + 3*2*L_embed)) \n",
    "    outputs = inputs\n",
    "    for i in range(D):\n",
    "        outputs = dense()(outputs)\n",
    "        if i%4==0 and i>0:\n",
    "            outputs = tf.concat([outputs, inputs], -1)\n",
    "            \n",
    "    #umcomment below for non-raydrop model\n",
    "#     outputs = dense(1, act=None)(outputs) #[depth] output only\n",
    "##     outputs = dense(2, act=None)(outputs) #[depth, ray drop] #does not work\n",
    "#     model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #extend \"color\" channels to small MLP after output of density channel\n",
    "    sigma_channel = dense(1, act=None)(outputs)\n",
    "    \n",
    "    #start ray drop branch\n",
    "    rd_start = tf.concat([outputs, inputs], -1)\n",
    "    rd_channel = dense(128, act=relu)(outputs)\n",
    "#     rd_channel = dense(128, act=relu)(rd_channel)\n",
    "    rd_channel = dense(128, act=relu)(rd_channel)\n",
    "    rd_channel = dense(1, act=tf.keras.activations.sigmoid)(rd_channel)\n",
    "    out = tf.concat([sigma_channel, rd_channel], -1)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
    "#     #pinhold camera projection model (TinyNeRF)~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#     dirs = tf.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -tf.ones_like(i)], -1)\n",
    "#     rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "\n",
    "    #spherical projection model (ours)~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    phimin = -0.53529248 #rad\n",
    "    phimax = 0.18622663 #rad\n",
    "\n",
    "    dirs_test = tf.stack([-tf.ones_like(i), \n",
    "                          -(j-(W*.5))/(focal) - (np.pi/2) - phimax, \n",
    "                          np.arctan((i-(H*.5))/(focal))], -1) #square\n",
    "#                           np.arctan((i-(H*.5))/(focal*32))], -1) #2 pixels wide\n",
    "    \n",
    "    dirs_test = tf.reshape(dirs_test,[-1,3])\n",
    "    #[r, theta, phi] --> [r, phi, theta]\n",
    "    dirs_test = LC.s2c(LC, tf.transpose(tf.Variable([dirs_test[:,0], dirs_test[:,2], dirs_test[:,1]], dtype = tf.float32)))\n",
    "    \n",
    "    rotm = R.from_euler('xyz', [0,-(np.pi/2)+(((phimax+phimin))/2),0]).as_matrix()\n",
    "    dirs_test = dirs_test @ rotm\n",
    "    dirs_test = dirs_test @ tf.transpose(c2w[:3,:3])\n",
    "    dirs = dirs_test @ (c2w[:3,:3] \n",
    "                          @ R.from_euler('xyz', [0,0,np.pi/2]).as_matrix()\n",
    "                          @ np.linalg.pinv(c2w[:3,:3]) )\n",
    "\n",
    "    dirs = tf.reshape(dirs, [64,64,3]) #square\n",
    "#     dirs = tf.reshape(dirs, [64,2,3]) #2 pixels wide\n",
    "    \n",
    "    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * np.eye(3), -1) \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
    "        \n",
    "    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, rand=True):\n",
    "\n",
    "    def batchify(fn, chunk=1024*512): #1024*128 converged for box3 #1024*32 in TinyNeRF\n",
    "        return lambda inputs : tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    \n",
    "    # Compute 3D query points\n",
    "    z_vals = tf.linspace(near, far, N_samples) \n",
    "    if rand:\n",
    "      z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "    #[image_height, image_width, batch_size, 3]\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "    \n",
    "    # Run network\n",
    "    pts_flat = tf.reshape(pts, [-1,3])\n",
    "    pts_flat = embed_fn(pts_flat)\n",
    "    raw = batchify(network_fn)(pts_flat)\n",
    "#     raw = tf.reshape(raw, list(pts.shape[:-1]) + [4]) #OG nerf\n",
    "#     raw = tf.reshape(raw, list(pts.shape[:-1]) + [1])  #[depth]\n",
    "    raw = tf.reshape(raw, list(pts.shape[:-1]) + [2]) # [depth, ray drop] \n",
    "    \n",
    "    # Compute opacities and colors\n",
    "    #OG TinyNeRF\n",
    "#     sigma_a = tf.nn.relu(raw[...,3])\n",
    "#     rgb = tf.math.sigmoid(raw[...,:3]) \n",
    "    #LiDAR-NeRF\n",
    "    sigma_a = tf.nn.relu(raw[...,0])\n",
    "    ray_drop = tf.nn.relu(raw[...,1])\n",
    "#     ray_drop = tf.math.sigmoid(raw[...,1]) #test\n",
    "\n",
    "    # Do volume rendering\n",
    "    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[...,:1].shape)], -1) \n",
    "    alpha = 1.-tf.exp(-sigma_a * dists)  \n",
    "    weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
    "\n",
    "#     print(\"weights[...,None]\",np.shape(weights[..., None]))\n",
    "#     print(\"ray_drop\", np.shape(ray_drop))\n",
    "#     print(np.shape(weights), np.shape(z_vals))\n",
    "    \n",
    "    depth_map = tf.reduce_sum(weights * z_vals, -1)\n",
    "    ray_drop_map = tf.reduce_sum(weights * ray_drop, -1) #axis was -2, changed to -1 \n",
    "    acc_map = tf.reduce_sum(weights, -1)\n",
    "\n",
    "#     return depth_map, acc_map\n",
    "    return depth_map, acc_map, ray_drop_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7fe9ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = init_model()\n",
    "optimizer = tf.keras.optimizers.Adam(5e-4) #default tiny-NeRF\n",
    "# optimizer = tf.keras.optimizers.Adam(5e-5)\n",
    "\n",
    "N_samples = 256 #256 #64 #decrease as needed for VRAM\n",
    "N_iters = 50_000\n",
    "psnrs = []\n",
    "iternums = []\n",
    "i_plot = 50\n",
    "\n",
    "import time\n",
    "t = time.time()\n",
    "for i in range(N_iters+1):\n",
    "    img_i = np.random.randint(images.shape[0])\n",
    "    target = images[img_i,:,:,:1]\n",
    "    target_drop_mask = images[img_i,:,:,1:]\n",
    "    pose = poses[img_i]\n",
    "    #get ray origins and ray directions\n",
    "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #just calculate loss via comparing depth output vs target (should also be depth!)\n",
    "#         depth, acc = render_rays(model, rays_o, rays_d, near=1., far=64., N_samples=N_samples, rand=True)\n",
    "        depth, acc, ray_drop = render_rays(model, rays_o, rays_d, near=1., far=64., N_samples=N_samples, rand=True)\n",
    "        depth = depth[:,:,None]\n",
    "        ray_drop = ray_drop[:,:,None]\n",
    "        \n",
    "#         print(np.shape(depth), np.shape(target))\n",
    "#         print(np.shape(ray_drop), np.shape(target_drop_mask))\n",
    "        \n",
    "        #default loss from TinyNERF\n",
    "#         loss = tf.reduce_mean(tf.square(depth - target)) \n",
    "        #Distance Loss only\n",
    "#         loss = tf.reduce_mean(tf.abs(depth - target)) # <-- works way better than dist^2\n",
    "    \n",
    "        #LiDAR-NeRF LoSS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        ## L_total = L_dist + lam1*L_intensity + lam2*L_raydrop + lam3*L_reg\n",
    "        ##          lam1=1, lam2=1, lam3=100\n",
    "        ## L_reg --> structural regularization: consider gradient loss only on high-texture areas\n",
    "        \n",
    "        #Gradient Loss (structural regularization for smooth surfaces)\n",
    "        thresh = 0.025 #was 0.1 in LiDAR-NeRF\n",
    "        mask = np.zeros(np.shape(target[:,:,0]))\n",
    "        vertical_grad_target = np.gradient(target[:,:,0])[0] \n",
    "        vertical_past_thresh = np.argwhere(tf.abs(vertical_grad_target) > thresh)\n",
    "        mask[vertical_past_thresh[:,0], vertical_past_thresh[:,1]] = 1\n",
    "        horizontal_grad_target = np.gradient(target[:,:,0])[1]\n",
    "        horizontal_past_thresh = np.argwhere(tf.abs(horizontal_grad_target) > thresh)\n",
    "        mask[horizontal_past_thresh[:,0], horizontal_past_thresh[:,1]] = 1\n",
    "        vertical_grad_inference = np.gradient(depth[:,:,0])[0]\n",
    "        horizontal_grad_inference = np.gradient(depth[:,:,0])[1]\n",
    "        mag_difference = tf.math.sqrt((vertical_grad_target-vertical_grad_inference)**2 + (horizontal_grad_target-horizontal_grad_inference)**2)\n",
    "#         L_reg = tf.reduce_mean(np.multiply(mag_difference, mask))\n",
    "        #suppress ray drop areas\n",
    "        L_reg = np.multiply(mag_difference, mask)\n",
    "        L_reg = tf.reduce_mean(tf.multiply(L_reg, target_drop_mask))\n",
    "        L_reg = tf.cast(L_reg, tf.float32)            \n",
    "                \n",
    "        #ray drop loss\n",
    "        L_raydrop = tf.keras.losses.binary_crossentropy(target_drop_mask, ray_drop)\n",
    "        L_raydrop = tf.math.reduce_mean(tf.abs(L_raydrop))\n",
    "#         print(\"Ray Drop Loss:\", L_raydrop)\n",
    "    \n",
    "        #distance loss\n",
    "#         L_dist = tf.reduce_mean(tf.abs(depth - target)) #simple loss\n",
    "        #suppressing ray drop areas\n",
    "        depth_nondrop = tf.math.multiply(depth, target_drop_mask)\n",
    "        target_nondrop = tf.math.multiply(target, target_drop_mask)\n",
    "        L_dist = tf.reduce_mean(tf.abs(depth_nondrop - target_nondrop))\n",
    "    \n",
    "        lam1 = 100\n",
    "        lam2 = 1 #1/(64**2)\n",
    "        loss = L_dist + lam1*L_reg + lam2*L_raydrop       \n",
    "\n",
    "#         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    if i%i_plot==0:\n",
    "        t = time.time()\n",
    "        # Render the holdout view for logging\n",
    "        rays_o, rays_d = get_rays(H, W, focal, testpose) #constant validation image\n",
    "#         sampl = int(np.random.uniform(low=0, high=len(testpose)-1))        \n",
    "#         rays_o, rays_d = get_rays(H, W, focal, testpose[sampl]) #90/10 split\n",
    "#         depth, acc = render_rays(model, rays_o, rays_d, near=1., far=64., N_samples=N_samples)\n",
    "        depth, acc, ray_drop = render_rays(model, rays_o, rays_d, near=1., far=64., N_samples=N_samples)\n",
    "        depth = depth[:,:,None]\n",
    "        ray_drop = ray_drop[:,:,None]\n",
    "#         print(ray_drop)\n",
    "#         #simple depth only\n",
    "#         loss = tf.reduce_mean(tf.square(depth[:,:,None] - testimg[:,:,0])) \n",
    "#         #ray drop only\n",
    "#         L_raydrop = tf.keras.losses.binary_crossentropy(testimg[:,:,1], ray_drop)\n",
    "#         loss = tf.math.reduce_sum(tf.abs(L_raydrop)).numpy()\n",
    "\n",
    "        #LiDAR-NeRF LoSS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        ## L_total = L_dist + lam1*L_intensity + lam2*L_raydrop + lam3*L_reg\n",
    "        ##          lam1=1, lam2=1, lam3=100\n",
    "        ## L_reg --> structural regularization: consider gradient loss only on high-texture areas\n",
    "        \n",
    "        target = testimg[:,:,:1]\n",
    "        target_drop_mask = testimg[:,:,1:]\n",
    "        \n",
    "        #Gradient Loss (structural regularization for smooth surfaces)\n",
    "        thresh = 0.025 #was 0.1 in LiDAR-NeRF\n",
    "        mask = np.zeros(np.shape(target[:,:,0]))\n",
    "        vertical_grad_target = np.gradient(target[:,:,0])[0] \n",
    "        vertical_past_thresh = np.argwhere(tf.abs(vertical_grad_target) > thresh)\n",
    "        mask[vertical_past_thresh[:,0], vertical_past_thresh[:,1]] = 1\n",
    "        horizontal_grad_target = np.gradient(target[:,:,0])[1]\n",
    "        horizontal_past_thresh = np.argwhere(tf.abs(horizontal_grad_target) > thresh)\n",
    "        mask[horizontal_past_thresh[:,0], horizontal_past_thresh[:,1]] = 1\n",
    "        vertical_grad_inference = np.gradient(depth[:,:,0])[0]\n",
    "        horizontal_grad_inference = np.gradient(depth[:,:,0])[1]\n",
    "        mag_difference = tf.math.sqrt((vertical_grad_target-vertical_grad_inference)**2 + (horizontal_grad_target-horizontal_grad_inference)**2)\n",
    "        L_reg = tf.reduce_mean(np.multiply(mag_difference, mask))\n",
    "#         L_reg = tf.reduce_mean(tf.abs(np.multiply(vertical_grad_target-vertical_grad_inference, mask)))\n",
    "        L_reg = tf.cast(L_reg, tf.float32)     \n",
    "        #suppress ray drop areas\n",
    "        L_reg = np.multiply(mag_difference, mask)\n",
    "        L_reg = tf.reduce_mean(tf.multiply(L_reg, target_drop_mask))\n",
    "        L_reg = tf.cast(L_reg, tf.float32)            \n",
    "    \n",
    "        #distance loss\n",
    "        L_dist = tf.reduce_mean(tf.abs(depth - target))\n",
    "        \n",
    "        #ray drop loss\n",
    "        L_raydrop = tf.keras.losses.binary_crossentropy(target_drop_mask, ray_drop)\n",
    "        L_raydrop = tf.math.reduce_sum(tf.abs(L_raydrop))\n",
    "#         print(\"Ray Drop Loss:\", L_raydrop)\n",
    "        \n",
    "        lam1 = 100\n",
    "        lam2 = 1/(64**2)\n",
    "        loss = L_dist + lam1*L_reg + lam2*L_raydrop       \n",
    "\n",
    "#         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "        psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n",
    "        psnrs.append(psnr.numpy())\n",
    "        iternums.append(i)\n",
    "        p.figure(figsize=(10,4))\n",
    "        p.subplot(131)\n",
    "        p.imshow(depth,cmap = \"gray\", norm='log')\n",
    "        p.title(f'Estimated Depth at Iteration: {i}')\n",
    "#         p.imshow(testimg[:,:,1],cmap = \"gray\") #, norm='log')\n",
    "#         p.title(f'Actual Mask at Iteration: {i}')\n",
    "        p.subplot(133)\n",
    "        p.plot(iternums, psnrs)\n",
    "        p.title('PSNR')\n",
    "        #look at depth map\n",
    "        p.subplot(132)\n",
    "        p.imshow(ray_drop, cmap=\"gray\", norm = 'log')\n",
    "        p.title(\"estimated ray drop mask\")\n",
    "        p.show()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a14342",
   "metadata": {},
   "source": [
    "# Debug: Show Depth Mask fit to Training Poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a428dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = 128 #64 #does not have to match what was used in training?? \n",
    "idx = 40\n",
    "# %matplotlib notebook\n",
    "\n",
    "rotm = poses[idx]\n",
    "# rotm = testpose\n",
    "\n",
    "#call NeRF using specified novel rotm\n",
    "rays_o, rays_d = get_rays(H, W, focal, rotm)\n",
    "depth, acc, ray_drop = render_rays(model, rays_o, rays_d, near=1., far=64., N_samples=N_samples)\n",
    "new_point_cloud_spherical = np.zeros([np.shape(depth)[0]*np.shape(depth)[1],3])\n",
    "\n",
    "depth = tf.transpose(depth).numpy() #need this\n",
    "depth = np.flip(depth, axis = 0) #needed\n",
    "\n",
    "fig, ax = p.subplots(1,3)\n",
    "ax[0].imshow(images[idx,:,:,0],cmap = \"gray\",norm='log')\n",
    "ax[0].set_title(\"depth image\")\n",
    "ax[1].imshow(images[idx,:,:,1],cmap = \"gray\")\n",
    "# ax[1].imshow(testimg[:,:,1],cmap = \"gray\")\n",
    "ax[1].set_title(\"true ray drop mask\")\n",
    "ax[2].set_title(\"estimated ray drop mask\")\n",
    "ax[2].imshow(ray_drop, cmap = \"gray\")\n",
    "\n",
    "L_raydrop = tf.keras.losses.binary_crossentropy(images[idx,:,:,1], ray_drop)\n",
    "# L_raydrop = tf.keras.losses.binary_crossentropy(testimg[:,:,1], ray_drop)\n",
    "loss = tf.math.reduce_sum(tf.abs(L_raydrop)).numpy()\n",
    "# print(loss)\n",
    "# print(L_raydrop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f012f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try suppressing distance loss in  training data with non-returns\n",
    "# L_dist = tf.reduce_mean(tf.multiply(tf.abs(depth - target), target_drop_mask))\n",
    "# print(L_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ebbc7",
   "metadata": {},
   "source": [
    "## Draw Full Point Cloud From Novel Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rots =  9 #279 \n",
    "N_samples = 256 #64 #does not have to match what was used in training?? \n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "for j in range(num_rots):\n",
    "# for j in np.linspace(0,7,15):\n",
    "    #get sensor transformation matrix\n",
    "    rotm = np.eye(4)\n",
    "\n",
    "    # account for image crop in rotation -------------------\n",
    "    crop_angle = -(phimax-phimin)/2 - j*(phimax-phimin)#square\n",
    "#     crop_angle = -(phimax-phimin)/16 - j*(phimax-phimin)/8 #eighth\n",
    "#     crop_angle = -(phimax-phimin)/32 - j*(phimax-phimin)/16 #1/16\n",
    "#     crop_angle = -(phimax-phimin)/32 - j*(phimax-phimin)/32 #1/32\n",
    "    rotm_crop = R.from_euler('xyz', [0,-crop_angle,0]).as_matrix()\n",
    "    rotm[:3,:3] = rotm[:3,:3] @ rotm_crop\n",
    "\n",
    "    #also need to account for the fact that the LIDAR beam isn't actually centered at horizon\n",
    "    sensor_elevation_zero_rotm = R.from_euler('xyz', [(phimin+phimax)/2,0,0]).as_matrix()\n",
    "    rotm[:3,:3] = rotm[:3,:3] @ sensor_elevation_zero_rotm\n",
    "    # ------------------------------------------------------\n",
    "\n",
    "    # flip x and z axis\n",
    "    rotm[0,-1], rotm[2,-1] = rotm[2,-1], rotm[0,-1] \n",
    "\n",
    "    rotm[0:3,2] *= -1 # flip sign of y and z axis\n",
    "    rotm[0:3,1] *= -1\n",
    "    rotm = rotm[[1,0,2,3],:]\n",
    "    rotm[2,:] *= -1 # flip whole world upside down\n",
    "    rotm[2,-1] += 30 #4 #x in world frame output\n",
    "#     rotm[2,-1] += 15 #- i/6 #x in world frame output\n",
    "    rotm[0,-1] = -1.5 #- (i/3) #z in world frame output\n",
    "    rotm[1,-1] = 2 #+ (i/6) #y in world frame\n",
    "\n",
    "    rotm = rotm.astype(np.float32)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    #call NeRF using specified novel rotm\n",
    "    rays_o, rays_d = get_rays(H, W, focal, rotm)\n",
    "#     depth, acc = render_rays(model, rays_o, rays_d, near=1., far=64., N_samples=N_samples)\n",
    "    depth, acc, ray_drop = render_rays(model, rays_o, rays_d, near=1., far=64., N_samples=N_samples)\n",
    "    end = time.time()\n",
    "    new_point_cloud_spherical = np.zeros([np.shape(depth)[0]*np.shape(depth)[1],3])\n",
    "    \n",
    "    depth = tf.transpose(depth).numpy() #need this\n",
    "    depth = np.flip(depth, axis = 0) #needed\n",
    "\n",
    "    ray_drop = tf.transpose(ray_drop).numpy() #test\n",
    "    ray_drop = np.flip(ray_drop, axis = 0) #test\n",
    "    \n",
    "    phimin = -0.53529248 #rad\n",
    "    phimax = 0.18622663 #rad\n",
    "\n",
    "    count = 0\n",
    "    for w in range(np.shape(pcs)[1]):\n",
    "        for h in range(np.shape(pcs)[0]):\n",
    "            #draw all points\n",
    "#             new_point_cloud_spherical[count,0] = depth[w,h] #radius\n",
    "            #suppress ray dropped points\n",
    "            if ray_drop[w,h] > 0.9:\n",
    "                    new_point_cloud_spherical[count,0] = depth[w,h] #radius\n",
    "            else:\n",
    "                    new_point_cloud_spherical[count,0] = 0#100\n",
    "\n",
    "            new_point_cloud_spherical[count,1] = (phimax-phimin)*(w/(np.shape(depth)[0])) #theta (square)\n",
    "#             new_point_cloud_spherical[count,1] = (phimax-phimin)*(w/(np.shape(depth)[0]))/8 #theta (eighth)\n",
    "#             new_point_cloud_spherical[count,1] = (phimax-phimin)*(w/(np.shape(depth)[0]))/16 #theta (1/16)\n",
    "#             new_point_cloud_spherical[count,1] = (phimax-phimin)*(w/(np.shape(depth)[0]))/32 #theta (1/32)\n",
    "            new_point_cloud_spherical[count,2] = np.pi/2 + phimax - (phimax-phimin)*(h/np.shape(depth)[1]) #phi\n",
    "            count+= 1\n",
    "\n",
    "    #account for sweep angle using j\n",
    "    new_point_cloud_spherical[:,1] -= j*(phimax-phimin) #square\n",
    "#     new_point_cloud_spherical[:,1] -= j*(phimax-phimin)/8 #eighth\n",
    "#     new_point_cloud_spherical[:,1] -= j*(phimax-phimin)/16 #1/16\n",
    "#     new_point_cloud_spherical[:,1] -= j*(phimax-phimin)/32 #1/32\n",
    "\n",
    "    #if last square patch in sweep, throw away any points that wrap around origin\n",
    "    if j==8:\n",
    "        new_point_cloud_spherical= new_point_cloud_spherical[new_point_cloud_spherical[:,1]>=(-2*np.pi + (phimax-phimin))]\n",
    "    \n",
    "    new_point_cloud_cart = LC.s2c(LC,new_point_cloud_spherical).numpy()\n",
    "    new_point_cloud_cart[:,2] = -new_point_cloud_cart[:,2] #need to flip z \n",
    "\n",
    "    # rainbow by z height\n",
    "    zheight = 100*(np.sin(0.25*new_point_cloud_cart[:,2])+1)\n",
    "    cname = np.array([1-zheight, zheight, 1.5*zheight]).T.tolist()\n",
    "    disp.append(Points(new_point_cloud_cart, c = cname, r = 3, alpha = 0.5))\n",
    "    \n",
    "#     pcfn = \"gazebo_scene2/box2_scan\" + str(3) + \".npy\" #box v3\n",
    "#     pc = np.load(pcfn)\n",
    "#     disp.append(Points(pc, c = 'k', r = 3, alpha = 0.3))\n",
    "    \n",
    "plt.show(disp, \"Novel Point Cloud From NeRF\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b31fc",
   "metadata": {},
   "source": [
    "# Animate GIF of sensor moving through scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bf383",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rots = 9\n",
    "num_frames = 50\n",
    "\n",
    "for i in range(num_frames):\n",
    "    print(i)\n",
    "    plt = Plotter(N = 1, axes = 4, bg = (1, 1, 1), offscreen=True)\n",
    "    disp=[] \n",
    "    for j in range(num_rots):\n",
    "        #get sensor transformation matrix\n",
    "        rotm = np.eye(4)\n",
    "\n",
    "        # account for image crop in rotation -------------------\n",
    "        crop_angle = -(phimax-phimin)/2 - j*(phimax-phimin)#square\n",
    "#         crop_angle = -(phimax-phimin)/32 - j*(phimax-phimin)/32 #1/32\n",
    "        rotm_crop = R.from_euler('xyz', [0,-crop_angle,0]).as_matrix()\n",
    "        rotm[:3,:3] = rotm[:3,:3] @ rotm_crop\n",
    "\n",
    "        #also need to account for the fact that the LIDAR beam isn't actually centered at horizon\n",
    "        sensor_elevation_zero_rotm = R.from_euler('xyz', [(phimin+phimax)/2,0,0]).as_matrix() #test\n",
    "        rotm[:3,:3] = rotm[:3,:3] @ sensor_elevation_zero_rotm\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "        # flip x and z axis\n",
    "        rotm[0,-1], rotm[2,-1] = rotm[2,-1], rotm[0,-1] \n",
    "        rotm[0:3,2] *= -1 # flip sign of y and z axis\n",
    "        rotm[0:3,1] *= -1\n",
    "        rotm = rotm[[1,0,2,3],:]\n",
    "        rotm[2,:] *= -1 # flip whole world upside down\n",
    "        rotm[2,-1] += 30 - i/2 #x in world frame output\n",
    "        rotm[0,-1] = -2 #- (i/3) #z in world frame output\n",
    "        rotm[1,-1] = 2 # + (i/6) #y in world frame\n",
    "        rotm = rotm.astype(np.float32)\n",
    "\n",
    "        #call NeRF using specified novel rotm\n",
    "        rays_o, rays_d = get_rays(H, W, focal, rotm)\n",
    "        depth, acc, ray_drop = render_rays(model, rays_o, rays_d, near=1., far=64., N_samples=N_samples)\n",
    "        end = time.time()\n",
    "        new_point_cloud_spherical = np.zeros([np.shape(depth)[0]*np.shape(depth)[1],3])\n",
    "\n",
    "        #need this\n",
    "        depth = tf.transpose(depth).numpy() \n",
    "        depth = np.flip(depth, axis = 0) \n",
    "        ray_drop = tf.transpose(ray_drop).numpy()\n",
    "        ray_drop = np.flip(ray_drop, axis = 0)\n",
    "        phimin = -0.53529248 #rad\n",
    "        phimax = 0.18622663 #rad\n",
    "\n",
    "        count = 0\n",
    "        for w in range(np.shape(pcs)[1]):\n",
    "            for h in range(np.shape(pcs)[0]):\n",
    "                #draw all points\n",
    "    #             new_point_cloud_spherical[count,0] = depth[w,h] #radius\n",
    "                #suppress ray dropped points\n",
    "                if ray_drop[w,h] > 0.9:\n",
    "                        new_point_cloud_spherical[count,0] = depth[w,h] #radius\n",
    "                else:\n",
    "                        new_point_cloud_spherical[count,0] = 0#100\n",
    "\n",
    "                \n",
    "                new_point_cloud_spherical[count,1] = (phimax-phimin)*(w/(np.shape(depth)[0])) #theta (square)\n",
    "#                 new_point_cloud_spherical[count,1] = (phimax-phimin)*(w/(np.shape(depth)[0]))/32 #theta (1/32)\n",
    "                new_point_cloud_spherical[count,2] = np.pi/2 + phimax - (phimax-phimin)*(h/np.shape(depth)[1]) #phi\n",
    "                count+= 1\n",
    "\n",
    "        #account for sweep angle using j\n",
    "        new_point_cloud_spherical[:,1] -= j*(phimax-phimin) #square\n",
    "#         new_point_cloud_spherical[:,1] -= j*(phimax-phimin)/32 #1/32            \n",
    "\n",
    "        #if last square patch in sweep, throw away any points that wrap around origin\n",
    "        if j==8:\n",
    "            new_point_cloud_spherical= new_point_cloud_spherical[new_point_cloud_spherical[:,1]>=(-2*np.pi + (phimax-phimin))]\n",
    "\n",
    "        new_point_cloud_cart = LC.s2c(LC,new_point_cloud_spherical).numpy()\n",
    "        new_point_cloud_cart[:,2] = -new_point_cloud_cart[:,2] #need to flip z \n",
    "\n",
    "        #translate to keep camera fixed in place\n",
    "        new_point_cloud_cart[:,0] += np.cos(-(phimax-phimin)) * (i/2)\n",
    "        new_point_cloud_cart[:,1] -= np.sin(-(phimax-phimin)) * (i/2)\n",
    "        \n",
    "        # rainbow by z height\n",
    "        zheight = 100*(np.sin(0.25*new_point_cloud_cart[:,2])+1)\n",
    "        cname = np.array([1-zheight, zheight, 1.5*zheight]).T.tolist()\n",
    "        disp.append(Points(new_point_cloud_cart, c = cname, r = 3., alpha = 0.5))\n",
    "    \n",
    "    #overhead diagonal\n",
    "    cam = dict(\n",
    "        pos=(-23.00344, -94.74697, 90.65512),\n",
    "        focalPoint=(-3.497593, 3.745954, 0.06619059),\n",
    "        viewup=(0.1094000, 0.6610702, 0.7423057),\n",
    "        distance=135.2320,\n",
    "        clippingRange=(44.94560, 235.8997),\n",
    "    )\n",
    "#     #follow cam\n",
    "#     cam = dict(\n",
    "#         pos=(-43.92513, -38.84776, 18.14660),\n",
    "#         focalPoint=(-2.117923, 1.000159, 2.273724),\n",
    "#         viewup=(0.2240760, 0.1485759, 0.9631797),\n",
    "#         distance=59.89697,\n",
    "#         clippingRange=(0.2096860, 209.6860),\n",
    "#     )\n",
    "\n",
    "    plt.show(disp, \"Novel Point Cloud From NeRF at [\" \n",
    "             + str(-rotm[2,-1]) + \", \" + str(-rotm[1,-1]) + \", \" + str(-rotm[0,-1]) + \"]\",\n",
    "             camera= cam).screenshot(\"lidar_nerf_demo/town1_\" + str(i) + \".png\")\n",
    "    plt.clear()\n",
    "    plt.close()\n",
    "# ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d034a7",
   "metadata": {},
   "source": [
    "## Loss Function:\n",
    "# $\\mathcal{L}_{total} = \\mathcal{L}_{distance} + \\lambda_{1}\\mathcal{L}_{intensity}(r) + \\lambda_{2}\\mathcal{L}_{raydrop}(r) + \\lambda_{3}\\mathcal{L}_{reg}(r)$\n",
    "\n",
    "## $\\mathcal{L}_{distance}(r) = \\Sigma_{r \\in R} || \\hat{D}(r) - D(r)||$\n",
    "\n",
    "R is the set of all rays, $\\lambda$ are the weights for each term\n",
    "\n",
    "## $\\mathcal{L}_{raydrop} = \\frac{1}{|R|} \\sum_{r\\in\\mathbb{R}} \n",
    "(\\mathcal{L}_{bce} + \\mathcal{L}_{ls}) $\n",
    "\n",
    "$\\mathcal{L}_{bce}$ is binary cross entropy. \n",
    "\n",
    "$\\mathcal{L}_{ls}$ is Lovasz loss, a commonly used metric for optimizing intersection-over-union\n",
    "\n",
    "$\\lambda_1 = 1, \\lambda_2 = 1, \\lambda_3 = 1e2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mask = testimg[:,:,1:]\n",
    "# true_mask = images[42,:,:,1:]\n",
    "\n",
    "fig, ax = p.subplots(1,2)\n",
    "ax[0].imshow(true_mask,cmap = \"gray\")\n",
    "ax[0].set_title(\"true depth mask\")\n",
    "\n",
    "# dummy_mask = testimg[:,:,1:]\n",
    "# #add false positives\n",
    "dummy_mask = (testimg[:,:,1:] * np.floor(0.99 + np.random.rand(np.shape(testimg)[0], np.shape(testimg)[1]))[:,:,None])\n",
    "#add slight underscaling\n",
    "dummy_mask = dummy_mask * (9 + np.random.rand(np.shape(testimg)[0], np.shape(testimg)[1])[:,:,None])/10\n",
    "# print(dummy_mask[:,:,0])\n",
    "ax[1].imshow(dummy_mask, cmap = \"gray\")\n",
    "# ax[1].imshow(ray_drop, cmap = \"gray\")\n",
    "ax[1].set_title(\"ray drop esimate\")\n",
    "\n",
    "print(np.shape(true_mask))\n",
    "\n",
    "loss = tf.keras.losses.binary_crossentropy(true_mask, dummy_mask)\n",
    "# print(loss)\n",
    "print(tf.reduce_sum(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c8a5b",
   "metadata": {},
   "source": [
    "# Debug: visualize frames for drawing points for depth testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f59bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#GOAL: try remaking <dirs> but with spherical projection (not pinhole camera intrinsics)\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
    "look_at_pose = 5\n",
    "c2w = poses[look_at_pose]\n",
    "# c2w = np.eye(3)\n",
    "c2w = tf.cast(c2w, tf.float32)\n",
    "near = 0.\n",
    "far = 64.\n",
    "focal = np.array(np.shape(images)[1]/(2*np.tan((phimax-phimin)/2))) #default image size\n",
    "\n",
    "plt = Plotter(N = 2, axes = 1, bg = (1, 1, 1), interactive = True, sharecam = False) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "disp_alt = []\n",
    "\n",
    "#pinhole camera projection ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "dirs = tf.stack([(i-(W*.5))/focal, -(j-(H*.5))/focal, -tf.ones_like(i)], -1) \n",
    "# c2w = np.eye(3)\n",
    "rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
    "z_vals = tf.linspace(near, far, N_samples) \n",
    "z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "#[image_height, image_width, batch_size, 3]\n",
    "pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "pts_flat = tf.reshape(pts, [-1,3])\n",
    "\n",
    "disp.append(Points(tf.reshape(rays_d, [-1,3]), c = 'light blue',r=5, alpha = 0.5)) #compare here, dirs doesn't invoke c2w\n",
    "disp.append(Points(tf.reshape(rays_d, [-1,3])[:1000], c = 'blue',r=10, alpha = 0.5))\n",
    "disp_alt.append(Points(pts_flat, c = 'blue', r = 5, alpha = 0.01))\n",
    "disp_alt.append(Points(pts_flat[:100], c = 'blue', r = 10, alpha = 0.2))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "#reformat for LiDAR depth measurements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "vert_fov = np.rad2deg(phimax-phimin)\n",
    "# #\"square\" lidar image\n",
    "# dirs_test = tf.stack([-tf.ones_like(i), np.pi/2+(j-(W*.5))/focal, (i-(H*.5))/focal], -1) \n",
    "# non-horizon centered lidar image\n",
    "dirs_test = tf.stack([-tf.ones_like(i), -(j-(W*.5))/(2*vert_fov) + np.rad2deg(phimax), (i-(H*.5))/(2*vert_fov)], -1) #looks right (but does not converge perfectly)\n",
    "# dirs_test = tf.stack([-tf.ones_like(i), -(j-(W*.5))/focal - np.rad2deg(phimax-phimin)/2, (i-(H*.5))/focal], -1) #test\n",
    "\n",
    "dirs_test = tf.reshape(dirs_test,[-1,3])\n",
    "#[r, theta, phi] --> [r, phi, theta]\n",
    "dirs_test = LC.s2c(LC, tf.transpose(tf.Variable([dirs_test[:,0], dirs_test[:,2], dirs_test[:,1]], dtype = tf.float32)))\n",
    "\n",
    "# #test-- try projecting skewed depth image flat\n",
    "# dirs_test = tf.transpose(tf.Variable([tf.ones(len(dirs_test)), dirs_test[:,1], dirs_test[:,2]], dtype = tf.float32))\n",
    "\n",
    "disp.append(Points(dirs_test, c = 'light green'))\n",
    "disp.append(Points(dirs_test[:1000], r = 10, c = 'green'))\n",
    "\n",
    "#need to rotate red points into same frame as blue points \n",
    "# rotm = R.from_euler('xyz', [0,np.pi/2,0]).as_matrix() #square\n",
    "# rotm = R.from_euler('xyz', [0,-np.rad2deg(phimax-phimin)/2,0]).as_matrix() #non-horizon centered lidar image v1\n",
    "# rotm = R.from_euler('xyz', [0,np.rad2deg(phimax-phimin)/2,0]).as_matrix() #need to flip sign for upside down test\n",
    "#try out accounting for horizon misalignment here\n",
    "# rotm = R.from_euler('xyz', [0,np.rad2deg(phimax),0]).as_matrix()\n",
    "rotm = R.from_euler('xyz', [0,-np.pi/2,0]).as_matrix() #test\n",
    "# rotm = R.from_euler('xyz', [0,-(np.pi/2)+phimax,0]).as_matrix() #test\n",
    "\n",
    "\n",
    "dirs_test = dirs_test @ rotm\n",
    "dirs_test = dirs_test @ tf.transpose(c2w[:3,:3]) #need this to get dirs to align (but not for pts_flat)\n",
    "\n",
    "# aligns dirs, doesn't get pts_flat to work yet\n",
    "dirs_test = dirs_test @ (c2w[:3,:3] \n",
    "                      @ R.from_euler('xyz', [0,0,np.pi/2]).as_matrix() #looked good but converged slightly off\n",
    "#                       @ R.from_euler('xyz', [0,0,np.pi/2]).as_matrix()  #test\n",
    "                      @ np.linalg.pinv(c2w[:3,:3]) )\n",
    "\n",
    "# dirs_test = dirs_test @ (c2w[:3,:3] \n",
    "#                       @ R.from_euler('xyz', [0,np.pi,0]).as_matrix() \n",
    "#                       @ np.linalg.pinv(c2w[:3,:3]) )\n",
    "\n",
    "# rays_d_test = tf.reduce_sum(dirs_test[..., np.newaxis, :] * c2w[:3,:3], -1) #supposed to be this\n",
    "rotm_fix = (c2w[:3,:3] \n",
    "#             @ R.from_euler('xyz', [0,0,-np.pi/2]).as_matrix() \n",
    "#             @ R.from_euler('xyz', [0,0,np.pi/2]).as_matrix() \n",
    "            @ np.linalg.pinv(c2w[:3,:3]))\n",
    "#             @ c2w[:3,:3] \n",
    "#             @ R.from_euler('xyz', [0,np.pi,0]).as_matrix() \n",
    "#             @ np.linalg.pinv(c2w[:3,:3]))\n",
    "rays_d_test = tf.reduce_sum(dirs_test[..., np.newaxis, :] * rotm_fix, -1) #looks like I need to do this instead\n",
    "rays_o_test = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d_test))\n",
    "z_vals_test = tf.linspace(near, far, N_samples) \n",
    "z_vals_test += tf.random.uniform(list(rays_o_test.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "#[image_height, image_width, batch_size, 3]\n",
    "pts_test = rays_o_test[...,None,:] + rays_d_test[...,None,:] * z_vals_test[...,:,None]\n",
    "pts_flat_test = tf.reshape(pts_test, [-1,3])\n",
    "\n",
    "disp.append(Points(dirs_test, c = 'pink', r = 5, alpha = 0.5))\n",
    "disp.append(Points(dirs_test[:1000], c = 'red', r = 10, alpha = 0.5))\n",
    "disp_alt.append(Points(pts_flat_test, c = 'red', r = 5, alpha = 0.01))\n",
    "disp_alt.append(Points(pts_flat_test[:100], c = 'red', r = 10, alpha = 0.5))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "#draw frames\n",
    "#forward view direction (-z in NeRF c2w convention)\n",
    "headings = poses[look_at_pose,:3,:3] @ np.array([0,0,-1])\n",
    "disp_alt.append(Arrows(poses[look_at_pose,:3,-1][None,:], (poses[look_at_pose,:3,-1] + headings)[None,:], c = \"yellow\"))\n",
    "disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"yellow\"))\n",
    "# x\n",
    "headings = poses[look_at_pose,:3,:3] @ np.array([1,0,0])\n",
    "disp_alt.append(Arrows(poses[look_at_pose,:3,-1][None,:], (poses[look_at_pose,:3,-1] + headings)[None,:], c = \"red\"))\n",
    "disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"red\"))\n",
    "#y\n",
    "headings = poses[look_at_pose,:3,:3] @ np.array([0,1,0])\n",
    "disp_alt.append(Arrows(poses[look_at_pose,:3,-1][None,:], (poses[look_at_pose,:3,-1] + headings)[None,:], c = \"green\"))\n",
    "disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"green\"))\n",
    "#z\n",
    "headings = poses[look_at_pose,:3,:3] @ np.array([0,0,1])\n",
    "disp_alt.append(Arrows(poses[look_at_pose,:3,-1][None,:], (poses[look_at_pose,:3,-1] + headings)[None,:], c = \"blue\"))\n",
    "disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"blue\"))\n",
    "\n",
    "\n",
    "plt.show(disp, \"dirs\", at = 0)\n",
    "plt.show(disp_alt, \"pts_flat\", at = 1)\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # focal = np.array(np.shape(images)[1]/(2*np.tan((phimax-phimin)/2))) #default image size\n",
    "# print(focal) #in pixels\n",
    "# # vert_fov = np.rad2deg(phimax-phimin)\n",
    "# print(vert_fov) #in degrees\n",
    "# print(np.rad2deg(phimax-phimin)/2)\n",
    "# print(phimax, phimin)\n",
    "\n",
    "print((phimin-phimax)/2)\n",
    "print(np.tan((phimin-phimax)/2))\n",
    "print(np.arctan((phimin-phimax)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug -- draw multiple poses old camera projection and new lidar projection on same graph\n",
    "#         -> Look for areas of overlap that may be poisoning training\n",
    "\n",
    "num_patches = 3\n",
    "\n",
    "phimin = -0.53529248 #rad\n",
    "phimax = 0.18622663 #rad\n",
    "\n",
    "#test\n",
    "# phimin, phimax = -(phimax-phimin)/2, (phimax-phimin)/2\n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True, sharecam = False) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "for count in range(num_patches):\n",
    "\n",
    "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
    "    look_at_pose = count\n",
    "    c2w = poses[look_at_pose]\n",
    "    # c2w = np.eye(3)\n",
    "    c2w = tf.cast(c2w, tf.float32)\n",
    "    near = 0.\n",
    "    far = 64.\n",
    "    focal = np.array(np.shape(images)[1]/(2*np.tan((phimax-phimin)/2))) #default image size\n",
    "\n",
    "    #pinhole camera projection ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    dirs = tf.stack([(i-(W*.5))/focal, -(j-(H*.5))/focal, -tf.ones_like(i)], -1) \n",
    "    # c2w = np.eye(3)\n",
    "    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
    "    z_vals = tf.linspace(near, far, N_samples) \n",
    "    z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "\n",
    "    disp.append(Points(tf.reshape(rays_d, [-1,3]), c = 'light blue',r=5, alpha = 0.75)) #compare here, dirs doesn't invoke c2w\n",
    "    # disp.append(Points(tf.reshape(rays_d, [-1,3])[:1000], c = 'blue',r=10, alpha = 0.5))\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    #reformat for LiDAR depth measurements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    vert_fov = np.rad2deg(phimax-phimin)\n",
    "    # #\"square\" lidar image\n",
    "    # dirs_test = tf.stack([-tf.ones_like(i), np.pi/2+(j-(W*.5))/focal, (i-(H*.5))/focal], -1) \n",
    "    # non-horizon centered lidar image\n",
    "#     dirs_test = tf.stack([-tf.ones_like(i), -(j-(W*.5))/(2*vert_fov) + np.rad2deg(phimax), (i-(H*.5))/(2*vert_fov)], -1) #looks right (but does not converge perfectly)\n",
    "    #looks like there is an issue with horizontal overlap\n",
    "#     dirs_test = tf.stack([-tf.ones_like(i), -(j-(W*.5))/(2*vert_fov) - (np.pi/2) - phimax, (i-(H*.5))/(2*vert_fov)], -1)\n",
    "    dirs_test = tf.stack([-tf.ones_like(i), \n",
    "                          -(j-(W*.5))/(focal) - (np.pi/2) - phimax, \n",
    "                          np.arctan((i-(H*.5))/(focal))], -1)\n",
    "    \n",
    "    dirs_test = tf.reshape(dirs_test,[-1,3])\n",
    "    #[r, theta, phi] --> [r, phi, theta]\n",
    "    dirs_test = LC.s2c(LC, tf.transpose(tf.Variable([dirs_test[:,0], dirs_test[:,2], dirs_test[:,1]], dtype = tf.float32)))\n",
    "\n",
    "    # #test-- try projecting skewed depth image flat\n",
    "    # dirs_test = tf.transpose(tf.Variable([tf.ones(len(dirs_test)), dirs_test[:,1], dirs_test[:,2]], dtype = tf.float32))\n",
    "\n",
    "    # disp.append(Points(dirs_test, c = 'light green'))\n",
    "    # # disp.append(Points(dirs_test[:1000], r = 10, c = 'green'))\n",
    "\n",
    "    #need to rotate red points into same frame as blue points \n",
    "    # rotm = R.from_euler('xyz', [0,np.pi/2,0]).as_matrix() #square\n",
    "    # rotm = R.from_euler('xyz', [0,np.rad2deg(phimax-phimin)/2,0]).as_matrix() #need to flip sign for upside down test\n",
    "    #try out accounting for horizon misalignment here\n",
    "    # rotm = R.from_euler('xyz', [0,np.rad2deg(phimax),0]).as_matrix()\n",
    "#     rotm = R.from_euler('xyz', [0,-np.pi/2,0]).as_matrix() #test\n",
    "#     rotm = R.from_euler('xyz', [0,-(np.pi/2)-(phimax),0]).as_matrix() #better\n",
    "    rotm = R.from_euler('xyz', [0,-(np.pi/2)+(((phimax+phimin))/2),0]).as_matrix() #test\n",
    "\n",
    "    dirs_test = dirs_test @ rotm\n",
    "    dirs_test = dirs_test @ tf.transpose(c2w[:3,:3]) #need this to get dirs to align (but not for pts_flat)\n",
    "\n",
    "    # aligns dirs, doesn't get pts_flat to work yet\n",
    "    dirs_test = dirs_test @ (c2w[:3,:3] \n",
    "                          @ R.from_euler('xyz', [0,0,np.pi/2]).as_matrix() #looked good but converged slightly off\n",
    "    #                       @ R.from_euler('xyz', [0,0,np.pi/2]).as_matrix()  #test\n",
    "                          @ np.linalg.pinv(c2w[:3,:3]) )\n",
    "\n",
    "    # dirs_test = dirs_test @ (c2w[:3,:3] \n",
    "    #                       @ R.from_euler('xyz', [0,np.pi,0]).as_matrix() \n",
    "    #                       @ np.linalg.pinv(c2w[:3,:3]) )\n",
    "\n",
    "    # rays_d_test = tf.reduce_sum(dirs_test[..., np.newaxis, :] * c2w[:3,:3], -1) #supposed to be this\n",
    "    rotm_fix = (c2w[:3,:3] \n",
    "    #             @ R.from_euler('xyz', [0,0,-np.pi/2]).as_matrix() \n",
    "    #             @ R.from_euler('xyz', [0,0,np.pi/2]).as_matrix() \n",
    "                @ np.linalg.pinv(c2w[:3,:3]))\n",
    "    #             @ c2w[:3,:3] \n",
    "    #             @ R.from_euler('xyz', [0,np.pi,0]).as_matrix() \n",
    "    #             @ np.linalg.pinv(c2w[:3,:3]))\n",
    "    rays_d_test = tf.reduce_sum(dirs_test[..., np.newaxis, :] * rotm_fix, -1) #looks like I need to do this instead\n",
    "    rays_o_test = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d_test))\n",
    "    z_vals_test = tf.linspace(near, far, N_samples) \n",
    "    z_vals_test += tf.random.uniform(list(rays_o_test.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "    #[image_height, image_width, batch_size, 3]\n",
    "    pts_test = rays_o_test[...,None,:] + rays_d_test[...,None,:] * z_vals_test[...,:,None]\n",
    "    pts_flat_test = tf.reshape(pts_test, [-1,3])\n",
    "\n",
    "#     cname = np.array([255*(np.random.rand()), 255*(np.random.rand()), 255*(np.random.rand())]).T.tolist()\n",
    "#     disp.append(Points(dirs_test, c = cname, r = 5, alpha = 0.5))\n",
    "    disp.append(Points(dirs_test, c = 'pink', r = 5, alpha = 0.35))\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    #draw frames first pose\n",
    "    #forward view direction (-z in NeRF c2w convention) \n",
    "    headings = poses[look_at_pose,:3,:3] @ np.array([0,0,-0.3])\n",
    "    disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"yellow\"))\n",
    "    # x\n",
    "    headings = poses[look_at_pose,:3,:3] @ np.array([0.3,0,0])\n",
    "    disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"red\"))\n",
    "    #y\n",
    "    headings = poses[look_at_pose,:3,:3] @ np.array([0,0.3,0])\n",
    "    disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"green\"))\n",
    "    #z\n",
    "    headings = poses[look_at_pose,:3,:3] @ np.array([0,0,0.3])\n",
    "    disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"blue\"))\n",
    "\n",
    "\n",
    "plt.show(disp, \"dirs\", at = 0)\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"models/townV1.ckpt\")\n",
    "# model.load_weights(\"models/townV1.ckpt\")\n",
    "# # print(model.summary())\n",
    "\n",
    "# #test for visualization with Netron\n",
    "# model.save('models/townV1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688214fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(\n",
    "#     model,\n",
    "#     to_file='model.png',\n",
    "#     show_shapes=False,\n",
    "#     show_dtype=False,\n",
    "#     show_layer_names=False,\n",
    "#     rankdir='TB',\n",
    "#     expand_nested=False,\n",
    "#     dpi=200,\n",
    "#     show_layer_activations=False,\n",
    "#     show_trainable=False,\n",
    "#     **kwargs\n",
    "# )\n",
    "# tf.keras.utils.plot_model(model)\n",
    "dim = (64,64,1)\n",
    "model.build((None, *dim))\n",
    "model.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64799d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
    "# #pinhole camera projection\n",
    "# # dirs = tf.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -tf.ones_like(i)], -1) \n",
    "# #test-- reformat for LiDAR depth measurements\n",
    "# vert_fov = np.rad2deg(phimax-phimin)\n",
    "# # print(focal)\n",
    "# print(vert_fov)\n",
    "# dirs = tf.stack([(i-(W*.5))/focal, -(j-(H*.5))/focal, -tf.ones_like(i)], -1) #pinhole camera projection\n",
    "\n",
    "# # print(np.shape(dirs))\n",
    "\n",
    "# # c2w = poses[0]\n",
    "# c2w = np.eye(3)\n",
    "# c2w = tf.cast(c2w, tf.float32)\n",
    "# rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "# rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
    "# # print(N_samples)\n",
    "\n",
    "# near = 0.\n",
    "# far = 64.\n",
    "# z_vals = tf.linspace(near, far, N_samples) \n",
    "# z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "# #[image_height, image_width, batch_size, 3]\n",
    "# pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "# pts_flat = tf.reshape(pts, [-1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a707ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3+3*(2**10)\n",
    "3*(2**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 63 params in positional branch\n",
    "3+3*2**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ba1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.rad2deg(phimax-phimin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7691e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
