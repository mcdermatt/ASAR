{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348a4524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 16:10:16.988802: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 16:10:17.086334: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 16:10:17.405694: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/lib:\n",
      "2024-10-09 16:10:17.405757: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/lib:\n",
      "2024-10-09 16:10:17.405762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-10-09 16:10:18.917127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-09 16:10:18.949389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-09 16:10:18.949599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-09 16:10:18.950053: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 16:10:18.950879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-09 16:10:18.951073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-09 16:10:18.951257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-09 16:10:19.257804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-09 16:10:19.258014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-09 16:10:19.258179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-10-09 16:10:19.258304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1687 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_graphics as tfg\n",
    "from tensorflow_graphics.nn.loss import chamfer_distance\n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "from matplotlib import pyplot as p\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from lidar_nerf_utils import *\n",
    "\n",
    "import torch\n",
    "from chamferdist import ChamferDistance\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7410ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to train NeRF\n",
    "images = np.load(\"/home/derm/Desktop/imagesUndistortedV7.npy\")\n",
    "poses = np.load(\"/home/derm/Desktop/posesUndistortedV7.npy\")\n",
    "rays_o_all = np.load(\"/home/derm/Desktop/rays_oUndistortedV7.npy\")\n",
    "rays_d_all = np.load(\"/home/derm/Desktop/rays_dUndistortedV7.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f806d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newer College Data\n",
    "dir_name = \"/media/derm/06EF-127D4/Newer College Dataset/\"\n",
    "experiment_name = \"01_short_experiment-20230331T172433Z-009/01_short_experiment/\"\n",
    "fn_gt = dir_name + experiment_name + \"ground_truth/registered_poses.csv\"\n",
    "#sec,nsec,x,y,z,qx,qy,qz,qw\n",
    "gt = np.loadtxt(fn_gt, delimiter=',',skiprows = 1)\n",
    "seconds = gt[:, 0]\n",
    "nano_seconds = gt[:, 1]\n",
    "xyz = gt[:, 2:5]\n",
    "qxyzw = gt[:, 5:]\n",
    "num_poses = qxyzw.shape[0]\n",
    "sensor_poses = np.eye(4, dtype=np.float64).reshape(1, 4, 4).repeat(num_poses, axis=0)\n",
    "sensor_poses[:, :3, :3] = R.from_quat(qxyzw).as_matrix()\n",
    "sensor_poses[:, :3, 3] = xyz\n",
    "T_CL = np.eye(4, dtype=np.float32)\n",
    "T_CL[:3, :3] = R.from_quat([0.0, 0.0, 0.924, 0.383]).as_matrix()\n",
    "T_CL[:3, 3] = np.array([-0.084, -0.025, 0.050], dtype=np.float32)\n",
    "sensor_poses = np.einsum(\"nij,jk->nik\", sensor_poses, T_CL)\n",
    "initial_pose = np.linalg.inv(sensor_poses[0]) \n",
    "poses_timestamps = seconds * 10e9 + nano_seconds\n",
    "sensor_poses = np.einsum(\"ij,njk->nik\", np.linalg.inv(sensor_poses[0]), sensor_poses) #TRY COMMENTING OUT...\n",
    "\n",
    "#get body frame vel to remove motion disortion from training data\n",
    "vel_world_frame = np.diff(sensor_poses[:,:3,-1], axis = 0)\n",
    "vel_body_frame = np.linalg.pinv(sensor_poses[1:,:3,:3]) @ vel_world_frame[:,:,None]\n",
    "vel_body_frame = vel_body_frame[:,:,0]\n",
    "#smooth out velocity estimates\n",
    "def moving_average(a, n=10):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "window=50\n",
    "MAx = moving_average(vel_body_frame[:,0], n = window)\n",
    "MAy = moving_average(vel_body_frame[:,1], n = window)\n",
    "MAz = moving_average(vel_body_frame[:,2], n = window)\n",
    "vel_body_frame = np.array([MAx, MAy, MAz]).T\n",
    "\n",
    "rot_vel_euls = np.diff(R.from_matrix(sensor_poses[:,:3,:3]).as_euler('xyz'), axis = 0)\n",
    "idx = np.argwhere(rot_vel_euls > (np.pi))\n",
    "rot_vel_euls[idx] = 0\n",
    "idx = np.argwhere(rot_vel_euls < (-np.pi))\n",
    "rot_vel_euls[idx] = 0\n",
    "\n",
    "pl = '/media/derm/06EF-127D4/Newer College Dataset/new-college-29-01-2020-1cm-resolution-1stSection - mesh.ply'\n",
    "HD_map = trimesh.load(pl).vertices\n",
    "show_nth = 5 #10\n",
    "submap = HD_map[::show_nth]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7bca8f",
   "metadata": {},
   "source": [
    "### Aligning raw scans with dataset-provided HD Map (instead of the one I stitched together). Try calculating performance metrics excluding points in nerf output that fall outside convex hull of provided HD Map.\n",
    "\n",
    "This is a bad way to evaluate performance since it throws out points in interior rooms that are excluded from the provided Newer College HD map (we should do especially well there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27efeae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980379662a864a3e8fa76d228d93cd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=960, layout=Layout(height='auto', width='100%'), width=960)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "frame_idx = 7650 + 480\n",
    "start_frame_idx = 480\n",
    "nerf_pc = np.load(\"generatedPointClouds/\" + str(frame_idx) + \"NeRF.npy\") \n",
    "real_pc = np.load(\"generatedPointClouds/\" + str(frame_idx) + \"real.npy\")\n",
    "\n",
    "#remove too close\n",
    "d = np.sqrt(np.sum(nerf_pc**2, axis = 1))\n",
    "good  = np.argwhere(d > 2.)[:,0]\n",
    "nerf_pc = nerf_pc[good]\n",
    "#bring to map frame\n",
    "nerf_to_map = sensor_poses[frame_idx].copy()\n",
    "nerf_pc = (np.linalg.pinv(initial_pose) @ nerf_to_map @ np.append(nerf_pc, np.ones([len(nerf_pc), 1]), axis = 1).T).T\n",
    "nerf_pc = nerf_pc[:,:3]\n",
    "\n",
    "#throw out points outside convex hull\n",
    "hull = ConvexHull(submap)\n",
    "deln = Delaunay(submap)\n",
    "inside_mask = deln.find_simplex(nerf_pc) > 0\n",
    "nerf_pc = nerf_pc[inside_mask]\n",
    "\n",
    "hull_points = submap[hull.vertices]\n",
    "convex_mesh = vedo.ConvexHull(hull_points)\n",
    "convex_mesh.alpha(0.1)\n",
    "disp.append(convex_mesh)\n",
    "\n",
    "# real_pc = (np.linalg.pinv(initial_pose) @ nerf_to_map @ np.append(real_pc, np.ones([len(real_pc), 1]), axis = 1).T).T\n",
    "# real_pc = real_pc[:,:3]\n",
    "# disp.append(Points(real_pc[:,:], c = \"red\", r = 3., alpha = 0.9))\n",
    "\n",
    "disp.append(Points(nerf_pc[:,:], c = \"blue\", r = 3., alpha = 0.9))\n",
    "disp.append(Points(submap[:,:3], c = \"gray\", r = 2, alpha = 0.5))\n",
    "plt.show(disp, \"aligning nerf output and real scans with provided HD map\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "842b4632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a62cf72609a46948a8b7cabdd3db2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=960, layout=Layout(height='auto', width='100%'), width=960)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recenter at origin to crop\n",
    "\n",
    "test = (np.linalg.pinv(nerf_to_map) @ initial_pose @ np.append(nerf_pc, np.ones([len(nerf_pc), 1]), axis = 1).T).T\n",
    "test = test[:,:3]\n",
    "\n",
    "map_nerfframe = (np.linalg.pinv(nerf_to_map) @ initial_pose @ np.append(submap, np.ones([len(submap), 1]), axis = 1).T).T\n",
    "map_nerfframe = map_nerfframe[:,:3]\n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "disp.append(Points(test, c = \"blue\", r = 3., alpha = 0.9))\n",
    "disp.append(Points(map_nerfframe, c = \"gray\", r = 3., alpha = 0.9))\n",
    "plt.show(disp, \"test\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "363bbc2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79505ee3db042d4991f170b23c68d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=960, layout=Layout(height='auto', width='100%'), width=960)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try throwing out points in cloud A that don't fall within convex hull of cloud B\n",
    "from scipy.spatial import ConvexHull, Delaunay\n",
    "import vedo\n",
    "\n",
    "a = np.random.randn(100,3)\n",
    "b = np.random.randn(100,3) * np.array([2.0, 1.0, 0.5])\n",
    "\n",
    "hull = ConvexHull(a)\n",
    "deln = Delaunay(a)\n",
    "inside_mask = deln.find_simplex(b) > 0\n",
    "\n",
    "c = b[inside_mask]\n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "disp.append(Points(a, c = \"red\", r = 5., alpha = 0.5))\n",
    "disp.append(Points(b, c = \"blue\", r =5., alpha = 0.5))\n",
    "disp.append(Points(c, c = \"purple\", r =10., alpha = 0.3))\n",
    "\n",
    "hull_points = a[hull.vertices]  # Points that form the vertices of the hull\n",
    "convex_mesh = vedo.ConvexHull(hull_points)\n",
    "convex_mesh.alpha(0.1)\n",
    "disp.append(convex_mesh)\n",
    "\n",
    "plt.show(disp, \"convex hull test\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09348dc6",
   "metadata": {},
   "source": [
    "### Newer College --- Loop through scans generated by NeRF, align with stitched HD Map, crop, and calculate performance metrics on each slice individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e12a1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame:  8370\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185827fbac7a478d82034d747499a45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=960, layout=Layout(height='auto', width='100%'), width=960)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DEBUG-- figure out how to crop nerf\n",
    "\n",
    "# for frame_idx in range(7650,8370,120): #7650,8850\n",
    "frame_idx = 8370\n",
    "crop_slices = 8\n",
    "print(\"Frame: \", frame_idx)\n",
    "start_frame_idx = frame_idx - 7650 #needed to index poses (data generation starts a 7650)\n",
    "\n",
    "full_nerf = np.zeros([0,3])\n",
    "full_real = np.zeros([0,3])\n",
    "\n",
    "#will run out of memory if we try to calculate forward and reverse distances for entire HD map\n",
    "# for c in range(crop_slices):\n",
    "for crop_part in range(6):\n",
    "    if crop_part > 2:\n",
    "        c = crop_part +2\n",
    "    else:\n",
    "        c = crop_part\n",
    "    #TODO-- researcher is constantly blocking sensor at \"\"  o'clock position\n",
    "    #need to not look at these slices\n",
    "\n",
    "    nerf_pc = np.load(\"generatedPointClouds/\" + str(frame_idx) + \"NeRF.npy\") \n",
    "    real_pc = np.load(\"generatedPointClouds/\" + str(frame_idx) + \"real.npy\")\n",
    "\n",
    "    # #remove points too close to origin\n",
    "    d = np.sqrt(np.sum(nerf_pc**2, axis = 1))\n",
    "    good  = np.argwhere(d > 2.)[:,0]\n",
    "    nerf_pc = nerf_pc[good]\n",
    "    d = np.sqrt(np.sum(real_pc**2, axis = 1))\n",
    "    good  = np.argwhere(d > 2.)[:,0]\n",
    "    real_pc = real_pc[good]\n",
    "\n",
    "    minb = -np.pi + (c/crop_slices)*2*np.pi\n",
    "    maxb = -np.pi + ((c+1)/crop_slices)*2*np.pi\n",
    "\n",
    "    #crop NeRF scan to azimuthal window~~~~~~~~~~~~~~\n",
    "    nerf_pc_spherical = cartesian_to_spherical(nerf_pc)\n",
    "    order = np.argsort(nerf_pc_spherical[:,1])#sort by azimuth angle\n",
    "    nerf_pc_spherical = nerf_pc_spherical.numpy()[order,:]\n",
    "\n",
    "    #crop by azimuth angle\n",
    "    nerf_pc_spherical = nerf_pc_spherical[nerf_pc_spherical[:,1] < maxb]\n",
    "    nerf_pc_spherical = nerf_pc_spherical[nerf_pc_spherical[:,1] > minb]\n",
    "    nerf_pc = spherical_to_cartesian(nerf_pc_spherical).numpy()\n",
    "    \n",
    "    full_nerf = np.append(full_nerf, nerf_pc, axis =0)\n",
    "    \n",
    "    #crop raw scan to azimuthal window~~~~~~~~~~~~~~~\n",
    "    real_pc_spherical = cartesian_to_spherical(real_pc)\n",
    "    order = np.argsort(real_pc_spherical[:,1])#sort by azimuth angle\n",
    "    real_pc_spherical = real_pc_spherical.numpy()[order,:]\n",
    "\n",
    "    #crop by azimuth angle\n",
    "    real_pc_spherical = real_pc_spherical[real_pc_spherical[:,1] < maxb]\n",
    "    real_pc_spherical = real_pc_spherical[real_pc_spherical[:,1] > minb]\n",
    "    real_pc = spherical_to_cartesian(real_pc_spherical).numpy()\n",
    "    full_real = np.append(full_real, real_pc, axis =0)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "        \n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "# disp.append(Points(real_pc[:,:], c = \"red\", r = 3., alpha = 0.9))\n",
    "disp.append(Points(full_real[:,:], c = \"red\", r = 3., alpha = 0.9))\n",
    "disp.append(Points(full_nerf[:,:], c = \"blue\", r = 3., alpha = 0.9))\n",
    "\n",
    "plt.show(disp, \"testing CD\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350309de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#map created by stitching together undistorted raw lidar scans \n",
    "hd_map = np.load(\"generatedPointClouds/stitched_map.npy\")\n",
    "crop_slices = 8\n",
    "\n",
    "cd_hist = []\n",
    "accuracy_hist = []\n",
    "completeness_hist = []\n",
    "good1_hist = []\n",
    "good2_hist = []\n",
    "\n",
    "for frame_idx in range(7650,7770,30): #7650,8850\n",
    "    print(\"Frame: \", frame_idx)\n",
    "    start_frame_idx = frame_idx - 7650 #needed to index poses (data generation starts a 7650)\n",
    "\n",
    "    #will run out of memory if we try to calculate forward and reverse distances for entire HD map\n",
    "#     for c in range(crop_slices):\n",
    "    for crop_part in range(6):\n",
    "        #researcher is constantly blocking sensor at \"~2-4\"  o'clock position\n",
    "        #need to not look at this part of each scan to keep things fair\n",
    "        if crop_part > 2:\n",
    "            c = crop_part +2\n",
    "        else:\n",
    "            c = crop_part\n",
    "\n",
    "        nerf_pc = np.load(\"generatedPointClouds/\" + str(frame_idx) + \"NeRF.npy\") \n",
    "        real_pc = np.load(\"generatedPointClouds/\" + str(frame_idx) + \"real.npy\")\n",
    "\n",
    "        # #remove points too close to origin\n",
    "        d = np.sqrt(np.sum(nerf_pc**2, axis = 1))\n",
    "        good  = np.argwhere(d > 2.)[:,0]\n",
    "        nerf_pc = nerf_pc[good]\n",
    "        d = np.sqrt(np.sum(real_pc**2, axis = 1))\n",
    "        good  = np.argwhere(d > 2.)[:,0]\n",
    "        real_pc = real_pc[good]\n",
    "    \n",
    "        minb = -np.pi + (c/crop_slices)*2*np.pi\n",
    "        maxb = -np.pi + ((c+1)/crop_slices)*2*np.pi\n",
    "\n",
    "        #crop NeRF scan to azimuthal window~~~~~~~~~~~~~~\n",
    "        nerf_pc_spherical = cartesian_to_spherical(nerf_pc)\n",
    "        order = np.argsort(nerf_pc_spherical[:,1])#sort by azimuth angle\n",
    "        nerf_pc_spherical = nerf_pc_spherical.numpy()[order,:]\n",
    "\n",
    "        #crop by azimuth angle\n",
    "        nerf_pc_spherical = nerf_pc_spherical[nerf_pc_spherical[:,1] < maxb]\n",
    "        nerf_pc_spherical = nerf_pc_spherical[nerf_pc_spherical[:,1] > minb]\n",
    "        nerf_pc = spherical_to_cartesian(nerf_pc_spherical).numpy()\n",
    "\n",
    "        #crop raw scan to azimuthal window~~~~~~~~~~~~~~~\n",
    "        real_pc_spherical = cartesian_to_spherical(real_pc)\n",
    "        order = np.argsort(real_pc_spherical[:,1])#sort by azimuth angle\n",
    "        real_pc_spherical = real_pc_spherical.numpy()[order,:]\n",
    "\n",
    "        #crop by azimuth angle\n",
    "        real_pc_spherical = real_pc_spherical[real_pc_spherical[:,1] < maxb]    \n",
    "        real_pc_spherical = real_pc_spherical[real_pc_spherical[:,1] > minb]    \n",
    "        real_pc = spherical_to_cartesian(real_pc_spherical).numpy()\n",
    "\n",
    "        # center map on NeRF scan so we can crop it by azimuth angle to prevent OOM errors ~~~~~~~~~~~~\n",
    "        n_rots = 128\n",
    "        n_vert_patches = 1\n",
    "        n_cols_to_skip = n_rots // 8 \n",
    "        poses_index = start_frame_idx*(n_rots * n_vert_patches - 2*n_cols_to_skip*n_vert_patches)\n",
    "        nerf_to_map = poses[poses_index].copy()\n",
    "        nerf_to_map[:3,-1] *= 200\n",
    "        map_nerfframe = (np.linalg.pinv(nerf_to_map) @ np.append(hd_map, np.ones([len(hd_map), 1]), axis = 1).T).T\n",
    "        map_nerfframe = map_nerfframe[:,:3] @ R.from_euler('xyz', [0.,0., np.pi/4 - np.pi/128]).as_matrix().T\n",
    "\n",
    "        #crop HD Map to azimuthal window\n",
    "        map_nerfframe_spherical = cartesian_to_spherical(map_nerfframe)\n",
    "        order = np.argsort(map_nerfframe_spherical[:,1])#sort by azimuth angle\n",
    "        map_nerfframe_spherical = map_nerfframe_spherical.numpy()[order,:]\n",
    "        #crop by azimuth angle\n",
    "        map_nerfframe_spherical = map_nerfframe_spherical[map_nerfframe_spherical[:,1] < maxb] #    \n",
    "        map_nerfframe_spherical = map_nerfframe_spherical[map_nerfframe_spherical[:,1] > minb] #    \n",
    "\n",
    "        map_nerfframe = spherical_to_cartesian(map_nerfframe_spherical).numpy()\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        try:\n",
    "            #calculate accuracy and completeness\n",
    "            nerf_pc_tensor = torch.tensor(nerf_pc.copy(), dtype=torch.float32)\n",
    "            map_pc_tensor = torch.tensor(map_nerfframe[:,:3].copy(), dtype=torch.float32)\n",
    "            real_pc_tensor = torch.tensor(real_pc.copy(), dtype=torch.float32)\n",
    "\n",
    "            #Accuracy--\n",
    "            #\"The accuracy metric is defined as the mean distance of points on the output \n",
    "            #   mesh to their nearest neighbors on the ground truth mesh.\"\n",
    "            ## Like SHINE mapping, for the \"forward\" distance we comare nerf-generated PCs against an \n",
    "            ##  HD-Map produced by stitching together undistorted raw scans (since no surfaces outside the courtyard\n",
    "            ##  are provided in the NC HD Map)\n",
    "            #map is used as to not overly punish highly oblique surfaces (none of these methods explicitly model beam spreading) \n",
    "            dists1 = torch.cdist(nerf_pc_tensor, map_pc_tensor)\n",
    "            min_dists1 = dists1.min(dim=1)[0]  # from nerf to stitched map\n",
    "            accuracy = min_dists1.mean()\n",
    "\n",
    "            #completeness--\n",
    "            dists2 = torch.cdist(nerf_pc_tensor, real_pc_tensor)\n",
    "            min_dists2 = dists2.min(dim=0)[0]  # from real_pc to nerf_output\n",
    "            completeness = min_dists2.mean()\n",
    "\n",
    "            #chamfer distance\n",
    "            cd = (min_dists1.mean() + min_dists2.mean())/2\n",
    "\n",
    "            cd_hist = np.append(cd_hist, cd)\n",
    "            accuracy_hist = np.append(accuracy_hist, accuracy)\n",
    "            completeness_hist = np.append(completeness_hist, completeness)\n",
    "\n",
    "            #get fraction under distance threshold for calculating F-score\n",
    "            good1 = np.where(min_dists1 < 0.2)[0]\n",
    "            good1 = len(good1)/len(min_dists1)\n",
    "            good2 = np.where(min_dists2 < 0.2)[0]\n",
    "            good2 = len(good2)/len(min_dists2)\n",
    "            good1_hist = np.append(good1_hist, good1)\n",
    "            good2_hist = np.append(good2_hist, good2)\n",
    "\n",
    "            print(\"\\n cd:\", cd, \"  accuracy: \", accuracy, \" completeness:\", completeness)\n",
    "        except:\n",
    "            print(\"error with numerics c = \", c)\n",
    "        print(\"good1, good2:\", good1, good2)\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "print(\"real_pc:\", len(real_pc))\n",
    "print(\"nerf_pc:\", len(nerf_pc))\n",
    "print(\"map_nerfframe:\", len(map_nerfframe))\n",
    "\n",
    "disp.append(Points(real_pc[:,:], c = \"red\", r = 3., alpha = 0.9))\n",
    "disp.append(Points(nerf_pc[:,:], c = \"blue\", r = 3., alpha = 0.9))\n",
    "disp.append(Points(map_nerfframe[:,:3], c = \"gray\", r = 2, alpha = 0.5))\n",
    "\n",
    "plt.show(disp, \"testing CD\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cd_hist))\n",
    "print(np.mean(accuracy_hist))\n",
    "print(np.mean(completeness_hist))\n",
    "\n",
    "# print(np.mean(good1_hist))\n",
    "# print(np.mean(good2_hist))\n",
    "print(2*(np.mean(good1_hist)*np.mean(good2_hist))/(np.mean(good1_hist) + np.mean(good2_hist)))\n",
    "\n",
    "# fig, ax = p.subplots()\n",
    "# ax.plot(cd_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nerf_pc = np.load(\"lidar_nerf_demo/generated_pc_at_start.npy\")\n",
    "# real_pc = np.load(\"lidar_nerf_demo/real_pc_at_start.npy\")\n",
    "# nerf_pc = np.random.randn(1000,3)\n",
    "# real_pc = np.random.randn(1000,3)\n",
    "# nerf_pc = np.load(\"generatedPointClouds/7650NeRF.npy\")\n",
    "# real_pc = np.load(\"generatedPointClouds/7650real.npy\")\n",
    "# nerf_pc = np.load(\"generatedPointClouds/7750NeRF.npy\")\n",
    "# real_pc = np.load(\"generatedPointClouds/7750real.npy\")\n",
    "nerf_pc = np.load(\"generatedPointClouds/8130NeRF.npy\")\n",
    "real_pc = np.load(\"generatedPointClouds/8130real.npy\")\n",
    "# nerf_pc = np.load(\"generatedPointClouds/MaiCityNeRFOutput.npy\")\n",
    "# real_pc = np.load(\"generatedPointClouds/MaiCityReal.npy\")\n",
    "\n",
    "##use \"HD map\" comprised of undistorted raw scans stitched together\n",
    "## (true HD map provided with dataset it cropped to not include interior features) \n",
    "hd_map = np.load(\"generatedPointClouds/stitched_map.npy\")\n",
    "# real_pc = real_pc[::20]\n",
    "\n",
    "#train set\n",
    "# nerf_pc = np.load(\"generatedPointClouds/MaiCityNeRFOutput_frame_10.npy\") \n",
    "# real_pc = np.load(\"generatedPointClouds/MaiCityReal_frame_10.npy\")\n",
    "# # #test set\n",
    "# nerf_pc = np.load(\"generatedPointClouds/MaiCityNeRFOutput_v2_frame_0.npy\") \n",
    "# real_pc = np.load(\"generatedPointClouds/MaiCityReal_v2_frame_0.npy\")\n",
    "\n",
    "# #remove ground plane\n",
    "# real_pc = real_pc[real_pc[:,2] > -1]\n",
    "# nerf_pc = nerf_pc[nerf_pc[:,2] > -1]\n",
    "\n",
    "#crop real PC down the same as NeRF output so bidirectional CD doesn't punish non-overlaps\n",
    "# real_pc = real_pc[len(real_pc)//8:7*len(real_pc)//8,:]\n",
    "# real_pc = real_pc[len(real_pc)//8:31*len(real_pc)//32,:]\n",
    "\n",
    "\n",
    "# #remove points too close to origin\n",
    "d = np.sqrt(np.sum(nerf_pc**2, axis = 1))\n",
    "good  = np.argwhere(d > 2.)[:,0]\n",
    "nerf_pc = nerf_pc[good]\n",
    "\n",
    "\n",
    "#for NC only\n",
    "# nerf_pc = np.flip(nerf_pc, axis = 0)\n",
    "\n",
    "# #remove points out of range of sensor (only needed for Mai City)\n",
    "# d = np.sqrt(np.sum(nerf_pc**2, axis = 1))\n",
    "# good = np.argwhere(d < 100)[:,0]\n",
    "# nerf_pc = nerf_pc[good]\n",
    "\n",
    "\n",
    "# spin_part = 1\n",
    "# splits = 4 #8\n",
    "# nerf_pc = nerf_pc[spin_part*len(nerf_pc)//splits:(spin_part+1)*len(nerf_pc)//splits,:]\n",
    "# # real_pc = real_pc[spin_part*len(real_pc)//splits:(spin_part+1)*len(real_pc)//splits,:]\n",
    "\n",
    "\n",
    "# #crop NeRF scan to azimuthal window~~~~~~~~~~~~~~\n",
    "# nerf_pc_spherical = cartesian_to_spherical(nerf_pc)\n",
    "# order = np.argsort(nerf_pc_spherical[:,1])#sort by azimuth angle\n",
    "# nerf_pc_spherical = nerf_pc_spherical.numpy()[order,:]\n",
    "\n",
    "# #crop by azimuth angle\n",
    "# nerf_pc_spherical = nerf_pc_spherical[nerf_pc_spherical[:,1] < -3*np.pi/4]\n",
    "\n",
    "# nerf_pc = spherical_to_cartesian(nerf_pc_spherical).numpy()\n",
    "# print(len(nerf_pc))\n",
    "# #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "#bring nerf generated pc to map frame ~~~~~~~~~~~~~\n",
    "start_frame_idx = 480 #120\n",
    "n_rots = 128\n",
    "n_vert_patches = 1\n",
    "n_cols_to_skip = n_rots // 8 \n",
    "poses_index = start_frame_idx*(n_rots * n_vert_patches - 2*n_cols_to_skip*n_vert_patches)\n",
    "# print(poses[poses_index])\n",
    "nerf_to_map = poses[poses_index].copy()\n",
    "nerf_to_map[:3,-1] *= 200\n",
    "# nerf_to_map[:3,:3] = nerf_to_map[:3,:3] @ R.from_euler('xyz', [0.,0.,-np.pi/4]).as_matrix()\n",
    "\n",
    "nerf_pc_rotated = nerf_pc @ R.from_euler('xyz', [0.,0.,np.pi/4 - np.pi/128]).as_matrix()\n",
    "nerf_mapframe = (nerf_to_map @ np.append(nerf_pc_rotated, np.ones([len(nerf_pc), 1]), axis = 1).T).T\n",
    "nerf_mapframe = nerf_mapframe[:,:3]\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# # center map on NeRF scan so we can crop it to prevent the dreaded OOM ~~~~~~~~\n",
    "# map_nerfframe = (np.linalg.pinv(nerf_to_map) @ np.append(real_pc, np.ones([len(map_rotated), 1]), axis = 1).T).T\n",
    "# map_nerfframe = map_nerfframe[:,:3] @ R.from_euler('xyz', [0.,0., np.pi/4 - np.pi/128]).as_matrix().T\n",
    "\n",
    "# #crop HD Map to azimuthal window\n",
    "# map_nerfframe_spherical = cartesian_to_spherical(map_nerfframe)\n",
    "# order = np.argsort(map_nerfframe_spherical[:,1])#sort by azimuth angle\n",
    "# map_nerfframe_spherical = map_nerfframe_spherical.numpy()[order,:]\n",
    "\n",
    "# #crop by azimuth angle\n",
    "# map_nerfframe_spherical = map_nerfframe_spherical[map_nerfframe_spherical[:,1] < -3*np.pi/4] #debug for viz\n",
    "\n",
    "# map_nerfframe = spherical_to_cartesian(map_nerfframe_spherical).numpy()\n",
    "# print(len(map_nerfframe))\n",
    "# #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "print(np.shape(nerf_pc))\n",
    "print(np.shape(real_pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22cc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618ffa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combine multiple nerf-generated point clouds to new HD Map for visualization\n",
    "\n",
    "nerf_map = np.zeros([0,3])\n",
    "\n",
    "for frame_idx in range(7650,8850,120): #7650,8850,30\n",
    "    print(\"Frame: \", frame_idx)\n",
    "    start_frame_idx = frame_idx - 7650 #needed to index poses (data generation starts a 7650)\n",
    "\n",
    "#     nerf_pc = np.load(\"generatedPointClouds/\" + str(frame_idx) + \"NeRF.npy\") \n",
    "    nerf_pc = np.load(\"generatedPointClouds/\" + str(frame_idx) + \"NeRF_v2.npy\") #override roll = 0.0001 on render \n",
    "    # #remove points too close to origin\n",
    "    d = np.sqrt(np.sum(nerf_pc**2, axis = 1))\n",
    "    good  = np.argwhere(d > 2.)[:,0]\n",
    "    nerf_pc = nerf_pc[good]\n",
    "\n",
    "    n_rots = 128\n",
    "    n_vert_patches = 1\n",
    "    n_cols_to_skip = n_rots // 8 \n",
    "    poses_index = start_frame_idx*(n_rots * n_vert_patches - 2*n_cols_to_skip*n_vert_patches)\n",
    "    # print(poses[poses_index])\n",
    "    nerf_to_map = poses[poses_index].copy()\n",
    "    nerf_to_map[:3,-1] *= 200\n",
    "    # nerf_to_map[:3,:3] = nerf_to_map[:3,:3] @ R.from_euler('xyz', [0.,0.,-np.pi/4]).as_matrix()\n",
    "\n",
    "    nerf_pc_rotated = nerf_pc @ R.from_euler('xyz', [0.,0.,np.pi/4 - np.pi/128]).as_matrix()\n",
    "    nerf_mapframe = (nerf_to_map @ np.append(nerf_pc_rotated, np.ones([len(nerf_pc), 1]), axis = 1).T).T\n",
    "    nerf_mapframe = nerf_mapframe[:,:3]\n",
    "    \n",
    "    nerf_map = np.append(nerf_map, nerf_mapframe, axis = 0)\n",
    "\n",
    "    \n",
    "#downsample using voxel grid\n",
    "import open3d as o3d\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(nerf_map)\n",
    "voxel_size = 0.02\n",
    "pcd = pcd.voxel_down_sample(voxel_size)\n",
    "nerf_map = np.asarray(pcd.points)\n",
    "\n",
    "\n",
    "#crop\n",
    "nerf_map = nerf_map[nerf_map[:,1] < 50]\n",
    "nerf_map = nerf_map[nerf_map[:,0] < 25]\n",
    "print(len(nerf_map))\n",
    "\n",
    "# save to .ply file to render with open3d\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(nerf_map)\n",
    "pcd.estimate_normals()\n",
    "o3d.io.write_point_cloud(\"newerCollegePLINKMap.ply\", pcd)\n",
    "np.save(\"generatedPointClouds/newerCollegePLINKMap.npy\", nerf_map)\n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "disp.append(Points(nerf_map, c = \"gray\", r = 3., alpha = 0.1))\n",
    "# disp.append(Points(hd_map[:,:3], c = \"gray\", r = 2, alpha = 0.5))\n",
    "\n",
    "plt.show(disp, \"Combining NeRF-Generated Scans\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxb = -np.pi/4\n",
    "minb = -np.pi\n",
    "\n",
    "#crop NeRF scan to azimuthal window~~~~~~~~~~~~~~\n",
    "nerf_pc_spherical = cartesian_to_spherical(nerf_pc)\n",
    "order = np.argsort(nerf_pc_spherical[:,1])#sort by azimuth angle\n",
    "nerf_pc_spherical = nerf_pc_spherical.numpy()[order,:]\n",
    "\n",
    "#crop by azimuth angle\n",
    "nerf_pc_spherical = nerf_pc_spherical[nerf_pc_spherical[:,1] < maxb]\n",
    "nerf_pc_spherical = nerf_pc_spherical[nerf_pc_spherical[:,1] > minb]\n",
    "nerf_pc = spherical_to_cartesian(nerf_pc_spherical).numpy()\n",
    "\n",
    "#crop raw scan to azimuthal window~~~~~~~~~~~~~~~\n",
    "real_pc_spherical = cartesian_to_spherical(real_pc)\n",
    "order = np.argsort(real_pc_spherical[:,1])#sort by azimuth angle\n",
    "real_pc_spherical = real_pc_spherical.numpy()[order,:]\n",
    "\n",
    "#crop by azimuth angle\n",
    "real_pc_spherical = real_pc_spherical[real_pc_spherical[:,1] < maxb]    \n",
    "real_pc_spherical = real_pc_spherical[real_pc_spherical[:,1] > minb]    \n",
    "real_pc = spherical_to_cartesian(real_pc_spherical).numpy()\n",
    "\n",
    "\n",
    "# center map on NeRF scan so we can crop it to prevent the dreaded OOM ~~~~~~~~\n",
    "start_frame_idx = 480 #120\n",
    "n_rots = 128\n",
    "n_vert_patches = 1\n",
    "n_cols_to_skip = n_rots // 8 \n",
    "poses_index = start_frame_idx*(n_rots * n_vert_patches - 2*n_cols_to_skip*n_vert_patches)\n",
    "# print(poses[poses_index])\n",
    "nerf_to_map = poses[poses_index].copy()\n",
    "nerf_to_map[:3,-1] *= 200\n",
    "map_nerfframe = (np.linalg.pinv(nerf_to_map) @ np.append(hd_map, np.ones([len(hd_map), 1]), axis = 1).T).T\n",
    "map_nerfframe = map_nerfframe[:,:3] @ R.from_euler('xyz', [0.,0., np.pi/4 - np.pi/128]).as_matrix().T\n",
    "\n",
    "#crop HD Map to azimuthal window\n",
    "map_nerfframe_spherical = cartesian_to_spherical(map_nerfframe)\n",
    "order = np.argsort(map_nerfframe_spherical[:,1])#sort by azimuth angle\n",
    "map_nerfframe_spherical = map_nerfframe_spherical.numpy()[order,:]\n",
    "\n",
    "#crop by azimuth angle\n",
    "# map_nerfframe_spherical = map_nerfframe_spherical[map_nerfframe_spherical[:,1] < -np.pi/2] #debug for viz\n",
    "\n",
    "map_nerfframe = spherical_to_cartesian(map_nerfframe_spherical).numpy()\n",
    "print(len(map_nerfframe))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "disp.append(Points(real_pc[:,:], c = \"red\", r = 3., alpha = 0.9))\n",
    "disp.append(Points(nerf_pc[:,:], c = \"blue\", r = 3., alpha = 0.9))\n",
    "disp.append(Points(map_nerfframe[:,:3], c = \"gray\", r = 2, alpha = 0.5))\n",
    "\n",
    "plt.show(disp, \"testing CD\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "points1 = torch.tensor(nerf_pc.copy(), dtype=torch.float32)\n",
    "points2 = torch.tensor(map_nerfframe[:,:3].copy(), dtype=torch.float32)\n",
    "\n",
    "dists = torch.cdist(points1, points2)\n",
    "# print(dists)\n",
    "# Find the minimum distances\n",
    "min_dists1 = dists.min(dim=1)[0]  # from points1 to points2\n",
    "\n",
    "#Accuracy--\n",
    "#\"The accuracy metric is defined as the mean distance of points on the output \n",
    "#   mesh to their nearest neighbors on the ground truth mesh.\" --\n",
    "accuracy = min_dists1.mean()\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(map_nerfframe[:,:3])\n",
    "o3d.io.write_point_cloud(\"NCStitchedMap.ply\", pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dac8bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# real_pc = np.linspace(0,9,10)[:,None]*np.array([1.,0.,0.])\n",
    "# nerf_pc = np.linspace(0,9,10)[:,None]*np.array([1.,1.,0.]) + np.array([0.,1.,0])\n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "# disp.append(Points(real_pc[:,:], c = \"red\", r = 5))\n",
    "disp.append(Points(real_pc[:,:], c = \"gray\", r = 2, alpha = 0.5))\n",
    "# disp.append(Points(nerf_pc[:,:], c = \"blue\", r = 3., alpha = 0.9))\n",
    "disp.append(Points(nerf_mapframe[:,:], c = \"blue\", r = 3., alpha = 0.9))\n",
    "\n",
    "plt.show(disp, \"testing CD\")\n",
    "ViewInteractiveWidget(plt.window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14436ae7",
   "metadata": {},
   "source": [
    "# manually calcluate CD with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946bc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "points1 = torch.tensor(nerf_pc.copy(), dtype=torch.float32)\n",
    "points2 = torch.tensor(real_pc.copy(), dtype=torch.float32)\n",
    "\n",
    "dists = torch.cdist(points1, points2)\n",
    "# print(dists)\n",
    "# Find the minimum distances\n",
    "min_dists1 = dists.min(dim=1)[0]  # from points1 to points2\n",
    "\n",
    "#Accuracy--\n",
    "#\"The accuracy metric is defined as the mean distance of points on the output \n",
    "#   mesh to their nearest neighbors on the ground truth mesh.\" --\n",
    "accuracy = min_dists1.mean()\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "min_dists2 = dists.min(dim=0)[0]  # from points2 to points1\n",
    "\n",
    "#completeness--\n",
    "# The completeness metric is defined similarly, but in opposite direction.\n",
    "completeness = min_dists2.mean()\n",
    "print(\"completeness:\", completeness)\n",
    "\n",
    "#chamfer distance\n",
    "cd = (min_dists1.mean() + min_dists2.mean())/2\n",
    "print(\"cd:\", cd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "good2 = np.where(min_dists2 < 0.2)[0]\n",
    "good2 = len(good2)/len(min_dists2)\n",
    "\n",
    "print(good2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (15.59+6.86)/2\n",
    "\n",
    "#exclude very beginning and end where points don't align due to staggering\n",
    "test1 = min_dists1[1000:-1000]\n",
    "test2 = min_dists2[1000:-1000]\n",
    "\n",
    "thresh = np.sqrt((np.sqrt(2)*.2)**2 + .2**2 )\n",
    "print(thresh)\n",
    "test1 = test1[test1 < thresh]\n",
    "test2 = test2[test2 < thresh]\n",
    "\n",
    "print(test1.mean(), test2.mean())\n",
    "print(test1.std(), test2.std())\n",
    "\n",
    "fig, ax = p.subplots()\n",
    "\n",
    "# ax.plot(min_dists1)\n",
    "ax.plot(test1)\n",
    "ax.set_xlim([4000,5000])\n",
    "\n",
    "print(min_dists1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b684c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43025c05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd_hist = []\n",
    "accuracy_hist = []\n",
    "completeness_hist = []\n",
    "good1_hist = []\n",
    "good2_hist = []\n",
    "\n",
    "for frame_idx in range(1): #25\n",
    "    print(\"frame: \", frame_idx, \"---------------\")\n",
    "\n",
    "    for spin_part in range(8):\n",
    "#         nerf_pc = np.load(\"generatedPointClouds/MaiCityNeRFOutput_frame_\" + str(frame_idx) + \".npy\")\n",
    "#         real_pc = np.load(\"generatedPointClouds/MaiCityReal_frame_\" + str(frame_idx) + \".npy\")\n",
    "        nerf_pc = np.load(\"generatedPointClouds/MaiCityNeRFOutput_v2_frame_\" + str(frame_idx) + \".npy\")\n",
    "        real_pc = np.load(\"generatedPointClouds/MaiCityReal_v2_frame_\" + str(frame_idx) + \".npy\")\n",
    "\n",
    "        # #remove ground plane\n",
    "        # real_pc = real_pc[real_pc[:,2] > -1]\n",
    "        # nerf_pc = nerf_pc[nerf_pc[:,2] > -1]\n",
    "\n",
    "        # #debug: create simple pc\n",
    "        # nerf_pc = np.array([[0,0,0], \n",
    "        #                     [1,1,1]])\n",
    "        # real_pc = np.array([[0,0,0], \n",
    "        #                     [0,0,0]])\n",
    "\n",
    "        #crop real PC down the same as NeRF output so bidirectional CD doesn't punish non-overlaps\n",
    "        # real_pc = real_pc[len(real_pc)//8:7*len(real_pc)//8,:]\n",
    "        # real_pc = real_pc[len(real_pc)//8:29*len(real_pc)//32,:]\n",
    "\n",
    "\n",
    "        #remove points too close to origin\n",
    "        d = np.sqrt(np.sum(nerf_pc**2, axis = 1))\n",
    "        good  = np.argwhere(d > 1)[:,0]\n",
    "        nerf_pc = nerf_pc[good]\n",
    "\n",
    "        # #remove points out of range of sensor (only needed for Mai City)\n",
    "        # d = np.sqrt(np.sum(nerf_pc**2, axis = 1))\n",
    "        # good = np.argwhere(d < 100)[:,0]\n",
    "        # nerf_pc = nerf_pc[good]\n",
    "\n",
    "        nerf_pc = nerf_pc[spin_part*len(real_pc)//8:(spin_part+1)*len(nerf_pc)//8,:]\n",
    "        real_pc = real_pc[spin_part*len(real_pc)//8:(spin_part+1)*len(real_pc)//8,:]\n",
    "\n",
    "        points1 = torch.tensor(nerf_pc)\n",
    "        points2 = torch.tensor(real_pc)\n",
    "\n",
    "        dists = torch.cdist(points1, points2)\n",
    "        # print(dists)\n",
    "        # Find the minimum distances\n",
    "        min_dists1 = dists.min(dim=1)[0]  # from points1 to points2\n",
    "        min_dists2 = dists.min(dim=0)[0]  # from points2 to points1\n",
    "\n",
    "    #     print(min_dists1, min_dists1.mean())\n",
    "    #     print(min_dists2, min_dists2.mean())\n",
    "\n",
    "        cd = (min_dists1.mean() + min_dists2.mean())/2        \n",
    "        accuracy = min_dists2.mean() #TEST-- try flipping these values...\n",
    "        completeness = min_dists1.mean()\n",
    "        print(\"\\n cd:\", cd, \"  accuracy: \", accuracy, \" completeness:\", completeness)\n",
    "\n",
    "        cd_hist = np.append(cd_hist, cd)\n",
    "        accuracy_hist = np.append(accuracy_hist, accuracy)\n",
    "        completeness_hist = np.append(completeness_hist, completeness)\n",
    "        \n",
    "        good1 = np.where(min_dists1 < 0.1)[0]\n",
    "        good1 = len(good1)/len(min_dists1)\n",
    "\n",
    "        good2 = np.where(min_dists2 < 0.1)[0]\n",
    "        good2 = len(good2)/len(min_dists2)\n",
    "\n",
    "        good1_hist = np.append(good1_hist, good1)\n",
    "        good2_hist = np.append(good2_hist, good2)\n",
    "        print(\"good1, good2:\", good1, good2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495aa59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = p.subplots()\n",
    "# ax.plot(cd_hist, label = \"CD\")\n",
    "# ax.plot(completeness_hist, label = \"completeness\")\n",
    "# ax.plot(accuracy_hist, label = \"accuracy\")\n",
    "ax.plot(good1_hist, label = \"gh1\")\n",
    "ax.plot(good2_hist, label = \"gh2\")\n",
    "ax.plot( np.mean(good1_hist**2)*np.ones([400]))\n",
    "\n",
    "cutoff = 5000\n",
    "\n",
    "print(np.mean(cd_hist[:cutoff]))\n",
    "print(np.mean(accuracy_hist[:cutoff]))\n",
    "print(np.mean(completeness_hist[:cutoff]))\n",
    "print(np.mean((good1_hist)[:cutoff])) #too high to be F-score...?\n",
    "print(np.mean((good2_hist)[:cutoff])) #too high to be F-score...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ec2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG ONLY-- look at data distribution witout tails\n",
    "\n",
    "bidx = np.linspace(0,3,100)\n",
    "\n",
    "fig, ax = p.subplots()\n",
    "d1 = min_dists1.numpy()\n",
    "d1_good_only = d1[d1 < 0.2]\n",
    "\n",
    "d2 = min_dists2.numpy()\n",
    "d2_good_only = d2[d2 < 0.2]\n",
    "\n",
    "ax.hist(min_dists1, bins = bidx, alpha = 0.5);\n",
    "ax.hist(d1_good, bins = bidx, alpha = 0.5);\n",
    "\n",
    "print(np.mean(d1))\n",
    "print(np.mean(d1_good_only))\n",
    "\n",
    "print(np.mean(d2))\n",
    "print(np.mean(d2_good_only))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a6327",
   "metadata": {},
   "source": [
    "# TEST: Get F-Score using torch cdists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd08931",
   "metadata": {},
   "outputs": [],
   "source": [
    "points1 = torch.tensor(nerf_pc)\n",
    "points2 = torch.tensor(real_pc)\n",
    "\n",
    "dists = torch.cdist(points1, points2)\n",
    "# print(dists)\n",
    "# Find the minimum distances\n",
    "min_dists1 = dists.min(dim=1)[0]  # from points1 to points2\n",
    "min_dists2 = dists.min(dim=0)[0]  # from points2 to points1\n",
    "print(min_dists1, min_dists1.mean())\n",
    "\n",
    "good1 = np.where(min_dists1 < 0.1)[0]\n",
    "print(len(good1)/len(min_dists1))\n",
    "\n",
    "good2 = np.where(min_dists2 < 0.1)[0]\n",
    "print(len(good2)/len(min_dists2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d4ddf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "real_pc = torch.tensor(real_pc)\n",
    "nerf_pc = torch.tensor(nerf_pc)\n",
    "\n",
    "if len(real_pc.shape) == 2:\n",
    "    real_pc = real_pc.unsqueeze(0)  # Add batch dimension\n",
    "if len(nerf_pc.shape) == 2:\n",
    "    nerf_pc = nerf_pc.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "# Convert to float\n",
    "real_pc = real_pc.float()\n",
    "nerf_pc = nerf_pc.float()\n",
    "print(nerf_pc.shape)\n",
    "\n",
    "chamferDist = ChamferDistance()\n",
    "dist_forward = chamferDist(nerf_pc, real_pc)\n",
    "print(dist_forward)\n",
    "# print(dist_forward/ nerf_pc.shape[1])\n",
    "# dist_bidirectional = chamferDist(nerf_pc, real_pc, bidirectional=True)\n",
    "# print(dist_bidirectional)\n",
    "# print(dist_bidirectional/ nerf_pc.shape[1])\n",
    "\n",
    "# #TEST -- modify chamfer.py to produce output for calculating F-scores\n",
    "# # dist_forward = chamferDist(nerf_pc, real_pc, reduction = 'fscore')\n",
    "# dist_forward = chamferDist(nerf_pc, real_pc, reduction = 'fscore', reverse = True)\n",
    "# # dist_forward is actually a SQUARED distance, so I need to take the sqrt first\n",
    "# print(dist_forward)\n",
    "# dist_forward = torch.sqrt(dist_forward)\n",
    "# print(dist_forward)\n",
    "\n",
    "# bad = np.argwhere(dist_forward > 0.2)\n",
    "\n",
    "# print(np.shape(dist_forward)[1])\n",
    "# print(np.shape(bad)[1])\n",
    "\n",
    "# print(1 - np.shape(bad)[1] / np.shape(dist_forward)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b28309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate map accuracy\n",
    "good = np.argwhere(dist_forward < 0.2)[1,:]\n",
    "print(len(good))\n",
    "print(len(dist_forward[0]))\n",
    "a = torch.mean(dist_forward[0][good])\n",
    "# a = torch.mean(dist_forward[0])\n",
    "print(a)\n",
    "\n",
    "# print(dist_forward[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0.9341 #true positives\n",
    "fp = 1 - 0.9341 #false positives\n",
    "\n",
    "#false negatives -- no estimated point close enough\n",
    "fn = 1-0.9286 #use reverse nn distnaces here\n",
    "\n",
    "\n",
    "# precision\n",
    "p = tp/(tp+fp)\n",
    "\n",
    "#recall\n",
    "r = tp/(tp+fn)\n",
    "\n",
    "f_score = 2*(p*r)/(p+r)\n",
    "print(f_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(savepc)\n",
    "# print(pc1_aligned)\n",
    "# print(dist_forward/ np.shape(nerf_pc)[1])\n",
    "\n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "#crop real PC down the same as NeRF output so bidirectional CD doesn't punish non-overlaps\n",
    "# test = real_pc[0,len(real_pc[0,:,:])//8:7*len(real_pc[0,:,:])//8,:]\n",
    "# print(np.shape(test))\n",
    "# print(len(real_pc[0,:,:])//8)\n",
    "# disp.append(Points(test, c = \"red\", r = 5))\n",
    "\n",
    "disp.append(Points(real_pc[0,:,:], c = \"red\", r = 5))\n",
    "disp.append(Points(nerf_pc[0,:,:], c = \"blue\", r = 5))\n",
    "\n",
    "plt.show(disp, \"testing CD\")\n",
    "ViewInteractiveWidget(plt.window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ed52c",
   "metadata": {},
   "source": [
    "# calculate chamfer distance between NeRF output true distance for 25 frames Mai City Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb639ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CD_hist = []\n",
    "chamferDist = ChamferDistance()\n",
    "\n",
    "for frame_idx in range(25):\n",
    "    print(frame_idx)\n",
    "    nerf_pc = np.load(\"generatedPointClouds/MaiCityNeRFOutput_frame_\" + str(frame_idx) + \".npy\")\n",
    "    real_pc = np.load(\"generatedPointClouds/MaiCityReal_frame_\" + str(frame_idx) + \".npy\")\n",
    "\n",
    "    #remove points too close to origin\n",
    "    d = np.sqrt(np.sum(nerf_pc**2, axis = 1))\n",
    "    good  = np.argwhere(d > 1)[:,0]\n",
    "    nerf_pc = nerf_pc[good]\n",
    "\n",
    "    #remove points out of range of sensor (only needed for Mai City)\n",
    "    d = np.sqrt(np.sum(nerf_pc**2, axis = 1))\n",
    "    good = np.argwhere(d < 100)[:,0]\n",
    "    nerf_pc = nerf_pc[good]\n",
    "    \n",
    "#     print(np.shape(nerf_pc))\n",
    "#     nerf_pc = nerf_pc[::3,:]\n",
    "#     real_pc = real_pc[::3,:]\n",
    "#     print(np.shape(nerf_pc))\n",
    "\n",
    "    real_pc = torch.tensor(real_pc)\n",
    "    nerf_pc = torch.tensor(nerf_pc)\n",
    "\n",
    "    if len(real_pc.shape) == 2:\n",
    "        real_pc = real_pc.unsqueeze(0)  # Add batch dimension\n",
    "    if len(nerf_pc.shape) == 2:\n",
    "        nerf_pc = nerf_pc.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Convert to float\n",
    "    real_pc = real_pc.float()\n",
    "    nerf_pc = nerf_pc.float()\n",
    "    \n",
    "#     dist_bidirectional = chamferDist(nerf_pc, real_pc, bidirectional=True)\n",
    "#     dist_bidirectional /= nerf_pc.shape[1]\n",
    "#     print(dist_bidirectional)\n",
    "#     CD_hist = np.append(CD_hist, dist_bidirectional)\n",
    "\n",
    "\n",
    "    dist_forward = chamferDist(nerf_pc, real_pc)\n",
    "#     dist_forward /= nerf_pc.shape[1]\n",
    "    print(dist_forward)\n",
    "    CD_hist = np.append(CD_hist, dist_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as p \n",
    "\n",
    "print(np.mean(CD_hist))\n",
    "# np.save(\"generatedPointClouds/MaiCity_CD_hist.npy\", CD_hist)\n",
    "# CD_hist = np.load(\"generatedPointClouds/MaiCity_CD_hist.npy\")\n",
    "# print(CD_hist)\n",
    "fig, ax = p.subplots()\n",
    "ax.plot(CD_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40609085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as p\n",
    "\n",
    "# CD_hist_2 = CD_hist.copy()\n",
    "\n",
    "fig, ax = p.subplots()\n",
    "ax.set_title(\"Chamfer Distance for Mai City Dataset\")\n",
    "# ax.plot(CD_hist_15, label = \"sample every 15th\")\n",
    "# ax.plot(CD_hist_10, label = \"sample every 10th\")\n",
    "# ax.plot(CD_hist_5, label = \"sample every 5th\")\n",
    "# ax.plot(CD_hist_2, label = \"sample every 2nd\")\n",
    "ax.plot(CD_hist, label = \"CD\")\n",
    "ax.set_xlabel(\"test frame index\")\n",
    "ax.set_ylabel(\"Chamfer Distance (cm)\")\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e91e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combine all NeRF outputs to single point cloud to render with open3d\n",
    "combined_pc = np.zeros([0,3])\n",
    "for frame_idx in range(50): #25\n",
    "    print(frame_idx)\n",
    "#     nerf_pc = np.load(\"generatedPointClouds/MaiCityNeRFOutput_frame_\" + str(frame_idx) + \".npy\") #\n",
    "    nerf_pc = np.load(\"generatedPointClouds/MaiCityNeRFOutput_velodyne_format_frame_\" + str(frame_idx) + \".npy\") #OS1 projections\n",
    "#     nerf_pc = np.load(\"generatedPointClouds/MaiCityReal_frame_\" + str(frame_idx) + \".npy\") #DEBUG -- true scans\n",
    "\n",
    "    #remove near zero points\n",
    "#     nerf_pc = nerf_pc*200\n",
    "    dists = np.sqrt(nerf_pc[:,0]**2 + nerf_pc[:,1]**2 + nerf_pc[:,2]**2)\n",
    "    good = np.argwhere(dists > 2)[:,0]\n",
    "#     print(good)\n",
    "    nerf_pc = nerf_pc[good]\n",
    "\n",
    "    nerf_pc = nerf_pc + np.array([frame_idx*.5,0.,0.])\n",
    "#     nerf_pc = nerf_pc + np.array([frame_idx*1.,0.,0.])\n",
    "    \n",
    "    combined_pc = np.append(combined_pc, nerf_pc, axis = 0)\n",
    "\n",
    "# print(np.shape(combined_pc))\n",
    "\n",
    "# #crop to help with marching cubes??\n",
    "# combined_pc = combined_pc[abs(combined_pc[:,0]) < 20]\n",
    "# combined_pc = combined_pc[abs(combined_pc[:,1]) < 20]\n",
    "# np.save(\"generatedPointClouds/CombinedMaiCity.npy\", combined_pc)\n",
    "\n",
    "import open3d as o3d\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(combined_pc)\n",
    "o3d.io.write_point_cloud(\"MaiCityNeRF.ply\", pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f788dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST -- load and crop HD Map to test marching cubes\n",
    "import trimesh\n",
    "pl = '/media/derm/06EF-127D4/Newer College Dataset/new-college-29-01-2020-1cm-resolution-1stSection - mesh.ply'\n",
    "raw_points = trimesh.load(pl).vertices\n",
    "# points[:,2] = -points[:,2] #flip z\n",
    "# points = points[::10,:] #downsample\n",
    "points = raw_points.copy()\n",
    "\n",
    "points = points[points[:,0]>0]\n",
    "points = points[points[:,1]> - 2]\n",
    "\n",
    "combined_pc = points\n",
    "# np.save(\"generatedPointClouds/NCTestMap.npy\", combined_pc)\n",
    "\n",
    "import open3d as o3d\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(combined_pc)\n",
    "o3d.io.write_point_cloud(\"NCTestMap.ply\", pcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "disp.append(Points(combined_pc, c = \"red\", r = 2.5, alpha = 0.3))\n",
    "disp.append(Points(raw_points, c = \"black\", r = 2.5, alpha = 0.1))\n",
    "\n",
    "plt.show(disp, \"testing CD\")\n",
    "ViewInteractiveWidget(plt.window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80657c",
   "metadata": {},
   "source": [
    "### Load 100 raw frames from Mai City sequence 01 and combine to a single point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3627ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test = np.loadtxt(\"/media/derm/06EF-127D4/mai_city/txt/01/000000\")\n",
    "\n",
    "combined_pc = np.zeros([0,3])\n",
    "for idx1 in range(100):\n",
    "    print(idx1)\n",
    "    pcfn = \"/media/derm/06EF-127D4/mai_city/txt/01/\" +  f\"{idx1:06}\"\n",
    "    \n",
    "    pc = np.loadtxt(pcfn)\n",
    "    pc += np.array([1.*idx1, 0.,0.])\n",
    "    pc = pc[::2]\n",
    "    \n",
    "    combined_pc = np.append(combined_pc, pc, axis = 0)\n",
    "\n",
    "print(combined_pc)\n",
    "import open3d as o3d\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(combined_pc)\n",
    "o3d.io.write_point_cloud(\"MaiCityGTMap.ply\", pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259bc96",
   "metadata": {},
   "source": [
    "# Calculate CD using pytorch3d chafer_distance() loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d729abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868d7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
