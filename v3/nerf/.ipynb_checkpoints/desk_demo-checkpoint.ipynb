{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcd45ce",
   "metadata": {},
   "source": [
    "### Using COLMAP Structure From Motion (SFM) Package\n",
    "\n",
    "see <https://github.com/Fyusion/LLFF/tree/master> for more documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc949426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pillow_heif import register_heif_opener\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from colmapParsingUtils import *\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 22*1024\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# current = os.getcwd()\n",
    "# sys.path.append(current+\"/LLFF\")\n",
    "\n",
    "# from LLFF.llff.poses.pose_utils import gen_poses\n",
    "# import sys\n",
    "\n",
    "#<imgs2poses.py>:\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--match_type', type=str, \n",
    "#                     default='exhaustive_matcher', help='type of matcher used.  Valid options: \\\n",
    "#                     exhaustive_matcher sequential_matcher.  Other matchers not supported at this time')\n",
    "# parser.add_argument('scenedir', type=str,\n",
    "#                     help='input scene directory')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# if args.match_type != 'exhaustive_matcher' and args.match_type != 'sequential_matcher':\n",
    "#     print('ERROR: matcher type ' + args.match_type + ' is not valid.  Aborting')\n",
    "#     sys.exit()\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     gen_poses(args.scenedir, args.match_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert HEIC images to PNG\n",
    "\n",
    "# register_heif_opener()\n",
    "\n",
    "# # image_i = Image.open('desk_images/IMG_2072.HEIC')\n",
    "# image_i = Image.open('bike_images/IMG_2167.HEIC')\n",
    "# i = np.asarray(image_i)\n",
    "# i = i[516:-516,12:-12,:] #crop square\n",
    "# i = i[::3,::3,:]\n",
    "# print(np.shape(i))\n",
    "# plt.imshow(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7faff78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loop through subdirectory and convert each .HEIC to similarly named .PNG\n",
    "register_heif_opener()\n",
    "# rootdir = 'desk_images'\n",
    "rootdir = 'bike_images'\n",
    "\n",
    "mega_image_array = np.zeros([0,1000,1000,3])\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith((\"HEIC\")):\n",
    "            print(file)\n",
    "            image_i = Image.open(rootdir+'/'+file)\n",
    "            i = np.asarray(image_i)\n",
    "#             #crop square ~~~~~~~~~~~~~~~~\n",
    "#             if np.shape(i)[0] > np.shape(i)[1]:\n",
    "#                 i = i[516:-516,12:-12,:]\n",
    "#             else:\n",
    "#                 i = i[12:-12,516:-516,:]\n",
    "#             i = i[::8,::8,:]\n",
    "#             #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#             mega_image_array = np.append(mega_image_array, i[None,:,:,:], axis = 0)\n",
    "#             im = Image.fromarray(i)\n",
    "#             im.save(rootdir+\"/\"+file[:-5]+'.png')\n",
    "#             os.remove(file) #remove HEIC image\n",
    "\n",
    "            #only save landscape images\n",
    "            if np.shape(i)[0] < np.shape(i)[1]:\n",
    "                i = i[::8,::8,:]\n",
    "                im = Image.fromarray(i)\n",
    "                im.save(rootdir+\"/\"+file[:-5]+'.png')\n",
    "\n",
    "# np.savez_compressed(\"desk_images\", mega_image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "4032/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load poses estimated by COLMAP\n",
    "\n",
    "# images_from_colmap = read_images_text(\"sparseDesk/sparse/0/images.txt\")\n",
    "# cameras = read_cameras_text(\"sparseDesk/sparse/0/cameras.txt\")\n",
    "# pts3d = read_points3D_text(\"sparseDesk/sparse/0/points3D.txt\")\n",
    "images_from_colmap = read_images_text(\"sparseBike/sparse/0/images.txt\")\n",
    "cameras = read_cameras_text(\"sparseBike/sparse/0/cameras.txt\")\n",
    "pts3d = read_points3D_text(\"sparseBike/sparse/0/points3D.txt\")\n",
    "\n",
    "print(images_from_colmap[1])\n",
    "# print(pts3d[1])\n",
    "print(len(cameras))\n",
    "print(\"\\n\",cameras[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855265d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert COLMAP poses (xyz,quats) to rotm\n",
    "\n",
    "poses = np.zeros([len(images_from_colmap),4,4])\n",
    "\n",
    "# images = np.zeros([len(poses),100,100,3])\n",
    "images = np.zeros([len(poses),189,252,3])\n",
    "# images = np.zeros([len(poses),250,250,3])\n",
    "# images = np.zeros([len(poses),1000,1000,3]) #full resolution\n",
    "\n",
    "#loop through <images_from_colmap> to get 3D poses of cameras at each timestamp\n",
    "# print(len(images_from_colmap))\n",
    "for n in range(len(images_from_colmap)):\n",
    "\n",
    "    \n",
    "    trans31 = images_from_colmap[n+1].tvec[:,None]\n",
    "    \n",
    "# #     was this ~~~~~~~~~~~~~~~~~~~~~\n",
    "#     QWXYZ = images_from_colmap[n+1].qvec\n",
    "#     QXYZW = np.array([QWXYZ[1],QWXYZ[2],QWXYZ[3],QWXYZ[0]])\n",
    "#     rot33 = R.from_quat(QXYZW).as_matrix()\n",
    "# #     rot33 = np.linalg.pinv(rot33)#test\n",
    "    \n",
    "#     #asdfasdf\n",
    "#     rot33[0,:] =  -rot33[0,:]\n",
    "#     rot33[1,:] =  -rot33[1,:]\n",
    "\n",
    "\n",
    "#     poses[n] = np.append(np.append(rot33, trans31, axis = 1), np.array([[0,0,0,1.]]), axis = 0)    \n",
    "# #     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    #shamelessly stolen from instant-ngp ~~~~~~~~~\n",
    "    from colmap2nerf import qvec2rotmat\n",
    "    \n",
    "#     #take 1...\n",
    "#     QWXYZ = images_from_colmap[n+1].qvec\n",
    "#     test= qvec2rotmat(QWXYZ)\n",
    "#     poses[n] = np.append(np.append(test, trans31, axis = 1), np.array([[0,0,0,1.]]), axis = 0)    \n",
    "\n",
    "\n",
    "    #take2....\n",
    "    qvec = images_from_colmap[n+1].qvec #raw\n",
    "#     QWXYZ = images_from_colmap[n+1].qvec\n",
    "#     qvec = np.array([QWXYZ[1],QWXYZ[2],QWXYZ[3],QWXYZ[0]])\n",
    "    tvec = images_from_colmap[n+1].tvec[:,None]\n",
    "    \n",
    "    t = tvec.reshape([3,1])\n",
    "    R = qvec2rotmat(-qvec)\n",
    "    \n",
    "    bottom = np.array([0.0, 0.0, 0.0, 1.0]).reshape([1, 4])\n",
    "    m = np.concatenate([np.concatenate([R, t], 1), bottom], 0)\n",
    "    c2w = np.linalg.inv(m)\n",
    "    \n",
    "    c2w[0:3,2] *= -1 # flip the y and z axis\n",
    "    c2w[0:3,1] *= -1\n",
    "    c2w = c2w[[1,0,2,3],:]\n",
    "    c2w[2,:] *= -1 # flip whole world upside down\n",
    "\n",
    "#     up += c2w[0:3,1]\n",
    "\n",
    "    poses[n] = c2w\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    #line up z with origin\n",
    "    poses[n,2,-1] += 16 #desk\n",
    "#     poses[n,2,-1] += 10 #bike\n",
    "\n",
    "    #downscale images\n",
    "    #sync order of images with order of poses\n",
    "    temp = cv2.imread(\"bike_images/\"+images_from_colmap[n+1].name)/255    \n",
    "#     temp = cv2.imread(\"desk_images/\"+images_from_colmap[n+1].name)/255    \n",
    "#     temp = cv2.resize(temp, dsize=(250,250), interpolation=cv2.INTER_CUBIC)\n",
    "#     temp = cv2.resize(temp, dsize=(100,100), interpolation=cv2.INTER_LINEAR)\n",
    "    temp = cv2.resize(temp, dsize=(252,189), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "#     #try cropping instead of downscaling\n",
    "#     # Calculate the center of the image\n",
    "#     image = cv2.imread(\"bike_images/\"+images_from_colmap[n+1].name)/255    \n",
    "#     center_x, center_y = image.shape[1] // 2, image.shape[0] // 2\n",
    "#     # Calculate cropping boundaries\n",
    "#     crop_x1 = center_x - np.shape(images)[1] // 2\n",
    "#     crop_x2 = center_x + np.shape(images)[1] // 2\n",
    "#     crop_y1 = center_y - np.shape(images)[1] // 2\n",
    "#     crop_y2 = center_y + np.shape(images)[1] // 2\n",
    "#     # Crop the image\n",
    "#     temp = image[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "    \n",
    "    images[n,:,:,0] = temp[:,:,2]\n",
    "    images[n,:,:,1] = temp[:,:,1]\n",
    "    images[n,:,:,2] = temp[:,:,0]\n",
    "\n",
    "#GET REST OF PARAMS NEEDED FOR tinyNeRF format~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
    "\n",
    "#fix order of colors\n",
    "images[:,:,:,0], images[:,:,:,1] = images[:,:,:,1], images[:,:,:,0]\n",
    "\n",
    "H,W = images.shape[1:3]\n",
    "# print(H,W)\n",
    "testimg, testpose = images[55], poses[55]\n",
    "\n",
    "focal = cameras[1].params[0] #test- see if same focal length can be shared across all images\n",
    "# focal = np.array(984.411).astype(np.double) #old\n",
    "# focal = np.array(98.4411).astype(np.double) # IMPORTANT --> LOOKS LIKE THIS NEEDS TO BE SCALED WHEN IMAGES ARE DOWNSAMPLED!\n",
    "focal = int(focal/30)\n",
    "# focal = focal/12\n",
    "# focal=focal//3\n",
    "# focal = np.array((133)/10).astype(np.double) #test\n",
    "print(focal)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test using LLFF code to convert colmap output to proper NeRF c2w() format\n",
    "from LLFF.llff.poses.pose_utils import load_colmap_data, save_poses\n",
    "\n",
    "# realdir = \"sparseDesk/\"\n",
    "realdir = \"sparseBike/\"\n",
    "poses, pts3d, perm = load_colmap_data(realdir)\n",
    "\n",
    "# basedir = \"sparseDesk/\"\n",
    "basedir = \"sparseBike/\"\n",
    "save_poses(basedir, poses, pts3d, perm)\n",
    "# print(np.shape(poses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load(\"tiny_nerf/data/nerf_llff_data/desk/poses_bounds.npy\")\n",
    "print(np.shape(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c2917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug coordinate system in <poses>\n",
    "\n",
    "# #scale poses to unit cube\n",
    "# radii = np.sqrt(np.sum(poses[:,:3,-1]**2, axis = 1))\n",
    "# print(max(radii))\n",
    "# poses[:,:3,-1] = 3*poses[:,:3,-1]/max(radii)\n",
    "\n",
    "\n",
    "# camera_centers = ax.scatter3D(poses[:,0,-1],poses[:,1,-1],poses[:,2,-1])\n",
    "headings = poses[:,:3,:3] @ np.array([0,0,-1]) #old\n",
    "\n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "\n",
    "plt = Plotter(N = 1, axes = 1, bg = (1, 1, 1), interactive = True)\n",
    "disp=[]\n",
    "disp.append(Points(poses[:,:3,-1], c = \"#CB2314\"))\n",
    "disp.append(Arrows(poses[:,:3,-1], poses[:,:3,-1] + headings[:,:3], c = \"#CB2314\"))\n",
    "disp.append(Points(np.array([[0,0,0]]), c = 'black'))\n",
    "\n",
    "plt.show(disp, \"camera poses\")\n",
    "ViewInteractiveWidget(plt.window)\n",
    "\n",
    "## mpl:\n",
    "# from mpl_toolkits.mplot3d import axes3d\n",
    "# from matplotlib import cm\n",
    "# # %matplotlib notebook\n",
    "# import matplotlib.pyplot as plt\n",
    "# # print(poses[0])\n",
    "# ax = plt.figure().add_subplot(projection='3d')\n",
    "# ax.set_xlim([-6,6])\n",
    "# ax.set_ylim([-6,6])\n",
    "# ax.set_zlim([-6,6])\n",
    "# ax.set_xlabel('x')\n",
    "# ax.set_ylabel('y')\n",
    "# ax.set_zlabel('z')\n",
    "# ax.grid(False)\n",
    "# X, Y, Z = axes3d.get_test_data(0.05)\n",
    "# # ax.contour(X, Y, Z, cmap=cm.coolwarm)  # Plot contour curves\n",
    "\n",
    "# #plot axis\n",
    "# ax.scatter3D(0,0,0, color='purple')\n",
    "# ax.plot([0,1],[0,0],[0,0], color = 'red')\n",
    "# ax.plot([0,0],[0,1],[0,0], color = 'green')\n",
    "# ax.plot([0,0],[0,0],[0,1], color = 'blue')\n",
    "\n",
    "# ax.quiver(poses[:,0,-1],poses[:,1,-1],poses[:,2,-1], headings[:,0], headings[:,1], headings[:,2])\n",
    "## headings = poses[:,:3,:3] @ np.array([1,0,0])\n",
    "## ax.quiver(poses[:,0,-1],poses[:,1,-1],poses[:,2,-1], headings[:,0], headings[:,1], headings[:,2], color = 'red', alpha = 0.5)\n",
    "## headings = poses[:,:3,:3] @ np.array([0,1,0])\n",
    "## ax.quiver(poses[:,0,-1],poses[:,1,-1],poses[:,2,-1], headings[:,0], headings[:,1], headings[:,2], color = 'green', alpha = 0.5)\n",
    "## headings = poses[:,:3,:3] @ np.array([0,0,1]) \n",
    "## ax.quiver(poses[:,0,-1],poses[:,1,-1],poses[:,2,-1], headings[:,0], headings[:,1], headings[:,2], color = 'blue', alpha = 0.5)\n",
    "# print(len(poses))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm\n",
    "# %matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c57ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_from_colmap[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #verify images are set up correclty\n",
    "# plt.imshow(images[55])\n",
    "# # plt.imshow(images_from_colmap[1])\n",
    "\n",
    "# print(poses[55])\n",
    "\n",
    "# print(R.from_matrix(poses[55,:3,:3]).as_euler('xyz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3262e3c",
   "metadata": {},
   "source": [
    "# TinyNeRF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2241bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posenc(x):\n",
    "  rets = [x]\n",
    "  for i in range(L_embed):\n",
    "    for fn in [tf.sin, tf.cos]:\n",
    "      rets.append(fn(2.**i * x))\n",
    "  return tf.concat(rets, -1)\n",
    "\n",
    "L_embed = 16\n",
    "embed_fn = posenc\n",
    "# L_embed = 0\n",
    "# embed_fn = tf.identity\n",
    "\n",
    "def init_model(D=8, W=256): #D=8, W=256\n",
    "    relu = tf.keras.layers.ReLU()    \n",
    "    dense = lambda W=W, act=relu : tf.keras.layers.Dense(W, activation=act)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(3 + 3*2*L_embed)) \n",
    "    outputs = inputs\n",
    "    for i in range(D):\n",
    "        outputs = dense()(outputs)\n",
    "        if i%4==0 and i>0:\n",
    "            outputs = tf.concat([outputs, inputs], -1)\n",
    "    outputs = dense(4, act=None)(outputs)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
    "    dirs = tf.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -tf.ones_like(i)], -1)\n",
    "    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "\n",
    "def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, rand=False):\n",
    "\n",
    "    def batchify(fn, chunk=1024*32): #was 1024*32\n",
    "        return lambda inputs : tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    \n",
    "    # Compute 3D query points\n",
    "    z_vals = tf.linspace(near, far, N_samples) \n",
    "#     print(\"\\n z_vals: \\n\", z_vals)\n",
    "    if rand:\n",
    "      z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "#     print(\"\\n test: \\n\", tf.shape(rays_o[...,None,:]))\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "    \n",
    "    # Run network\n",
    "    pts_flat = tf.reshape(pts, [-1,3])\n",
    "#     print(\"\\n pts_flat shape: \\n\", tf.shape(pts_flat)) # <---------------- This is the OOM Bottleneck\n",
    "    pts_flat = embed_fn(pts_flat)\n",
    "    raw = batchify(network_fn)(pts_flat)\n",
    "    raw = tf.reshape(raw, list(pts.shape[:-1]) + [4])\n",
    "    \n",
    "    # Compute opacities and colors\n",
    "    sigma_a = tf.nn.relu(raw[...,3])\n",
    "    rgb = tf.math.sigmoid(raw[...,:3]) \n",
    "    \n",
    "    # Do volume rendering\n",
    "    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[...,:1].shape)], -1) \n",
    "    alpha = 1.-tf.exp(-sigma_a * dists)  \n",
    "    weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
    "    \n",
    "    rgb_map = tf.reduce_sum(weights[...,None] * rgb, -2) \n",
    "    depth_map = tf.reduce_sum(weights * z_vals, -1) \n",
    "    acc_map = tf.reduce_sum(weights, -1)\n",
    "\n",
    "    return rgb_map, depth_map, acc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb643b8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = init_model(D=8, W=256)\n",
    "# model = init_model(D=8, W=512) #debug\n",
    "optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "# optimizer = tf.keras.optimizers.Adam() \n",
    "\n",
    "N_samples = 32 #64 #decrease as needed for VRAM(?)\n",
    "N_iters = 10_000 #10_000\n",
    "psnrs = []\n",
    "iternums = []\n",
    "i_plot = 100 #25\n",
    "\n",
    "import time\n",
    "t = time.time()\n",
    "for i in range(N_iters+1):\n",
    "#     print(i, \"-------------------------------\")\n",
    "    \n",
    "    img_i = np.random.randint(images.shape[0])\n",
    "    target = images[img_i]\n",
    "    pose = poses[img_i]\n",
    "    \n",
    "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "    rays_o = tf.cast(rays_o, tf.float32)\n",
    "    rays_d = tf.cast(rays_d, tf.float32)\n",
    "    \n",
    "#     print(\"\\n rays_o, rays_d \\n\", tf.shape(rays_o), tf.shape(rays_d))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=0., far=128., N_samples=N_samples, rand=True)\n",
    "        loss = tf.reduce_mean(tf.square(rgb - target))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    if i%i_plot==0:\n",
    "        print(i, (time.time() - t) / i_plot, 'secs per iter')\n",
    "        t = time.time()\n",
    "        \n",
    "        # Render the holdout view for logging\n",
    "        rays_o, rays_d = get_rays(H, W, focal, testpose)\n",
    "        rays_o = tf.cast(rays_o, tf.float32)\n",
    "        rays_d = tf.cast(rays_d, tf.float32)\n",
    "        \n",
    "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=0., far=128., N_samples=N_samples)\n",
    "        loss = tf.reduce_mean(tf.square(rgb - testimg))\n",
    "        psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n",
    "\n",
    "        psnrs.append(psnr.numpy())\n",
    "        iternums.append(i)\n",
    "        \n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(rgb)\n",
    "        plt.title(f'Iteration: {i}')\n",
    "        plt.subplot(122)\n",
    "        plt.plot(iternums, psnrs)\n",
    "        plt.title('PSNR')\n",
    "        plt.show()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb26154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = init_model(D=8, W=256)\n",
    "# print(model.summary())\n",
    "\n",
    "print(tf.shape(rays_o))\n",
    "print(tf.shape(rays_o[...,None,:]))\n",
    "\n",
    "# test = np.ones([1000,1000,3])\n",
    "# print(np.shape(test))\n",
    "# test = test[...,None,:]\n",
    "# print(np.shape(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8304545",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1,2,3])\n",
    "b = tf.constant([4,5,6])\n",
    "x, y = tf.meshgrid(a,b)\n",
    "print(x, \"\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d2f0d",
   "metadata": {},
   "source": [
    "# Alternate keras NeRF implementation \n",
    "https://keras.io/examples/vision/nerf/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29610e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Initialize global variables.\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 1 #5\n",
    "NUM_SAMPLES = 32 #16 #number of sample points along a ray\n",
    "POS_ENCODE_DIMS = 16 #16\n",
    "EPOCHS = 50_000\n",
    "num_images = len(images)\n",
    "plt.imshow(images[np.random.randint(low=0, high=num_images)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_position(x):\n",
    "    \"\"\"Encodes the position into its corresponding Fourier feature.\n",
    "\n",
    "    Args:\n",
    "        x: The input coordinate.\n",
    "\n",
    "    Returns:\n",
    "        Fourier features tensors of the position.\n",
    "    \"\"\"\n",
    "    positions = [x]\n",
    "    for i in range(POS_ENCODE_DIMS):\n",
    "        for fn in [tf.sin, tf.cos]:\n",
    "            positions.append(fn(2.0 ** i * x))\n",
    "    return tf.concat(positions, axis=-1)\n",
    "\n",
    "\n",
    "def get_rays(height, width, focal, pose):\n",
    "    \"\"\"Computes origin point and direction vector of rays.\n",
    "\n",
    "    Args:\n",
    "        height: Height of the image.\n",
    "        width: Width of the image.\n",
    "        focal: The focal length between the images and the camera.\n",
    "        pose: The pose matrix of the camera.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of origin point and direction vector for rays.\n",
    "    \"\"\"\n",
    "    # Build a meshgrid for the rays.\n",
    "    i, j = tf.meshgrid(\n",
    "        tf.range(width, dtype=tf.float32),\n",
    "        tf.range(height, dtype=tf.float32),\n",
    "        indexing=\"xy\",\n",
    "    )\n",
    "\n",
    "    # Normalize the x axis coordinates.\n",
    "    transformed_i = (i - width * 0.5) / focal\n",
    "\n",
    "    # Normalize the y axis coordinates.\n",
    "    transformed_j = (j - height * 0.5) / focal\n",
    "\n",
    "    # Create the direction unit vectors.\n",
    "    directions = tf.stack([transformed_i, -transformed_j, -tf.ones_like(i)], axis=-1)\n",
    "\n",
    "    # Get the camera matrix.\n",
    "    camera_matrix = pose[:3, :3]\n",
    "    height_width_focal = pose[:3, -1]\n",
    "\n",
    "    # Get origins and directions for the rays.\n",
    "    transformed_dirs = directions[..., None, :]\n",
    "    camera_dirs = transformed_dirs * camera_matrix\n",
    "    ray_directions = tf.reduce_sum(camera_dirs, axis=-1)\n",
    "    ray_origins = tf.broadcast_to(height_width_focal, tf.shape(ray_directions))\n",
    "\n",
    "    # Return the origins and directions.\n",
    "    return (ray_origins, ray_directions)\n",
    "\n",
    "\n",
    "def render_flat_rays(ray_origins, ray_directions, near, far, num_samples, rand=False):\n",
    "    \"\"\"Renders the rays and flattens it.\n",
    "\n",
    "    Args:\n",
    "        ray_origins: The origin points for rays.\n",
    "        ray_directions: The direction unit vectors for the rays.\n",
    "        near: The near bound of the volumetric scene.\n",
    "        far: The far bound of the volumetric scene.\n",
    "        num_samples: Number of sample points in a ray.\n",
    "        rand: Choice for randomising the sampling strategy.\n",
    "\n",
    "    Returns:\n",
    "       Tuple of flattened rays and sample points on each rays.\n",
    "    \"\"\"\n",
    "    # Compute 3D query points.\n",
    "    # Equation: r(t) = o+td -> Building the \"t\" here.\n",
    "    t_vals = tf.linspace(near, far, num_samples)\n",
    "    if rand:\n",
    "        # Inject uniform noise into sample space to make the sampling\n",
    "        # continuous.\n",
    "        shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
    "        noise = tf.random.uniform(shape=shape) * (far - near) / num_samples\n",
    "        t_vals = t_vals + noise\n",
    "\n",
    "    # Equation: r(t) = o + td -> Building the \"r\" here.\n",
    "    rays = ray_origins[..., None, :] + (\n",
    "        ray_directions[..., None, :] * t_vals[..., None]\n",
    "    )\n",
    "    rays_flat = tf.reshape(rays, [-1, 3])\n",
    "    rays_flat = encode_position(rays_flat)\n",
    "    return (rays_flat, t_vals)\n",
    "\n",
    "\n",
    "def map_fn(pose):\n",
    "    \"\"\"Maps individual pose to flattened rays and sample points.\n",
    "\n",
    "    Args:\n",
    "        pose: The pose matrix of the camera.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of flattened rays and sample points corresponding to the\n",
    "        camera pose.\n",
    "    \"\"\"\n",
    "    (ray_origins, ray_directions) = get_rays(height=H, width=W, focal=focal, pose=pose)\n",
    "    (rays_flat, t_vals) = render_flat_rays(\n",
    "        ray_origins=ray_origins,\n",
    "        ray_directions=ray_directions,\n",
    "        near= 0., #was 2.\n",
    "        far= 128.,  #was 6. for Lego, tried 32 for bike, 8 for desk\n",
    "        num_samples=NUM_SAMPLES,\n",
    "        rand=True,\n",
    "    )\n",
    "    return (rays_flat, t_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f256145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training split.\n",
    "split_index = int(num_images * 0.8) #was this\n",
    "# split_index = int(num_images * 0.9) #really want to max out limited training data\n",
    "\n",
    "# Split the images into training and validation.\n",
    "train_images = images[:split_index]\n",
    "val_images = images[split_index:]\n",
    "\n",
    "# Split the poses into training and validation.\n",
    "train_poses = poses[:split_index]\n",
    "val_poses = poses[split_index:]\n",
    "\n",
    "# Make the training pipeline.\n",
    "train_img_ds = tf.data.Dataset.from_tensor_slices(tf.cast(train_images, tf.float32)) #need to convert train data\n",
    "train_pose_ds = tf.data.Dataset.from_tensor_slices(tf.cast(train_poses, tf.float32))\n",
    "\n",
    "train_ray_ds = train_pose_ds.map(map_fn, num_parallel_calls=AUTO)\n",
    "training_ds = tf.data.Dataset.zip((train_img_ds, train_ray_ds))\n",
    "train_ds = (\n",
    "    training_ds.shuffle(BATCH_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# Make the validation pipeline.\n",
    "val_img_ds = tf.data.Dataset.from_tensor_slices(tf.cast(val_images, tf.float32))\n",
    "val_pose_ds = tf.data.Dataset.from_tensor_slices(tf.cast(val_poses, tf.float32))\n",
    "val_ray_ds = val_pose_ds.map(map_fn, num_parallel_calls=AUTO)\n",
    "validation_ds = tf.data.Dataset.zip((val_img_ds, val_ray_ds))\n",
    "val_ds = (\n",
    "    validation_ds.shuffle(BATCH_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b978c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nerf_model(num_layers, num_pos):\n",
    "    \"\"\"Generates the NeRF neural network.\n",
    "\n",
    "    Args:\n",
    "        num_layers: The number of MLP layers.\n",
    "        num_pos: The number of dimensions of positional encoding.\n",
    "\n",
    "    Returns:\n",
    "        The [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) model.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(num_pos, 2 * 3 * POS_ENCODE_DIMS + 3))\n",
    "    x = inputs\n",
    "    for i in range(num_layers):\n",
    "        x = layers.Dense(units=64, activation=\"relu\")(x)\n",
    "        if i % 4 == 0 and i > 0:\n",
    "            # Inject residual connection.\n",
    "            x = layers.concatenate([x, inputs], axis=-1)\n",
    "    outputs = layers.Dense(units=4)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def render_rgb_depth(model, rays_flat, t_vals, rand=True, train=True):\n",
    "    \"\"\"Generates the RGB image and depth map from model prediction.\n",
    "\n",
    "    Args:\n",
    "        model: The MLP model that is trained to predict the rgb and\n",
    "            volume density of the volumetric scene.\n",
    "        rays_flat: The flattened rays that serve as the input to\n",
    "            the NeRF model.\n",
    "        t_vals: The sample points for the rays.\n",
    "        rand: Choice to randomise the sampling strategy.\n",
    "        train: Whether the model is in the training or testing phase.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of rgb image and depth map.\n",
    "    \"\"\"\n",
    "    # Get the predictions from the nerf model and reshape it.\n",
    "    if train:\n",
    "        predictions = model(rays_flat)\n",
    "    else:\n",
    "        predictions = model.predict(rays_flat)\n",
    "    predictions = tf.reshape(predictions, shape=(BATCH_SIZE, H, W, NUM_SAMPLES, 4))\n",
    "\n",
    "    # Slice the predictions into rgb and sigma.\n",
    "    rgb = tf.sigmoid(predictions[..., :-1])\n",
    "    sigma_a = tf.nn.relu(predictions[..., -1])\n",
    "\n",
    "    # Get the distance of adjacent intervals.\n",
    "    delta = t_vals[..., 1:] - t_vals[..., :-1]\n",
    "    # delta shape = (num_samples)\n",
    "    if rand:\n",
    "        delta = tf.concat(\n",
    "            [delta, tf.broadcast_to([1e10], shape=(BATCH_SIZE, H, W, 1))], axis=-1\n",
    "        )\n",
    "        alpha = 1.0 - tf.exp(-sigma_a * delta)\n",
    "    else:\n",
    "        delta = tf.concat(\n",
    "            [delta, tf.broadcast_to([1e10], shape=(BATCH_SIZE, 1))], axis=-1\n",
    "        )\n",
    "        alpha = 1.0 - tf.exp(-sigma_a * delta[:, None, None, :])\n",
    "\n",
    "    # Get transmittance.\n",
    "    exp_term = 1.0 - alpha\n",
    "    epsilon = 1e-10\n",
    "    transmittance = tf.math.cumprod(exp_term + epsilon, axis=-1, exclusive=True)\n",
    "    weights = alpha * transmittance\n",
    "    rgb = tf.reduce_sum(weights[..., None] * rgb, axis=-2)\n",
    "\n",
    "    if rand:\n",
    "        depth_map = tf.reduce_sum(weights * t_vals, axis=-1)\n",
    "    else:\n",
    "        depth_map = tf.reduce_sum(weights * t_vals[:, None, None], axis=-1)\n",
    "    return (rgb, depth_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2b373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NeRF(keras.Model):\n",
    "    def __init__(self, nerf_model):\n",
    "        super().__init__()\n",
    "        self.nerf_model = nerf_model\n",
    "\n",
    "    def compile(self, optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.psnr_metric = keras.metrics.Mean(name=\"psnr\")\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        # Get the images and the rays.\n",
    "        (images, rays) = inputs\n",
    "        (rays_flat, t_vals) = rays\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get the predictions from the model.\n",
    "            rgb, _ = render_rgb_depth(\n",
    "                model=self.nerf_model, rays_flat=rays_flat, t_vals=t_vals, rand=True\n",
    "            )\n",
    "            loss = self.loss_fn(images, rgb)\n",
    "\n",
    "        # Get the trainable variables.\n",
    "        trainable_variables = self.nerf_model.trainable_variables\n",
    "\n",
    "        # Get the gradeints of the trainiable variables with respect to the loss.\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "        # Apply the grads and optimize the model.\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        # Get the PSNR of the reconstructed images and the source images.\n",
    "        psnr = tf.image.psnr(images, rgb, max_val=1.0)\n",
    "\n",
    "        # Compute our own metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.psnr_metric.update_state(psnr)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_metric.result()}\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        # Get the images and the rays.\n",
    "        (images, rays) = inputs\n",
    "        (rays_flat, t_vals) = rays\n",
    "\n",
    "        # Get the predictions from the model.\n",
    "        rgb, _ = render_rgb_depth(\n",
    "            model=self.nerf_model, rays_flat=rays_flat, t_vals=t_vals, rand=True\n",
    "        )\n",
    "        loss = self.loss_fn(images, rgb)\n",
    "\n",
    "        # Get the PSNR of the reconstructed images and the source images.\n",
    "        psnr = tf.image.psnr(images, rgb, max_val=1.0)\n",
    "\n",
    "        # Compute our own metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.psnr_metric.update_state(psnr)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_metric.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.psnr_metric]\n",
    "\n",
    "\n",
    "test_imgs, test_rays = next(iter(train_ds))\n",
    "test_rays_flat, test_t_vals = test_rays\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "class TrainMonitor(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = logs[\"loss\"]\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "            test_recons_images, depth_maps = render_rgb_depth(\n",
    "                model=self.model.nerf_model,\n",
    "                rays_flat=test_rays_flat,\n",
    "                t_vals=test_t_vals,\n",
    "                rand=True,\n",
    "                train=False,\n",
    "            )\n",
    "            # Plot the rgb, depth and the loss plot.\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "            ax[0].imshow(keras.utils.array_to_img(test_recons_images[0]))\n",
    "            ax[0].set_title(f\"Predicted Image: {epoch:03d}\")\n",
    "\n",
    "            ax[1].imshow(keras.utils.array_to_img(depth_maps[0, ..., None]))\n",
    "            ax[1].set_title(f\"Depth Map: {epoch:03d}\")\n",
    "\n",
    "            ax[2].plot(loss_list)\n",
    "#             ax[2].set_xticks(np.arange(0, EPOCHS + 1, 5.0))\n",
    "            ax[2].set_title(f\"Loss Plot: {epoch:03d}\")\n",
    "\n",
    "            fig.savefig(f\"images/{epoch:03d}.png\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            model.save_weights(\"models/bike.ckpt\") #save checkpoint\n",
    "#             model.save_weights(\"models/desk.ckpt\") #save checkpoint\n",
    "\n",
    "\n",
    "num_pos = H * W * NUM_SAMPLES\n",
    "nerf_model = get_nerf_model(num_layers=8, num_pos=num_pos)\n",
    "\n",
    "model = NeRF(nerf_model)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(5e-3), loss_fn=keras.losses.MeanSquaredError()\n",
    ")\n",
    "#1e-5 too slow for checkpoint\n",
    "\n",
    "# model.load_weights(\"models/bike.ckpt\")\n",
    "\n",
    "# Create a directory to save the images during training.\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.makedirs(\"images\")\n",
    "    \n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[TrainMonitor()],\n",
    "    steps_per_epoch=split_index // BATCH_SIZE,\n",
    ")\n",
    "\n",
    "\n",
    "def create_gif(path_to_images, name_gif):\n",
    "    filenames = glob.glob(path_to_images)\n",
    "    filenames = sorted(filenames)\n",
    "    images = []\n",
    "    for filename in tqdm(filenames):\n",
    "        images.append(imageio.imread(filename))\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(name_gif, images, \"GIF\", **kargs)\n",
    "\n",
    "\n",
    "create_gif(\"images/*.png\", \"training.gif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e24ab",
   "metadata": {},
   "source": [
    "# render animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aee90b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_translation_t(t):\n",
    "    \"\"\"Get the translation matrix for movement in t.\"\"\"\n",
    "    matrix = [\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0.0],\n",
    "        [0, 0, 1, t],\n",
    "        [0, 0, 0, 1],\n",
    "    ]\n",
    "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def get_rotation_phi(phi):\n",
    "    \"\"\"Get the rotation matrix for movement in phi.\"\"\"\n",
    "    matrix = [\n",
    "        [1, 0, 0, 0],\n",
    "        [0, tf.cos(phi), -tf.sin(phi), 0],\n",
    "        [0, tf.sin(phi), tf.cos(phi), 0],\n",
    "        [0, 0, 0, 1],\n",
    "    ]\n",
    "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def get_rotation_theta(theta):\n",
    "    \"\"\"Get the rotation matrix for movement in theta.\"\"\"\n",
    "    matrix = [\n",
    "        [tf.cos(theta), 0, -tf.sin(theta), 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [tf.sin(theta), 0, tf.cos(theta), 0],\n",
    "        [0, 0, 0, 1],\n",
    "    ]\n",
    "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, t):\n",
    "    \"\"\"\n",
    "    Get the camera to world matrix for the corresponding theta, phi\n",
    "    and t.\n",
    "    \"\"\"\n",
    "    c2w = get_translation_t(t)\n",
    "    c2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w\n",
    "    c2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w\n",
    "    c2w = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]) @ c2w\n",
    "    return c2w\n",
    "\n",
    "\n",
    "rgb_frames = []\n",
    "batch_flat = []\n",
    "batch_t = []\n",
    "\n",
    "# Iterate over different theta value and generate scenes.\n",
    "for index, theta in tqdm(enumerate(np.linspace(0.0, 360.0, 60, endpoint=False))):\n",
    "# for index, fwd_trans in tqdm(enumerate(np.linspace(-8, -5, 60, endpoint=False))):\n",
    "    # Get the camera to world matrix.\n",
    "    c2w = pose_spherical(0,theta, -8)\n",
    "#     c2w = pose_spherical(0, 180, fwd_trans)\n",
    "\n",
    "    #\n",
    "    ray_oris, ray_dirs = get_rays(H, W, focal, c2w)\n",
    "    rays_flat, t_vals = render_flat_rays(\n",
    "        ray_oris, ray_dirs, near=2., far=32., num_samples=NUM_SAMPLES, rand=False\n",
    "    )\n",
    "\n",
    "    if index % BATCH_SIZE == 0 and index > 0:\n",
    "        batched_flat = tf.stack(batch_flat, axis=0)\n",
    "        batch_flat = [rays_flat]\n",
    "\n",
    "        batched_t = tf.stack(batch_t, axis=0)\n",
    "        batch_t = [t_vals]\n",
    "\n",
    "        rgb, _ = render_rgb_depth(\n",
    "            nerf_model, batched_flat, batched_t, rand=False, train=False\n",
    "        )\n",
    "\n",
    "        temp_rgb = [np.clip(255 * img, 0.0, 255.0).astype(np.uint8) for img in rgb]\n",
    "\n",
    "        rgb_frames = rgb_frames + temp_rgb\n",
    "    else:\n",
    "        batch_flat.append(rays_flat)\n",
    "        batch_t.append(t_vals)\n",
    "\n",
    "rgb_video = \"rgb_video.mp4\"\n",
    "imageio.mimwrite(rgb_video, rgb_frames, fps=30, quality=7, macro_block_size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01171675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"models/bike.keras\") #doesn't work with subclassed model\n",
    "# tf.keras.saving.save_model(model, \"models/bike.keras\") #not in tf2.X\n",
    "\n",
    "# model.save_weights(\"models/bike.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e761d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.summary())\n",
    "# model.load_weights(\"models/bike.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e51973",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91ef90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the trained NeRF model and infer.\n",
    "nerf_model = model.nerf_model\n",
    "test_recons_images, depth_maps = render_rgb_depth(\n",
    "    model=nerf_model,\n",
    "    rays_flat=test_rays_flat,\n",
    "    t_vals=test_t_vals,\n",
    "    rand=True,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "# print(tf.shape(test_t_vals))\n",
    "print(tf.shape(test_imgs))\n",
    "# print(len(ori_img))\n",
    "\n",
    "# Create subplots.\n",
    "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(10, 20))\n",
    "\n",
    "for ax, ori_img, recons_img, depth_map in zip(\n",
    "    axes, test_imgs, test_recons_images, depth_maps\n",
    "):\n",
    "    ax[0].imshow(keras.utils.array_to_img(ori_img))\n",
    "    ax[0].set_title(\"Original\")\n",
    "\n",
    "    ax[1].imshow(keras.utils.array_to_img(recons_img))\n",
    "    ax[1].set_title(\"Reconstructed\")\n",
    "\n",
    "    ax[2].imshow(\n",
    "        keras.utils.array_to_img(depth_map[..., None]), cmap=\"inferno\"\n",
    "    )\n",
    "    ax[2].set_title(\"Depth Map\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d3d6a",
   "metadata": {},
   "source": [
    "# Interactive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e73848",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interactive, widgets\n",
    "\n",
    "\n",
    "trans_t = lambda t : tf.convert_to_tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,t],\n",
    "    [0,0,0,1],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "rot_phi = lambda phi : tf.convert_to_tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,tf.cos(phi),-tf.sin(phi),0],\n",
    "    [0,tf.sin(phi), tf.cos(phi),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "rot_theta = lambda th : tf.convert_to_tensor([\n",
    "    [tf.cos(th),0,-tf.sin(th),0],\n",
    "    [0,1,0,0],\n",
    "    [tf.sin(th),0, tf.cos(th),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
    "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
    "    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w\n",
    "    return c2w\n",
    "\n",
    "\n",
    "def f(**kwargs):\n",
    "    c2w = pose_spherical(**kwargs)\n",
    "    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n",
    "#     rays_o, rays_d = get_rays(500, 500, focal, c2w[:3,:4]) #test upscaling\n",
    "    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n",
    "    img = np.clip(rgb,0,1)\n",
    "    \n",
    "#     plt.figure(2, figsize=(20,6))\n",
    "    plt.figure(2)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "sldr = lambda v, mi, ma: widgets.FloatSlider(\n",
    "    value=v,\n",
    "    min=mi,\n",
    "    max=ma,\n",
    "    step=.01,\n",
    ")\n",
    "\n",
    "names = [\n",
    "    ['theta', [0, -180., 180]],\n",
    "    ['phi', [0, -180., 180]],\n",
    "    ['radius', [4., 1., 10.]],\n",
    "]\n",
    "\n",
    "interactive_plot = interactive(f, **{s[0] : sldr(*s[1]) for s in names})\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1868a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
