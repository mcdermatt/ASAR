{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load requirements for working with PCs\n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 20*1024\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "import sys\n",
    "import os\n",
    "current = os.getcwd()\n",
    "parent_directory = os.path.dirname(current)\n",
    "sys.path.append(parent_directory)\n",
    "sys.path.append(parent_directory+\"/point_cloud_rectification\")\n",
    "from ICET_spherical import ICET\n",
    "from linear_corrector import LC\n",
    "\n",
    "from utils import R_tf\n",
    "from metpy.calc import lat_lon_grid_deltas\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "import trimesh\n",
    "\n",
    "\n",
    "from pillow_heif import register_heif_opener\n",
    "from matplotlib import pyplot as p\n",
    "from colmapParsingUtils import *\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af594157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load raw point cloud data from Newer College Dataset\n",
    "\n",
    "#NEWER COLLEGE\n",
    "idx = 900 #1500 \n",
    "fn1 = \"/media/derm/06EF-127D4/Newer College Dataset/01_Short_Experiment/point_clouds/frame_\" + str(idx) + \".npy\"\n",
    "pc1 = np.load(fn1)\n",
    "pc1 = np.flip(pc1, axis = 0)#flip to maintain CCW convention used in VICET\n",
    "\n",
    "plt = Plotter(N = 1, axes = 1, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "## purple -> green\n",
    "color = 255*np.linspace(0,1,len(pc1)) \n",
    "cname = np.array([255-color, color, 255-color]).T.tolist()\n",
    "disp.append(Points(pc1, c=cname,  r = 3.5, alpha =0.5))\n",
    "print(len(pc1) / 128)\n",
    "\n",
    "#remove NaNs\n",
    "# pc1[pc1[:,0]>64] = 100\n",
    "# pc1[pc1[:,0]<-64] = 100\n",
    "# pc1 = np.nan_to_num(pc1, nan=0.0)\n",
    "\n",
    "plt.show(disp, \"Raw Point Cloud\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf16c2",
   "metadata": {},
   "source": [
    "# Convert Point Cloud to Depth Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde1ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "pc1_spherical = LC.c2s(LC,pc1).numpy() #[r, theta, phi]\n",
    "pcs = np.reshape(pc1_spherical, [-1,64,3])\n",
    "pcs = np.flip(pcs, axis = 1)\n",
    "raw_data = pcs[:,:,0].T\n",
    "\n",
    "# print(np.shape(data))\n",
    "\n",
    "data = np.zeros([64, 1024])\n",
    "for i in range(np.shape(data)[0]//4):\n",
    "    #shift left\n",
    "#     data[4*i,:-18] = raw_data[4*i,18:]\n",
    "#     data[4*i+1,:-12] = raw_data[4*i+1,12:]\n",
    "#     data[4*i+2,:-6] = raw_data[4*i+2,6:]\n",
    "#     data[4*i+3,:] = raw_data[4*i+3,:]\n",
    "#     #shift right\n",
    "#     data[4*i,:] = raw_data[4*i,:]\n",
    "#     data[4*i+1,6:] = raw_data[4*i+1,:-6]\n",
    "#     data[4*i+2,12:] = raw_data[4*i+2,:-12]\n",
    "#     data[4*i+3,18:] = raw_data[4*i+3,:-18]\n",
    "    #keep centered-- avoids needing to fill in gaps\n",
    "#     data[4*i,:-9] = raw_data[4*i,9:]\n",
    "#     data[4*i+1,:-3] = raw_data[4*i+1,3:]\n",
    "#     data[4*i+2,3:] = raw_data[4*i+2,:-3]\n",
    "#     data[4*i+3,9:] = raw_data[4*i+3,:-9]\n",
    "    data[4*i,1:-8] = raw_data[4*i,9:]\n",
    "    data[4*i+1,1:-2] = raw_data[4*i+1,3:]\n",
    "    data[4*i+2,4:] = raw_data[4*i+2,:-4]\n",
    "    data[4*i+3,10:] = raw_data[4*i+3,:-10]\n",
    "    \n",
    "# data = np.flip(data, axis =1)\n",
    "\n",
    "fig, ax = p.subplots(2,1)\n",
    "ax[0].set_title(\"Raw Depth Image\")\n",
    "ax[0].imshow(raw_data, cmap = \"YlGnBu\", norm='log')\n",
    "ax[0].set_aspect(5)\n",
    "ax[1].set_title(\"Destaggered Depth Image\")\n",
    "ax[1].imshow(data, cmap = \"YlGnBu\", norm='log')\n",
    "ax[1].set_aspect(5)\n",
    "print(np.shape(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d0fec",
   "metadata": {},
   "source": [
    "# Reproject 2D Depth image back to 3D point cloud\n",
    "\n",
    "#### IMPORTANT: look into pixel_shift_by_row parameter from OUSTER\n",
    "https://ouster.com/insights/blog/firmware-2-4-industry-standard-protocols-and-improved-alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OS1 LiDAR Intrinsics\n",
    "#as specified in datasheet\n",
    "# phimin = np.deg2rad(-16.6) #supposed to be this\n",
    "# phimax = np.deg2rad(16.6)\n",
    "#angles required to reproduce observed point clouds\n",
    "phimin = np.deg2rad(-16) \n",
    "phimax = np.deg2rad(17.75)\n",
    "depth_img  = data.T\n",
    "\n",
    "new_point_cloud_spherical = np.zeros([np.shape(pcs)[0]*np.shape(pcs)[1],3])\n",
    "pc1_spherical = new_point_cloud_spherical\n",
    "\n",
    "count = 0\n",
    "for w in range(np.shape(depth_img)[0]):\n",
    "    for h in range(np.shape(depth_img)[1]):\n",
    "        new_point_cloud_spherical[count,0] = depth_img[w,h] #radius\n",
    "        new_point_cloud_spherical[count,1] = 2*np.pi*(w/np.shape(depth_img)[0]) #theta\n",
    "        new_point_cloud_spherical[count,2] = np.pi/2 + phimax - (phimax-phimin)*(h/np.shape(depth_img)[1]) #phi\n",
    "        count+= 1\n",
    "\n",
    "new_point_cloud_cart = LC.s2c(LC,new_point_cloud_spherical).numpy()\n",
    "new_point_cloud_cart[:,2] = -new_point_cloud_cart[:,2] #need to flip z \n",
    "\n",
    "#shape image to have same angular field of view in width and height\n",
    "vert_fov = np.rad2deg(phimax-phimin)\n",
    "# print(vert_fov)\n",
    "# vert_fov/360\n",
    "# print((vert_fov/360)*np.shape(pcs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = Plotter(N = 1, axes = 1, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "## purple -> green\n",
    "color = 255*np.linspace(0,1,len(new_point_cloud_cart)) \n",
    "cname = np.array([255-color, color, 255-color]).T.tolist()\n",
    "disp.append(Points(new_point_cloud_cart, c=cname,  r = 3.5, alpha =0.5))\n",
    "# disp.append(Points(new_point_cloud_cart[::32], c='red',  r = 3.5, alpha =0.8))\n",
    "# disp.append(Points(new_point_cloud_cart[::4], c='blue',  r = 5., alpha =0.125))\n",
    "\n",
    "disp.append(Points(pc1, c = 'blue', r = 3.5, alpha = 0.5))\n",
    "\n",
    "plt.show(disp, \"Raw Point Cloud\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490510ba",
   "metadata": {},
   "source": [
    "# Load ground truth poses (map frame) and convert to NeRF LH Coordinate System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "dir_name = \"/media/derm/06EF-127D4/Newer College Dataset/\"\n",
    "experiment_name = \"01_short_experiment-20230331T172433Z-009/01_short_experiment/\"\n",
    "fn_gt = dir_name + experiment_name + \"ground_truth/registered_poses.csv\"\n",
    "# print(fn_gt)\n",
    "\n",
    "#sec,nsec,x,y,z,qx,qy,qz,qw\n",
    "gt = np.loadtxt(fn_gt, delimiter=',',skiprows = 1)\n",
    "# print(np.shape(gt))\n",
    "# print(gt[20,:])\n",
    "\n",
    "scan_dir = dir_name + experiment_name + \"raw_format/ouster_zip_files/ouster_scan-007/ouster_scan/\"\n",
    "list_of_all_scans = sorted(listdir(scan_dir))\n",
    "\n",
    "#plot ground truth trajectory\n",
    "fig, ax = p.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel(\"x (m)\")\n",
    "ax.set_ylabel(\"y (m)\")\n",
    "ax.plot(gt[:,2], gt[:,3], label = \"ground truth\")\n",
    "\n",
    "#superimpose trajectory from <short_experiment_01>\n",
    "# first_timestamp = int(list_of_all_scans[0][6:16])\n",
    "# last_timestamp = int(list_of_all_scans[-1][6:16]) #scrape timestamp from name of velodyne .pcl file\n",
    "# first_idx = np.argwhere(gt[:,0] == first_timestamp)[0][0]\n",
    "# last_idx = np.argwhere(gt[:,0] == last_timestamp)[0][0]\n",
    "first_idx = 600 #1450 #700\n",
    "last_idx = 1000 #1950 #1000\n",
    "\n",
    "ax.plot(gt[first_idx:last_idx,2], gt[first_idx:last_idx,3], 'r', lw = 10, alpha = 0.3, label = 'training region')\n",
    "ax.legend(loc = 'best')\n",
    "ax.set_ylim([-150,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca27abd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "#convert gt from xyzquat to homogenous rotation matrix\n",
    "n_images = 50 #20 \n",
    "n_rots = 8 #8 \n",
    "#Ouster OS1-64\n",
    "phimin = np.deg2rad(-16) \n",
    "phimax = np.deg2rad(17.75)\n",
    "#just for debug\n",
    "# phimin = np.deg2rad(-6) \n",
    "# phimax = np.deg2rad(27.75)\n",
    "# phimin = -0.53529248 #rad\n",
    "# phimax = 0.18622663 #rad\n",
    "vert_fov = np.rad2deg(phimax-phimin)\n",
    "\n",
    "poses = np.zeros([n_images*n_rots,4,4])\n",
    "images = np.ones([n_images*n_rots, 64, 64, 2]) #depth image and raydrop\n",
    "\n",
    "#focal length (in pixels) = Image Size / (2 tan(FOV/2)) #needs to be array!\n",
    "focal = np.array(np.shape(images)[1]/(2*np.tan((phimax-phimin)/2))) #default image size\n",
    "H, W = images.shape[1:3]\n",
    "print(focal, H, W)\n",
    "\n",
    "for i in range(n_images):\n",
    "    #load point cloud\n",
    "    idx = i*10 + 600 #1450 #700    \n",
    "    fn1 = \"/media/derm/06EF-127D4/Newer College Dataset/01_Short_Experiment/point_clouds/frame_\" + str(idx) + \".npy\"\n",
    "    pc1 = np.load(fn1)\n",
    "    pc1 = np.flip(pc1, axis = 0)# uncomment to flip and maintain CCW convention used in VICET\n",
    "\n",
    "    #convert to depth image\n",
    "    pc1_spherical = LC.c2s(LC,pc1).numpy() #[r, theta, phi]\n",
    "    pcs = np.reshape(pc1_spherical, [-1,64,3])\n",
    "    pcs = np.flip(pcs, axis = 1)\n",
    "    raw_data = pcs[:,:,:]\n",
    "    raw_data = np.transpose(pcs, [1,0,2])\n",
    "\n",
    "    #destagger depth images (OS1 unit has delay in sensor return bus)\n",
    "    data = np.zeros([64, 1024])\n",
    "    for k in range(np.shape(data)[0]//4):\n",
    "        data[4*k,1:-8] = raw_data[4*k,9:,0]\n",
    "        data[4*k+1,1:-2] = raw_data[4*k+1,3:,0]\n",
    "        data[4*k+2,4:] = raw_data[4*k+2,:-4,0]\n",
    "        data[4*k+3,10:] = raw_data[4*k+3,:-10,0]\n",
    "    data = np.flip(data, axis =1)\n",
    "\n",
    "#     print(np.shape(data))\n",
    "#     print(np.shape(pc1_spherical))\n",
    "\n",
    "    for j in range(n_rots):\n",
    "        \n",
    "        #get cropped depth image ~~~~~~~~~~~~~~~~~~~~\n",
    "#         pcs = np.flip(data, axis = 1) #flip vertical\n",
    "#         pcs = np.flip(pcs, axis = 0) #flip horizontal to look at first clockwise patch of scan sweep \n",
    "#         pcs = pcs.T\n",
    "        pcs = data.T\n",
    "        \n",
    "        #resize image to 64x64\n",
    "        image_width = int((vert_fov/360)*np.shape(pcs)[0])        \n",
    "#         pcs = pcs[j*image_width:(j+1)*image_width,:].T #SQUARE\n",
    "        pcs = pcs[(j+1)*image_width:(j+2)*image_width,:].T #SQUARE-- skip beginning of frame (blocked by human)\n",
    "#         pcs = pcs[(j*image_width//32):((j+1)*image_width//32),:,0].T #32nds\n",
    "        pcs = cv2.resize(pcs, (64, 64), cv2.INTER_NEAREST) #keep square\n",
    "        #TEST\n",
    "#         pcs = np.flip(pcs, axis =1)\n",
    "    \n",
    "        #preserve aspect ratio and focal length but just take middle\n",
    "#         pcs = pcs[:,31:33] #uncomment for 32nds\n",
    "        images[j+(i*n_rots),:,:,0] = pcs #save depth information to first channel\n",
    "        a = np.argwhere(pcs == 0)\n",
    "        #TODO: why do some distant windows register as being close (but not non-returns)??\n",
    "        images[j+(i*n_rots),a[:,0],a[:,1],1] = 0 #save raydrop mask to 2nd channel\n",
    "        \n",
    "        #get transformation matrix ~~~~~~~~~~~~~~~~~~\n",
    "        rotm = np.eye(4)\n",
    "        rotm[1,3] = -gt[idx,2] #x\n",
    "        rotm[2,3] = gt[idx,3] #y\n",
    "        rotm[0,3] = -gt[idx,4] #z\n",
    "        rotm[:3,:3] = R.from_quat(gt[idx,5:]).as_matrix() \n",
    "\n",
    "# #         #orient yellow (-z) pointing forward\n",
    "        fix1 = R.from_euler('xyz', [0,-np.pi/2,0]).as_matrix()\n",
    "        fix2 = R.from_euler('xyz', [np.pi/2,0,0]).as_matrix()\n",
    "        rotm[:3,:3] = rotm[:3,:3] @ fix1 @ fix2\n",
    "# #         #orient yellow (-z) pointing back left (like in dataset)\n",
    "#         fix1 = R.from_euler('xyz', [0,-np.pi/2,0]).as_matrix()\n",
    "#         fix2 = R.from_euler('xyz', [3*np.pi/4,0,0]).as_matrix()\n",
    "#         rotm[:3,:3] = rotm[:3,:3] @ fix1 @ fix2\n",
    "#         fix1 = R.from_euler('xyz', [0,0,-np.pi/2]).as_matrix()\n",
    "#         fix2 = R.from_euler('xyz', [0,0,np.pi/2]).as_matrix()\n",
    "#         rotm[:3,:3] = np.linalg.pinv(rotm[:3,:3]) @ fix1 @ rotm[:3,:3]\n",
    "#         rotm[:3,:3] = rotm[:3,:3] @ fix2  \n",
    "#         fix1 = R.from_euler('xyz', [0,-np.pi/2,0]).as_matrix()\n",
    "#         fix2 = R.from_euler('xyz', [np.pi/2,0,0]).as_matrix()\n",
    "#         rotm[:3,:3] = rotm[:3,:3] @ fix1 @ fix2\n",
    "\n",
    "        #unreliable\n",
    "        temp = R.from_matrix(rotm[:3,:3]).as_euler('zxy')\n",
    "        rotm[:3,:3] = R.from_euler('xyz', [temp[0], temp[1], -temp[2]]).as_matrix()\n",
    "#         #do manually\n",
    "#         fix3 = R.from_euler('xyz', [0,0,3*np.pi/2]).as_matrix()\n",
    "#         fix4 = R.from_euler('xyz', [0,np.pi/2,0]).as_matrix()\n",
    "#         rotm[:3,:3] = np.linalg.pinv(rotm[:3,:3]) @ fix3 @ rotm[:3,:3]\n",
    "#         rotm[:3,:3] = np.linalg.pinv(rotm[:3,:3]) @ fix4 @ rotm[:3,:3]\n",
    "#         fix3 = R.from_euler('xyz', [np.pi,0,0]).as_matrix()\n",
    "#         fix4 = R.from_euler('xyz', [0,0,-np.pi/2]).as_matrix()\n",
    "# #         rotm[:3,:3] = np.linalg.pinv(rotm[:3,:3]) @ fix3 @ fix4 @ rotm[:3,:3]\n",
    "#         rotm[:3,:3] = np.linalg.pinv(rotm[:3,:3]) @ fix3 @ rotm[:3,:3]\n",
    "#         rotm[:3,:3] = rotm[:3,:3] @ fix4 \n",
    "\n",
    "        #account for image crop in rotation\n",
    "        #swapping sign convention from what was used in synthetic data(?) ...+j*(phimax-phimin)\n",
    "#         crop_angle = 0 #DEBUG ONLY\n",
    "#         crop_angle = -(phimax-phimin)/2 + j*(phimax-phimin) #square\n",
    "        crop_angle = -(phimax-phimin)/2 + (j+1)*(phimax-phimin) #square-- but skip beginning of frame\n",
    "        #         crop_angle = -(phimax-phimin)/64 - j*(phimax-phimin)/32 #2-pixels wide\n",
    "#         rotm_crop = R.from_euler('xyz', [0, -crop_angle,0]).as_matrix() #was this\n",
    "        rotm_crop = R.from_euler('xyz', [0,-crop_angle + np.pi/2,0]).as_matrix() #looks better\n",
    "        rotm[:3,:3] = rotm[:3,:3] @ rotm_crop\n",
    "\n",
    "        #also need to account for the fact that the LIDAR beam isn't actually centered at horizon\n",
    "        sensor_elevation_zero_rotm = R.from_euler('xyz', [(phimin+phimax)/2,0,0]).as_matrix() #was this\n",
    "#         sensor_elevation_zero_rotm = R.from_euler('xyz', [0,(phimin+phimax)/2,0]).as_matrix() #TODO: not sure if rot about y or z\n",
    "        rotm[:3,:3] = rotm[:3,:3] @ sensor_elevation_zero_rotm\n",
    "\n",
    "        #flip x and z axis\n",
    "#         rotm[0,-1], rotm[2,-1] = rotm[2,-1], rotm[0,-1] \n",
    "#         rotm[0,-1], rotm[1,-1] = rotm[1,-1], rotm[0,-1]  #test\n",
    "\n",
    "# #         # flip sign of axis\n",
    "#         rotm[0:3,2] *= -1 #was this\n",
    "#         rotm[0:3,1] *= -1 #was this\n",
    "#         rotm[0:3,0] *= -1 #test\n",
    "#         rotm = rotm[[1,0,2,3],:] #was this\n",
    "#         rotm = rotm[[2,0,1,3],:]\n",
    "#         rotm[2,:] *= -1 # flip whole world upside down\n",
    "#         #translate all frames above xy plane\n",
    "        rotm[2,-1] += 45 \n",
    "        \n",
    "        #Linearly scale down translations and ranges ~~~~~~~~~~~~~~~~~~~\n",
    "        rotm[:3,-1] *= 0.05\n",
    "        images[j+(i*n_rots),:,:,0] *= 0.05\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        poses[j+(i*n_rots)] = rotm\n",
    "\n",
    "images = images.astype(np.float32)\n",
    "poses = poses.astype(np.float32)\n",
    "        \n",
    "# test on one only\n",
    "testimg, testpose = images[(n_images*n_rots)-1], poses[(n_images*n_rots)-1]\n",
    "images = images[:((n_images*n_rots)-1),...,:3]\n",
    "poses = poses[:((n_images*n_rots)-1)]\n",
    "\n",
    "# #90/10 split\n",
    "# cutoff = (n_images*9)//10\n",
    "# print(cutoff)\n",
    "# testimg, testpose = images[cutoff:], poses[cutoff:]\n",
    "# images = images[:cutoff,...,:3]\n",
    "# poses = poses[:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846225b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#draw orientation of frames used for each patch in world frame\n",
    "phimin = np.deg2rad(-16) \n",
    "phimax = np.deg2rad(17.75)\n",
    "plt = Plotter(N = 1, axes = 1, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "for i in range(np.shape(images)[0]-1):\n",
    "    #convert depth image back to point cloud\n",
    "    count = 0\n",
    "    for w in range(np.shape(images)[1]):\n",
    "        for h in range(np.shape(images)[2]):\n",
    "            new_point_cloud_spherical[count,0] = images[i,w,h,0] #radius\n",
    "            new_point_cloud_spherical[count,1] = -(phimax-phimin)*(w/np.shape(images)[1]) #theta\n",
    "            new_point_cloud_spherical[count,2] = np.pi/2 + phimax - (phimax-phimin)*(h/np.shape(images)[2]) #phi\n",
    "            count+= 1\n",
    "            \n",
    "    #transform each to base frame using <poses>\n",
    "    new_point_cloud_cart = LC.s2c(LC,new_point_cloud_spherical).numpy()\n",
    "    new_point_cloud_cart[:,2] = -new_point_cloud_cart[:,2] #need to flip z -- does this indicate an error somewhere???\n",
    "\n",
    "    #draw frames for each sub-scan \n",
    "    alph = 1-(i%(n_rots)/n_rots)\n",
    "    #forward view direction (-z in NeRF c2w convention)\n",
    "    headings = poses[i,:3,:3] @ np.array([0,0,-0.03])\n",
    "    disp.append(Arrows(poses[i,:3,-1][None,:], (poses[i,:3,-1] + headings)[None,:], c = \"yellow\", alpha = alph))\n",
    "    # x\n",
    "    headings = poses[i,:3,:3] @ np.array([.03,0,0])\n",
    "    disp.append(Arrows(poses[i,:3,-1][None,:], (poses[i,:3,-1] + headings)[None,:], c = \"red\", alpha = alph))\n",
    "    #y\n",
    "    headings = poses[i,:3,:3] @ np.array([0,.03,0])\n",
    "    disp.append(Arrows(poses[i,:3,-1][None,:], (poses[i,:3,-1] + headings)[None,:], c = \"green\", alpha = alph))\n",
    "    #z\n",
    "    headings = poses[i,:3,:3] @ np.array([0,0,.03])\n",
    "    disp.append(Arrows(poses[i,:3,-1][None,:], (poses[i,:3,-1] + headings)[None,:], c = \"blue\", alpha = alph))\n",
    "disp.append(Points(np.array([[0,0,0]]), r = 10, c = 'black'))\n",
    "plt.show(disp, \"Training Data Sample\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "fig, ax = p.subplots(2,5)\n",
    "ax[0,0].set_title(\"depth image patches for training\")\n",
    "\n",
    "# ax[0,0].imshow(images[0,:,:,0])#, vmin=0, vmax=64)\n",
    "# ax[0,1].imshow(images[1,:,:,0])#, vmin=0, vmax=64)\n",
    "# ax[0,2].imshow(images[2,:,:,0])#, vmin=0, vmax=64)\n",
    "# ax[0,3].imshow(images[3,:,:,0])#, vmin=0, vmax=64)\n",
    "# ax[0,4].imshow(images[4,:,:,0])#, vmin=0, vmax=64)\n",
    "\n",
    "ax[0,0].imshow(images[-4,:,:,0])#, vmin=0, vmax=64)\n",
    "ax[0,1].imshow(images[-3,:,:,0])#, vmin=0, vmax=64)\n",
    "ax[0,2].imshow(images[-2,:,:,0])#, vmin=0, vmax=64)\n",
    "ax[0,3].imshow(images[-1,:,:,0])#, vmin=0, vmax=64)\n",
    "ax[0,4].imshow(testimg[:,:,0])#, vmin=0, vmax=64)\n",
    "\n",
    "ax[1,0].set_title(\"ray drop masks\")\n",
    "ax[1,0].imshow(images[-4,:,:,1],cmap=\"gray\", vmin=0,vmax=1)\n",
    "ax[1,1].imshow(images[-3,:,:,1],cmap=\"gray\", vmin=0,vmax=1)\n",
    "ax[1,2].imshow(images[-2,:,:,1],cmap=\"gray\", vmin=0,vmax=1)\n",
    "ax[1,3].imshow(images[-1,:,:,1],cmap=\"gray\", vmin=0,vmax=1)\n",
    "ax[1,4].imshow(testimg[:,:,1],cmap=\"gray\", vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad196c",
   "metadata": {},
   "source": [
    "# Debug: draw frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08210f64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Raw frames, in Newer College RHCS convention\n",
    "phimin = np.deg2rad(-16) \n",
    "phimax = np.deg2rad(17.75)\n",
    "\n",
    "plt = Plotter(N = 1, axes = 1, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "# new_point_cloud_spherical = np.zeros([np.shape(images)[1]*np.shape(images)[2],3])\n",
    "\n",
    "for i in range(n_images):\n",
    "    idx = i*20 + 600    \n",
    "    \n",
    "    rotm = np.eye(4)\n",
    "    rotm[0,3] = gt[idx,2] #x\n",
    "    rotm[1,3] = gt[idx,3] #y\n",
    "    rotm[2,3] = gt[idx,4] #z\n",
    "    rotm[:3,:3] = R.from_quat(gt[idx,5:]).as_matrix() \n",
    "    \n",
    "    #draw frames for each sub-scan \n",
    "    alph = 1-(i/n_images)\n",
    "    #forward view direction (-z in NeRF c2w convention)\n",
    "    headings = rotm[:3,:3] @ np.array([0,0,-1])\n",
    "    disp.append(Arrows(rotm[:3,-1][None,:], (rotm[:3,-1] + headings)[None,:], c = \"yellow\", alpha = alph))\n",
    "    # x\n",
    "    headings = rotm[:3,:3] @ np.array([1,0,0])\n",
    "    disp.append(Arrows(rotm[:3,-1][None,:], (rotm[:3,-1] + headings)[None,:], c = \"red\", alpha = alph))\n",
    "    #y\n",
    "    headings = rotm[:3,:3] @ np.array([0,1,0])\n",
    "    disp.append(Arrows(rotm[:3,-1][None,:], (rotm[:3,-1] + headings)[None,:], c = \"green\", alpha = alph))\n",
    "    #z\n",
    "    headings = rotm[:3,:3] @ np.array([0,0,1])\n",
    "    disp.append(Arrows(rotm[:3,-1][None,:], (rotm[:3,-1] + headings)[None,:], c = \"blue\", alpha = alph))\n",
    "\n",
    "disp.append(Points(np.array([[0,0,0]]), r = 10, c = 'black'))\n",
    "plt.show(disp, \"Actual Ground Truth Path (Newer College Frame)\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a088a1",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posenc(x):\n",
    "  rets = [x]\n",
    "  for i in range(L_embed):\n",
    "    for fn in [tf.sin, tf.cos]:\n",
    "      rets.append(fn(2.**i * x))\n",
    "  return tf.concat(rets, -1)\n",
    "\n",
    "L_embed = 6 #6\n",
    "embed_fn = posenc\n",
    "# L_embed = 0\n",
    "# embed_fn = tf.identity\n",
    "\n",
    "def init_model(D=8, W=256): #8,256\n",
    "    relu = tf.keras.layers.ReLU()    \n",
    "    dense = lambda W=W, act=relu : tf.keras.layers.Dense(W, activation=act)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(3 + 3*2*L_embed)) \n",
    "    outputs = inputs\n",
    "    for i in range(D):\n",
    "        outputs = dense()(outputs)\n",
    "        if i%4==0 and i>0:\n",
    "            outputs = tf.concat([outputs, inputs], -1)\n",
    "\n",
    "    #extend \"color\" channels to small MLP after output of density channel\n",
    "    sigma_channel = dense(1, act=None)(outputs)\n",
    "    \n",
    "    #start ray drop branch\n",
    "    rd_start = tf.concat([outputs, inputs], -1)\n",
    "    rd_channel = dense(128, act=relu)(outputs)\n",
    "#     rd_channel = dense(128, act=relu)(rd_channel)\n",
    "    rd_channel = dense(128, act=relu)(rd_channel)\n",
    "    rd_channel = dense(1, act=tf.keras.activations.sigmoid)(rd_channel)\n",
    "    out = tf.concat([sigma_channel, rd_channel], -1)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
    "\n",
    "    #spherical projection model (ours)~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    phimin = np.deg2rad(-16) \n",
    "    phimax = np.deg2rad(17.75)\n",
    "\n",
    "    #[r, theta, phi]\n",
    "    dirs_test = tf.stack([-tf.ones_like(i), #r\n",
    "                          #theta\n",
    "                          ((i-31.5)/63)*(phimax-phimin), #was this\n",
    "#                         ((-i+31.5)/63)*(phimax-phimin), #test                          \n",
    "                          #phi\n",
    "#                           (phimax + phimin)/2 + ((j-31.5)/63)*(phimax-phimin) -np.pi/2 #rows in wrong order\n",
    "                          (phimax + phimin)/2 + ((-j+31.5)/63)*(phimax-phimin) -np.pi/2\n",
    "                         ], -1)\n",
    "    dirs_test = tf.reshape(dirs_test,[-1,3])\n",
    "    dirs_test = LC.s2c(LC, dirs_test) \n",
    "    \n",
    "    rotm = R.from_euler('xyz', [0,-np.pi/2 + (phimax + phimin)/2,0]).as_matrix()\n",
    "    dirs_test = dirs_test @ rotm\n",
    "    dirs_test = dirs_test @ tf.transpose(c2w[:3,:3])\n",
    "    dirs = dirs_test @ (c2w[:3,:3] \n",
    "                          @ R.from_euler('xyz', [0,0,np.pi/2]).as_matrix()\n",
    "                          @ np.linalg.pinv(c2w[:3,:3]) )\n",
    "\n",
    "    dirs = tf.reshape(dirs, [64,64,3]) #square\n",
    "    \n",
    "    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * np.eye(3), -1) \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
    "        \n",
    "    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, rand=True):\n",
    "\n",
    "    def batchify(fn, chunk=1024*512): #1024*128 converged for box3 #1024*32 in TinyNeRF\n",
    "        return lambda inputs : tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    \n",
    "    # Compute 3D query points\n",
    "    z_vals = tf.linspace(near, far, N_samples) \n",
    "    if rand:\n",
    "      z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "    #[image_height, image_width, batch_size, 3]\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "    \n",
    "    # Run network\n",
    "    pts_flat = tf.reshape(pts, [-1,3])\n",
    "    pts_flat = embed_fn(pts_flat)\n",
    "    raw = batchify(network_fn)(pts_flat)\n",
    "#     raw = tf.reshape(raw, list(pts.shape[:-1]) + [4]) #OG nerf\n",
    "#     raw = tf.reshape(raw, list(pts.shape[:-1]) + [1])  #[depth]\n",
    "    raw = tf.reshape(raw, list(pts.shape[:-1]) + [2]) # [depth, ray drop] \n",
    "    \n",
    "    # Compute opacities and colors\n",
    "    #OG TinyNeRF\n",
    "#     sigma_a = tf.nn.relu(raw[...,3])\n",
    "#     rgb = tf.math.sigmoid(raw[...,:3]) \n",
    "    #LiDAR-NeRF\n",
    "    sigma_a = tf.nn.relu(raw[...,0])\n",
    "    ray_drop = tf.nn.relu(raw[...,1])\n",
    "#     ray_drop = tf.math.sigmoid(raw[...,1]) #test\n",
    "\n",
    "    # Do volume rendering\n",
    "    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[...,:1].shape)], -1) \n",
    "    alpha = 1.-tf.exp(-sigma_a * dists)  \n",
    "    weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
    "\n",
    "#     print(\"weights[...,None]\",np.shape(weights[..., None]))\n",
    "#     print(\"ray_drop\", np.shape(ray_drop))\n",
    "#     print(np.shape(weights), np.shape(z_vals))\n",
    "    \n",
    "    depth_map = tf.reduce_sum(weights * z_vals, -1)\n",
    "    ray_drop_map = tf.reduce_sum(weights * ray_drop, -1) #axis was -2, changed to -1 \n",
    "    acc_map = tf.reduce_sum(weights, -1)\n",
    "\n",
    "#     return depth_map, acc_map\n",
    "    return depth_map, acc_map, ray_drop_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec43a49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = init_model()\n",
    "# optimizer = tf.keras.optimizers.Adam(5e-4) #default tiny-NeRF\n",
    "optimizer = tf.keras.optimizers.Adam(5e-5)\n",
    "\n",
    "N_samples = 256 #256 #64 #decrease as needed for VRAM\n",
    "N_iters = 50_000\n",
    "psnrs = []\n",
    "iternums = []\n",
    "i_plot = 50\n",
    "\n",
    "import time\n",
    "t = time.time()\n",
    "for i in range(N_iters+1):\n",
    "    img_i = np.random.randint(images.shape[0])\n",
    "    target = images[img_i,:,:,:1]\n",
    "    target_drop_mask = images[img_i,:,:,1:]\n",
    "    pose = poses[img_i]\n",
    "    #get ray origins and ray directions\n",
    "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        #just calculate loss via comparing depth output vs target (should also be depth!)\n",
    "        depth, acc, ray_drop = render_rays(model, rays_o, rays_d, near=0., far=4., N_samples=N_samples, rand=True)\n",
    "        depth = depth[:,:,None]\n",
    "        ray_drop = ray_drop[:,:,None]\n",
    "        \n",
    "#         print(np.shape(depth), np.shape(target))\n",
    "#         print(np.shape(ray_drop), np.shape(target_drop_mask))\n",
    "        \n",
    "        #default loss from TinyNERF\n",
    "#         loss = tf.reduce_mean(tf.square(depth - target)) \n",
    "        #Distance Loss only\n",
    "#         loss = tf.reduce_mean(tf.abs(depth - target)) # <-- works way better than dist^2\n",
    "    \n",
    "        #LiDAR-NeRF LoSS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        ## L_total = L_dist + lam1*L_intensity + lam2*L_raydrop + lam3*L_reg\n",
    "        ##          lam1=1, lam2=1, lam3=100\n",
    "        ## L_reg --> structural regularization: consider gradient loss only on high-texture areas\n",
    "        \n",
    "        #Gradient Loss (structural regularization for smooth surfaces)\n",
    "        thresh = 0.025 #was 0.1 in LiDAR-NeRF\n",
    "        mask = np.zeros(np.shape(target[:,:,0]))\n",
    "        vertical_grad_target = np.gradient(target[:,:,0])[0] \n",
    "        vertical_past_thresh = np.argwhere(tf.abs(vertical_grad_target) > thresh)\n",
    "        mask[vertical_past_thresh[:,0], vertical_past_thresh[:,1]] = 1\n",
    "        horizontal_grad_target = np.gradient(target[:,:,0])[1]\n",
    "        horizontal_past_thresh = np.argwhere(tf.abs(horizontal_grad_target) > thresh)\n",
    "        mask[horizontal_past_thresh[:,0], horizontal_past_thresh[:,1]] = 1\n",
    "        vertical_grad_inference = np.gradient(depth[:,:,0])[0]\n",
    "        horizontal_grad_inference = np.gradient(depth[:,:,0])[1]\n",
    "        mag_difference = tf.math.sqrt((vertical_grad_target-vertical_grad_inference)**2 + (horizontal_grad_target-horizontal_grad_inference)**2)\n",
    "#         L_reg = tf.reduce_mean(np.multiply(mag_difference, mask))\n",
    "        #suppress ray drop areas\n",
    "        L_reg = np.multiply(mag_difference, mask)\n",
    "        L_reg = tf.reduce_mean(tf.multiply(L_reg, target_drop_mask))\n",
    "        L_reg = tf.cast(L_reg, tf.float32)            \n",
    "                \n",
    "        #ray drop loss\n",
    "        L_raydrop = tf.keras.losses.binary_crossentropy(target_drop_mask, ray_drop)\n",
    "        L_raydrop = tf.math.reduce_mean(tf.abs(L_raydrop))\n",
    "#         print(\"Ray Drop Loss:\", L_raydrop)\n",
    "    \n",
    "        #distance loss\n",
    "#         L_dist = tf.reduce_mean(tf.abs(depth - target)) #simple loss\n",
    "        #suppressing ray drop areas\n",
    "        depth_nondrop = tf.math.multiply(depth, target_drop_mask)\n",
    "        target_nondrop = tf.math.multiply(target, target_drop_mask)\n",
    "        L_dist = tf.reduce_mean(tf.abs(depth_nondrop - target_nondrop))\n",
    "    \n",
    "        lam1 = 100\n",
    "        lam2 = 1 #1/(64**2)\n",
    "        loss = L_dist + lam1*L_reg + lam2*L_raydrop       \n",
    "\n",
    "#         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    if i%i_plot==0:\n",
    "        t = time.time()\n",
    "        # Render the holdout view for logging\n",
    "        rays_o, rays_d = get_rays(H, W, focal, testpose) #constant validation image\n",
    "#         sampl = int(np.random.uniform(low=0, high=len(testpose)-1))        \n",
    "#         rays_o, rays_d = get_rays(H, W, focal, testpose[sampl]) #90/10 split\n",
    "#         depth, acc = render_rays(model, rays_o, rays_d, near=1., far=64., N_samples=N_samples)\n",
    "        depth, acc, ray_drop = render_rays(model, rays_o, rays_d, near=0., far=4., N_samples=N_samples)\n",
    "        depth = depth[:,:,None]\n",
    "        ray_drop = ray_drop[:,:,None]\n",
    "#         print(ray_drop)\n",
    "#         #simple depth only\n",
    "#         loss = tf.reduce_mean(tf.square(depth[:,:,None] - testimg[:,:,0])) \n",
    "#         #ray drop only\n",
    "#         L_raydrop = tf.keras.losses.binary_crossentropy(testimg[:,:,1], ray_drop)\n",
    "#         loss = tf.math.reduce_sum(tf.abs(L_raydrop)).numpy()\n",
    "\n",
    "        #LiDAR-NeRF LoSS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        ## L_total = L_dist + lam1*L_intensity + lam2*L_raydrop + lam3*L_reg\n",
    "        ##          lam1=1, lam2=1, lam3=100\n",
    "        ## L_reg --> structural regularization: consider gradient loss only on high-texture areas\n",
    "        \n",
    "        target = testimg[:,:,:1]\n",
    "        target_drop_mask = testimg[:,:,1:]\n",
    "        \n",
    "        #Gradient Loss (structural regularization for smooth surfaces)\n",
    "        thresh = 0.025 #was 0.1 in LiDAR-NeRF\n",
    "        mask = np.zeros(np.shape(target[:,:,0]))\n",
    "        vertical_grad_target = np.gradient(target[:,:,0])[0] \n",
    "        vertical_past_thresh = np.argwhere(tf.abs(vertical_grad_target) > thresh)\n",
    "        mask[vertical_past_thresh[:,0], vertical_past_thresh[:,1]] = 1\n",
    "        horizontal_grad_target = np.gradient(target[:,:,0])[1]\n",
    "        horizontal_past_thresh = np.argwhere(tf.abs(horizontal_grad_target) > thresh)\n",
    "        mask[horizontal_past_thresh[:,0], horizontal_past_thresh[:,1]] = 1\n",
    "        vertical_grad_inference = np.gradient(depth[:,:,0])[0]\n",
    "        horizontal_grad_inference = np.gradient(depth[:,:,0])[1]\n",
    "        mag_difference = tf.math.sqrt((vertical_grad_target-vertical_grad_inference)**2 + (horizontal_grad_target-horizontal_grad_inference)**2)\n",
    "        L_reg = tf.reduce_mean(np.multiply(mag_difference, mask))\n",
    "#         L_reg = tf.reduce_mean(tf.abs(np.multiply(vertical_grad_target-vertical_grad_inference, mask)))\n",
    "        L_reg = tf.cast(L_reg, tf.float32)     \n",
    "        #suppress ray drop areas\n",
    "        L_reg = np.multiply(mag_difference, mask)\n",
    "        L_reg = tf.reduce_mean(tf.multiply(L_reg, target_drop_mask))\n",
    "        L_reg = tf.cast(L_reg, tf.float32)            \n",
    "    \n",
    "        #distance loss\n",
    "        L_dist = tf.reduce_mean(tf.abs(depth - target))\n",
    "        \n",
    "        #ray drop loss\n",
    "        L_raydrop = tf.keras.losses.binary_crossentropy(target_drop_mask, ray_drop)\n",
    "        L_raydrop = tf.math.reduce_sum(tf.abs(L_raydrop))\n",
    "#         print(\"Ray Drop Loss:\", L_raydrop)\n",
    "        \n",
    "        lam1 = 100\n",
    "        lam2 = 1 #1/(64**2)\n",
    "        loss = L_dist + lam1*L_reg + lam2*L_raydrop       \n",
    "\n",
    "#         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "        psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n",
    "        psnrs.append(psnr.numpy())\n",
    "        iternums.append(i)\n",
    "        p.figure(figsize=(10,4))\n",
    "        p.subplot(131)\n",
    "        p.imshow(depth,cmap = \"gray\")#, norm='log')\n",
    "        p.title(f'Estimated Depth at Iteration: {i}')\n",
    "#         p.imshow(testimg[:,:,1],cmap = \"gray\") #, norm='log')\n",
    "#         p.title(f'Actual Mask at Iteration: {i}')\n",
    "        p.subplot(133)\n",
    "        p.plot(iternums, psnrs)\n",
    "        p.title('PSNR')\n",
    "        #look at depth map\n",
    "        p.subplot(132)\n",
    "        p.imshow(ray_drop, cmap=\"gray\")#, norm = 'log')\n",
    "        p.title(\"estimated ray drop mask\")\n",
    "        p.show()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13a546",
   "metadata": {},
   "source": [
    "# Infer point cloud at novel frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rots =  11\n",
    "N_samples = 256 #64 #does not have to match what was used in training?? \n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "for j in range(num_rots):\n",
    "# for j in np.linspace(0,7,15):\n",
    "    #get sensor transformation matrix\n",
    "    rotm = np.eye(4)\n",
    "\n",
    "    # account for image crop in rotation -------------------\n",
    "#     crop_angle = -(phimax-phimin)/2 - j*(phimax-phimin)#square\n",
    "    crop_angle = -(phimax-phimin)/2 + (j+1)*(phimax-phimin)#square-- skip first\n",
    "#     crop_angle = -(phimax-phimin)/16 - j*(phimax-phimin)/8 #eighth\n",
    "#     crop_angle = -(phimax-phimin)/32 - j*(phimax-phimin)/16 #1/16\n",
    "#     crop_angle = -(phimax-phimin)/32 - j*(phimax-phimin)/32 #1/32\n",
    "#     rotm_crop = R.from_euler('xyz', [0,-crop_angle,0]).as_matrix()\n",
    "    rotm_crop = R.from_euler('xyz', [0,-crop_angle + np.pi/2,0]).as_matrix() #looks better\n",
    "    rotm[:3,:3] = rotm[:3,:3] @ rotm_crop\n",
    "\n",
    "    #also need to account for the fact that the LIDAR beam isn't actually centered at horizon\n",
    "    sensor_elevation_zero_rotm = R.from_euler('xyz', [(phimin+phimax)/2,0,0]).as_matrix()\n",
    "    rotm[:3,:3] = rotm[:3,:3] @ sensor_elevation_zero_rotm\n",
    "    # ------------------------------------------------------\n",
    "\n",
    "    # flip x and z axis\n",
    "    rotm[0,-1], rotm[2,-1] = rotm[2,-1], rotm[0,-1] \n",
    "\n",
    "    rotm[0:3,2] *= -1 # flip sign of y and z axis\n",
    "    rotm[0:3,1] *= -1\n",
    "    rotm = rotm[[1,0,2,3],:]\n",
    "    rotm[2,:] *= -1 # flip whole world upside down\n",
    "    rotm[2,-1] = 0.35 #4 #x in world frame output\n",
    "    rotm[0,-1] = -0.005 #- (i/3) #z in world frame output\n",
    "    rotm[1,-1] = -1. #+ (i/6) #y in world frame\n",
    "\n",
    "    rotm = rotm.astype(np.float32)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    #call NeRF using specified novel rotm\n",
    "    rays_o, rays_d = get_rays(H, W, focal, rotm)\n",
    "#     depth, acc = render_rays(model, rays_o, rays_d, near=1., far=64., N_samples=N_samples)\n",
    "    depth, acc, ray_drop = render_rays(model, rays_o, rays_d, near=0., far=4., N_samples=N_samples)\n",
    "    end = time.time()\n",
    "    new_point_cloud_spherical = np.zeros([np.shape(depth)[0]*np.shape(depth)[1],3])\n",
    "    \n",
    "    depth = tf.transpose(depth).numpy() #need this\n",
    "    depth = np.flip(depth, axis = 0) #needed\n",
    "    \n",
    "    #scale back up to normal size\n",
    "    depth *= 20\n",
    "\n",
    "    ray_drop = tf.transpose(ray_drop).numpy() #test\n",
    "    ray_drop = np.flip(ray_drop, axis = 0) #test\n",
    "    \n",
    "#     phimin = -0.53529248 #rad\n",
    "#     phimax = 0.18622663 #rad\n",
    "    #Ouster OS1-64\n",
    "    phimin = np.deg2rad(-16) \n",
    "    phimax = np.deg2rad(17.75)\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for w in range(np.shape(pcs)[1]):\n",
    "        for h in range(np.shape(pcs)[0]):\n",
    "#             #draw all points\n",
    "#             new_point_cloud_spherical[count,0] = depth[w,h] #radius\n",
    "#             suppress ray dropped points\n",
    "            if ray_drop[w,h] > 0.9:\n",
    "                    new_point_cloud_spherical[count,0] = depth[w,h] #radius\n",
    "            else:\n",
    "                    new_point_cloud_spherical[count,0] = 0#100\n",
    "\n",
    "            new_point_cloud_spherical[count,1] = (phimax-phimin)*(w/(np.shape(depth)[0])) #theta (square)\n",
    "#             new_point_cloud_spherical[count,1] = (phimax-phimin)*(w/(np.shape(depth)[0]))/8 #theta (eighth)\n",
    "#             new_point_cloud_spherical[count,1] = (phimax-phimin)*(w/(np.shape(depth)[0]))/16 #theta (1/16)\n",
    "#             new_point_cloud_spherical[count,1] = (phimax-phimin)*(w/(np.shape(depth)[0]))/32 #theta (1/32)\n",
    "            #was this (for simulated data)\n",
    "            new_point_cloud_spherical[count,2] = np.pi/2 + phimax - (phimax-phimin)*(h/np.shape(depth)[1]) #phi\n",
    "            #this works better with Newer College Data?\n",
    "#             new_point_cloud_spherical[count,2] = np.pi/2 - (h/np.shape(depth)[1]) #phi\n",
    "            count+= 1\n",
    "\n",
    "    #account for sweep angle using j\n",
    "#     new_point_cloud_spherical[:,1] -= j*(phimax-phimin) #square\n",
    "    new_point_cloud_spherical[:,1] += (j+1)*(phimax-phimin) #square-- skip first\n",
    "#     new_point_cloud_spherical[:,1] -= j*(phimax-phimin)/8 #eighth\n",
    "#     new_point_cloud_spherical[:,1] -= j*(phimax-phimin)/16 #1/16\n",
    "#     new_point_cloud_spherical[:,1] -= j*(phimax-phimin)/32 #1/32\n",
    "\n",
    "    #if last square patch in sweep, throw away any points that wrap around origin\n",
    "    if j==11:\n",
    "        new_point_cloud_spherical= new_point_cloud_spherical[new_point_cloud_spherical[:,1]>=(-2*np.pi + (phimax-phimin))]\n",
    "    \n",
    "    new_point_cloud_cart = LC.s2c(LC,new_point_cloud_spherical).numpy()\n",
    "    new_point_cloud_cart[:,2] = -new_point_cloud_cart[:,2] #need to flip z \n",
    "\n",
    "    # rainbow by z height\n",
    "    zheight = 100*(np.sin(0.25*new_point_cloud_cart[:,2])+1)\n",
    "    cname = np.array([1-zheight, zheight, 1.5*zheight]).T.tolist()\n",
    "    disp.append(Points(new_point_cloud_cart, c = cname, r = 3, alpha = 0.5))\n",
    "    \n",
    "print(testpose)\n",
    "print(rotm)\n",
    "    \n",
    "plt.show(disp, \"Newer College NeRF\")\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608290dd",
   "metadata": {},
   "source": [
    "# DEBUG\n",
    "#### draw multiple poses old camera projection and new lidar projection on same graph\n",
    "#####         -> Look for areas of overlap that may be poisoning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patches = 3\n",
    "\n",
    "#Ouster OS1-64\n",
    "phimin = np.deg2rad(-16) \n",
    "phimax = np.deg2rad(17.75)\n",
    "# phimin = np.deg2rad(-6) \n",
    "# phimax = np.deg2rad(27.75)\n",
    "\n",
    "#test\n",
    "# phimin, phimax = -(phimax-phimin)/2, (phimax-phimin)/2\n",
    "\n",
    "plt = Plotter(N = 1, axes = 0, bg = (1, 1, 1), interactive = True, sharecam = False) #axes = 4 (simple), 1(scale)\n",
    "disp=[]\n",
    "\n",
    "for count in range(num_patches):\n",
    "\n",
    "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
    "    look_at_pose = count\n",
    "    c2w = poses[look_at_pose]\n",
    "    # c2w = np.eye(3)\n",
    "    c2w = tf.cast(c2w, tf.float32)\n",
    "    near = 0.\n",
    "    far = 64.\n",
    "    focal = np.array(np.shape(images)[1]/(2*np.tan((phimax-phimin)/2))) #default image size\n",
    "    #pinhole camera projection ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    dirs = tf.stack([(i-(W*.5))/focal, -(j-(H*.5))/focal, -tf.ones_like(i)], -1) \n",
    "    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
    "    z_vals = tf.linspace(near, far, N_samples) \n",
    "    z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "\n",
    "    disp.append(Points(tf.reshape(rays_d, [-1,3]), c = 'light blue',r=5, alpha = 0.75)) #compare here, dirs doesn't invoke c2w\n",
    "    disp.append(Points(tf.reshape(rays_d, [-1,3])[:1000], c = 'blue',r=10, alpha = 0.5))\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    #reformat for LiDAR depth measurements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    vert_fov = np.rad2deg(phimax-phimin)\n",
    "#     #had this for simulated data, doensn't work here\n",
    "#     dirs_test = tf.stack([-tf.ones_like(i), \n",
    "#                           -(j-(W*.5))/(focal) - (np.pi/2) - phimax, \n",
    "#                           np.arctan((i-(H*.5))/(focal))], -1)\n",
    "#     dirs_test = tf.reshape(dirs_test,[-1,3])\n",
    "#     #[r, theta, phi] --> [r, phi, theta]\n",
    "#     dirs_test = LC.s2c(LC, tf.transpose(tf.Variable([dirs_test[:,0], dirs_test[:,2], dirs_test[:,1]], dtype = tf.float32)))\n",
    "\n",
    "    #trying again with Newer College data\n",
    "    #[r, theta, phi]\n",
    "    dirs_test = tf.stack([-tf.ones_like(i), #r\n",
    "                          \n",
    "#                           (i-(H*.5))/(focal), #theta\n",
    "                          ((i-31.5)/63)*(phimax-phimin), #test                          \n",
    "                          #phi\n",
    "#                           (phimax + phimin)/2 + ((j-W/2)/64)*(phimax-phimin) -np.pi/2 #correct linspace between phimin/max\n",
    "#                         (phimax + phimin)/2 + ((j-31.5)/63)*(phimax-phimin) -np.pi/2 #right pose but rows in wrong order\n",
    "                          (phimax + phimin)/2 + ((-j+31.5)/63)*(phimax-phimin) -np.pi/2 #better\n",
    "\n",
    "                         ], -1)\n",
    "    dirs_test = tf.reshape(dirs_test,[-1,3])\n",
    "#     print(dirs_test[0,1].numpy(), dirs_test[-1,1].numpy())\n",
    "    dirs_test = LC.s2c(LC, dirs_test)    \n",
    "    \n",
    "    #need to rotate red points into same frame as blue points \n",
    "#     rotm = R.from_euler('xyz', [0,-(np.pi/2)-(((phimax+phimin))/2),0]).as_matrix() #was this\n",
    "    rotm = R.from_euler('xyz', [0,-np.pi/2 + (phimax + phimin)/2,0]).as_matrix() #test\n",
    "    dirs_test = dirs_test @ rotm\n",
    "    dirs_test = dirs_test @ tf.transpose(c2w[:3,:3]) #need this to get dirs to align (but not for pts_flat)\n",
    "    # aligns dirs, doesn't get pts_flat to work yet\n",
    "    dirs_test = dirs_test @ (c2w[:3,:3] \n",
    "                          @ R.from_euler('xyz', [0,0,np.pi/2]).as_matrix() #looked good but converged slightly off\n",
    "                          @ np.linalg.pinv(c2w[:3,:3]) )\n",
    "\n",
    "    # rays_d_test = tf.reduce_sum(dirs_test[..., np.newaxis, :] * c2w[:3,:3], -1) #supposed to be this\n",
    "    rotm_fix = (c2w[:3,:3] \n",
    "                @ np.linalg.pinv(c2w[:3,:3]))\n",
    "    rays_d_test = tf.reduce_sum(dirs_test[..., np.newaxis, :] * rotm_fix, -1) #looks like I need to do this instead\n",
    "    rays_o_test = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d_test))\n",
    "    z_vals_test = tf.linspace(near, far, N_samples) \n",
    "    z_vals_test += tf.random.uniform(list(rays_o_test.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "    #[image_height, image_width, batch_size, 3]\n",
    "    pts_test = rays_o_test[...,None,:] + rays_d_test[...,None,:] * z_vals_test[...,:,None]\n",
    "    pts_flat_test = tf.reshape(pts_test, [-1,3])\n",
    "\n",
    "#     cname = np.array([255*(np.random.rand()), 255*(np.random.rand()), 255*(np.random.rand())]).T.tolist()\n",
    "#     disp.append(Points(dirs_test, c = cname, r = 5, alpha = 0.5))\n",
    "    disp.append(Points(dirs_test, c = 'pink', r = 3, alpha = 0.8))\n",
    "    disp.append(Points(dirs_test[:1000], c = 'red', r = 5, alpha = 0.8))\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "    #draw frames first pose\n",
    "    #forward view direction (-z in NeRF c2w convention) \n",
    "    headings = poses[look_at_pose,:3,:3] @ np.array([0,0,-0.3])\n",
    "    disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"yellow\"))\n",
    "    # x\n",
    "    headings = poses[look_at_pose,:3,:3] @ np.array([0.3,0,0])\n",
    "    disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"red\"))\n",
    "    #y\n",
    "    headings = poses[look_at_pose,:3,:3] @ np.array([0,0.3,0])\n",
    "    disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"green\"))\n",
    "    #z\n",
    "    headings = poses[look_at_pose,:3,:3] @ np.array([0,0,0.3])\n",
    "    disp.append(Arrows(np.zeros([3,1]), (np.zeros([3,1]) + headings), c = \"blue\"))\n",
    "\n",
    "\n",
    "plt.show(disp, \"dirs\", at = 0)\n",
    "ViewInteractiveWidget(plt.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df99dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phimin = np.deg2rad(-6)     # -0.1047 rad\n",
    "# phimax = np.deg2rad(27.75)  #  0.4843 rad\n",
    "phimin = np.deg2rad(-16)     # -0.2793 rad\n",
    "phimax = np.deg2rad(27.75)   #  0.4843 rad\n",
    "\n",
    "# test = (phimax + phimin)/2 + ((j-W/2)/64)*(phimax-phimin) #correct\n",
    "test = (phimax + phimin)/2 + ((j-31.5)/63)*(phimax-phimin) #correct\n",
    "\n",
    "\n",
    "print(test[0,0].numpy())\n",
    "print(test[-1,0].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cbefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asdf = (i-(H*.5))/(focal) #was this\n",
    "asdf = ((i-31.5)/63)*(phimax-phimin)\n",
    "\n",
    "print(asdf[0,0])\n",
    "print(asdf[0,-1])\n",
    "print(asdf[0,0]-asdf[0,-1])\n",
    "\n",
    "test = phimax-phimin\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86426f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "focal = np.array(np.shape(images)[1]/(2*np.tan((phimax-phimin)/2))) #default image size\n",
    "print(focal)\n",
    "print(phimax-phimin)\n",
    "print(W, H)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug-- plot elevation of sensor over time\n",
    "#need to make sure that isn't flipped\n",
    "fig, ax = p.subplots()\n",
    "ax.plot(gt[0:1500,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"models/NCv1.ckpt\")\n",
    "# model.save('models/NCv1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452008f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
