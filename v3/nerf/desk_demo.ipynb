{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcd45ce",
   "metadata": {},
   "source": [
    "### Using COLMAP Structure From Motion (SFM) Package\n",
    "\n",
    "see <https://github.com/Fyusion/LLFF/tree/master> for more documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fde80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pillow_heif import register_heif_opener\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from colmapParsingUtils import *\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# #limit GPU memory ------------------------------------------------\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(gpus)\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     memlim = 12*1024\n",
    "#     tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "#   except RuntimeError as e:\n",
    "#     print(e)\n",
    "# #-----------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# current = os.getcwd()\n",
    "# sys.path.append(current+\"/LLFF\")\n",
    "\n",
    "# from LLFF.llff.poses.pose_utils import gen_poses\n",
    "# import sys\n",
    "\n",
    "#<imgs2poses.py>:\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--match_type', type=str, \n",
    "#                     default='exhaustive_matcher', help='type of matcher used.  Valid options: \\\n",
    "#                     exhaustive_matcher sequential_matcher.  Other matchers not supported at this time')\n",
    "# parser.add_argument('scenedir', type=str,\n",
    "#                     help='input scene directory')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# if args.match_type != 'exhaustive_matcher' and args.match_type != 'sequential_matcher':\n",
    "#     print('ERROR: matcher type ' + args.match_type + ' is not valid.  Aborting')\n",
    "#     sys.exit()\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     gen_poses(args.scenedir, args.match_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert HEIC images to PNG\n",
    "\n",
    "# register_heif_opener()\n",
    "\n",
    "# # image_i = Image.open('desk_images/IMG_2072.HEIC')\n",
    "# image_i = Image.open('bike_images/IMG_2167.HEIC')\n",
    "# i = np.asarray(image_i)\n",
    "# i = i[516:-516,12:-12,:] #crop square\n",
    "# i = i[::3,::3,:]\n",
    "# print(np.shape(i))\n",
    "# plt.imshow(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7faff78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loop through subdirectory and convert each .HEIC to similarly named .PNG\n",
    "register_heif_opener()\n",
    "# rootdir = 'desk_images'\n",
    "rootdir = 'bike_images'\n",
    "\n",
    "mega_image_array = np.zeros([0,1000,1000,3])\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith((\"HEIC\")):\n",
    "            print(file)\n",
    "            image_i = Image.open(rootdir+'/'+file)\n",
    "            i = np.asarray(image_i)\n",
    "            #crop square\n",
    "            if np.shape(i)[0] > np.shape(i)[1]:\n",
    "                i = i[516:-516,12:-12,:]\n",
    "            else:\n",
    "                i = i[12:-12,516:-516,:]\n",
    "            i = i[::3,::3,:]\n",
    "            mega_image_array = np.append(mega_image_array, i[None,:,:,:], axis = 0)\n",
    "            im = Image.fromarray(i)\n",
    "#             im.save(rootdir+\"/\"+file[:-5]+'.PNG')\n",
    "#             os.remove(file) #remove HEIC image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775e8ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(np.shape(mega_image_array[0]))\n",
    "    \n",
    "# for n in range(10):\n",
    "#     plt.imshow(mega_image_array[n]/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed(\"desk_images\", mega_image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load poses estimated by COLMAP\n",
    "\n",
    "images_from_colmap = read_images_text(\"sparseBike/0/images.txt\")\n",
    "cameras = read_cameras_text(\"sparseBike/0/cameras.txt\")\n",
    "pts3d = read_points3D_text(\"sparseBike/0/points3D.txt\")\n",
    "print(images_from_colmap[1])\n",
    "# print(pts3d[1])\n",
    "print(\"\\n\",cameras[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855265d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert COLMAP poses (xyz,quats) to rotm\n",
    "\n",
    "poses = np.zeros([len(images_from_colmap),4,4])\n",
    "# focal = \n",
    "\n",
    "# images = np.zeros([len(poses),100,100,3])\n",
    "images = np.zeros([len(poses),250,250,3])\n",
    "\n",
    "#loop through <images_from_colmap> to get 3D poses of cameras at each timestamp\n",
    "print(len(images_from_colmap))\n",
    "for n in range(1,len(images_from_colmap)+1):\n",
    "#     print(images[n].qvec)\n",
    "    rot33 = R.from_quat(images_from_colmap[n].qvec).as_matrix()\n",
    "    trans31 = images_from_colmap[n].tvec[:,None]\n",
    "    poses[n-1] = np.append(np.append(rot33, trans31, axis = 1), np.array([[0,0,0,1.]]), axis = 0)\n",
    "\n",
    "    #sync order of images with order of poses\n",
    "    temp = cv2.imread(\"bike_images/\"+images_from_colmap[n].name)/255    \n",
    "#     print(np.shape(temp))\n",
    "#     print(np.shape(images[n-1]))\n",
    "#     images[n-1] = temp[::10,::10,:]#downsample\n",
    "    images[n-1] = temp[::4,::4,:]#downsample\n",
    "\n",
    "#GET REST OF PARAMS NEEDED FOR tinyNeRF format~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
    "#out of order??\n",
    "# images = mega_image_array/255 #rescale mega_image_array to [0,1]\n",
    "# images = images[:,::10,::10,:]#downsample\n",
    "\n",
    "H,W = images.shape[1:3]\n",
    "testimg, testpose = images[55], poses[55]\n",
    "\n",
    "# focal = cameras[1].params[0] #test- see if same focal length can be shared across all images\n",
    "# print(focal)\n",
    "focal = np.array(984.411).astype(np.double)\n",
    "print(type(focal))\n",
    "print(focal)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c670a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a038851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify images are set up correclty\n",
    "plt.imshow(images[0])\n",
    "# plt.imshow(images_from_colmap[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e22857",
   "metadata": {},
   "source": [
    "# Construct NeRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587552ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posenc(x):\n",
    "  rets = [x]\n",
    "  for i in range(L_embed):\n",
    "    for fn in [tf.sin, tf.cos]:\n",
    "      rets.append(fn(2.**i * x))\n",
    "  return tf.concat(rets, -1)\n",
    "\n",
    "L_embed = 6\n",
    "embed_fn = posenc\n",
    "# L_embed = 0\n",
    "# embed_fn = tf.identity\n",
    "\n",
    "def init_model(D=8, W=256):\n",
    "    relu = tf.keras.layers.ReLU()    \n",
    "    dense = lambda W=W, act=relu : tf.keras.layers.Dense(W, activation=act)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(3 + 3*2*L_embed)) \n",
    "    outputs = inputs\n",
    "    for i in range(D):\n",
    "        outputs = dense()(outputs)\n",
    "        if i%4==0 and i>0:\n",
    "            outputs = tf.concat([outputs, inputs], -1)\n",
    "    outputs = dense(4, act=None)(outputs)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
    "    dirs = tf.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -tf.ones_like(i)], -1)\n",
    "    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
    "    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "\n",
    "def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, rand=False):\n",
    "\n",
    "    def batchify(fn, chunk=1024*32):\n",
    "        return lambda inputs : tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    \n",
    "    # Compute 3D query points\n",
    "    z_vals = tf.linspace(near, far, N_samples) \n",
    "    if rand:\n",
    "      z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "    \n",
    "    # Run network\n",
    "    pts_flat = tf.reshape(pts, [-1,3])\n",
    "    pts_flat = embed_fn(pts_flat)\n",
    "    raw = batchify(network_fn)(pts_flat)\n",
    "    raw = tf.reshape(raw, list(pts.shape[:-1]) + [4])\n",
    "    \n",
    "    # Compute opacities and colors\n",
    "    sigma_a = tf.nn.relu(raw[...,3])\n",
    "    rgb = tf.math.sigmoid(raw[...,:3]) \n",
    "    \n",
    "    # Do volume rendering\n",
    "    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[...,:1].shape)], -1) \n",
    "    alpha = 1.-tf.exp(-sigma_a * dists)  \n",
    "    weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
    "    \n",
    "    rgb_map = tf.reduce_sum(weights[...,None] * rgb, -2) \n",
    "    depth_map = tf.reduce_sum(weights * z_vals, -1) \n",
    "    acc_map = tf.reduce_sum(weights, -1)\n",
    "\n",
    "    return rgb_map, depth_map, acc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cb37a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = init_model(D=8, W=256)\n",
    "# model = init_model(D=8, W=64) #test simple model -- nope\n",
    "optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "\n",
    "N_samples = 32 #64 #decrease as needed for VRAM(?)\n",
    "N_iters = 1000\n",
    "psnrs = []\n",
    "iternums = []\n",
    "i_plot = 10 #25\n",
    "\n",
    "import time\n",
    "t = time.time()\n",
    "for i in range(N_iters+1):\n",
    "    \n",
    "    img_i = np.random.randint(images.shape[0])\n",
    "    target = images[img_i]\n",
    "    pose = poses[img_i]\n",
    "    \n",
    "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "    rays_o = tf.cast(rays_o, tf.float32)\n",
    "    rays_d = tf.cast(rays_d, tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples, rand=True)\n",
    "        loss = tf.reduce_mean(tf.square(rgb - target))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    if i%i_plot==0:\n",
    "        print(i, (time.time() - t) / i_plot, 'secs per iter')\n",
    "        t = time.time()\n",
    "        \n",
    "        # Render the holdout view for logging\n",
    "        rays_o, rays_d = get_rays(H, W, focal, testpose)\n",
    "        rays_o = tf.cast(rays_o, tf.float32)\n",
    "        rays_d = tf.cast(rays_d, tf.float32)\n",
    "        \n",
    "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n",
    "        loss = tf.reduce_mean(tf.square(rgb - testimg))\n",
    "        psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n",
    "\n",
    "        psnrs.append(psnr.numpy())\n",
    "        iternums.append(i)\n",
    "        \n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(rgb)\n",
    "        plt.title(f'Iteration: {i}')\n",
    "        plt.subplot(122)\n",
    "        plt.plot(iternums, psnrs)\n",
    "        plt.title('PSNR')\n",
    "        plt.show()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interactive, widgets\n",
    "\n",
    "\n",
    "trans_t = lambda t : tf.convert_to_tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,t],\n",
    "    [0,0,0,1],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "rot_phi = lambda phi : tf.convert_to_tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,tf.cos(phi),-tf.sin(phi),0],\n",
    "    [0,tf.sin(phi), tf.cos(phi),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "rot_theta = lambda th : tf.convert_to_tensor([\n",
    "    [tf.cos(th),0,-tf.sin(th),0],\n",
    "    [0,1,0,0],\n",
    "    [tf.sin(th),0, tf.cos(th),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
    "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
    "    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w\n",
    "    return c2w\n",
    "\n",
    "\n",
    "def f(**kwargs):\n",
    "    c2w = pose_spherical(**kwargs)\n",
    "    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n",
    "    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n",
    "    img = np.clip(rgb,0,1)\n",
    "    \n",
    "    plt.figure(2, figsize=(20,6))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "sldr = lambda v, mi, ma: widgets.FloatSlider(\n",
    "    value=v,\n",
    "    min=mi,\n",
    "    max=ma,\n",
    "    step=.01,\n",
    ")\n",
    "\n",
    "names = [\n",
    "    ['theta', [100., 0., 360]],\n",
    "    ['phi', [-30., -90, 0]],\n",
    "    ['radius', [4., 3., 5.]],\n",
    "]\n",
    "\n",
    "interactive_plot = interactive(f, **{s[0] : sldr(*s[1]) for s in names})\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea0cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
