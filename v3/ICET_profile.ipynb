{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df034054",
   "metadata": {},
   "source": [
    "# Notebook for identifying and removing bottlenecks from ICET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc1c9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 15:53:47.488622: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-06 15:53:47.584271: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-06 15:53:47.959506: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/derm/anaconda3/envs/py39/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-12-06 15:53:47.959560: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/derm/anaconda3/envs/py39/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-12-06 15:53:47.959564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-06 15:53:48.370912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 15:53:48.396393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 15:53:48.396621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 15:53:48.510803: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 15:53:48.511589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 15:53:48.511761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 15:53:48.511891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 15:53:48.847811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 15:53:48.847985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 15:53:48.848112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 15:53:48.848223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "from vedo import *\n",
    "import os\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "import pykitti\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 12*1024\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "# tf.config.set_visible_devices([], 'GPU') #run on CPU only -- seems to actually execute main parts of code faster here...\n",
    "\n",
    "from tensorflow.math import sin, cos, tan\n",
    "import tensorflow_probability as tfp\n",
    "from ICET_spherical import ICET\n",
    "from utils import R_tf\n",
    "from metpy.calc import lat_lon_grid_deltas\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180\n",
    "# %matplotlib notebook\n",
    "\n",
    "# %%bash\n",
    "# # python -m cProfile scan_match.py\n",
    "# python scan_match.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74e4c23f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth poses are not avaialble for sequence 09.\n",
      "\n",
      " converting to spherical took 0.019266366958618164 \n",
      " total:  0.019269704818725586\n",
      "\n",
      " getting bounds took 0.019212722778320312 seconds\n",
      "\n",
      " took  0.02342844009399414 seconds to get points in cluster\n",
      "\n",
      " getting spherical grid 0.06996917724609375 \n",
      " total:  0.08925962448120117\n",
      "\n",
      " fit_gaussian for scan 1 0.015787124633789062 \n",
      " total:  0.10505485534667969\n",
      "\n",
      " took  0.014899492263793945 seconds to get points in cluster\n",
      "\n",
      " took  0.015195131301879883 seconds to get points in cluster\n",
      "\n",
      " took  0.014623403549194336 seconds to get points in cluster\n",
      "\n",
      " got U and L cluster 0.05681180953979492 \n",
      " total:  0.16188287734985352\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " transforming scan2 0.0020759105682373047 \n",
      " total:  0.1639714241027832 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " took  0.023812532424926758 seconds to get points in cluster\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " fit_gaussian for scan 2 0.03985929489135742 \n",
      " total:  0.203843355178833 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor([ 0.2872482   0.0211786   0.00267774 -0.00082576  0.001083   -0.00499498], shape=(6,), dtype=float32)\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " correcting solution estimate 0.01920175552368164 \n",
      " total:  0.22307348251342773 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " transforming scan2 0.0020885467529296875 \n",
      " total:  0.22517728805541992 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " took  0.02413797378540039 seconds to get points in cluster\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " fit_gaussian for scan 2 0.039897918701171875 \n",
      " total:  0.2650883197784424 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 5.6044531e-01  1.6629867e-02  7.1529699e-03 -5.0318253e-04\n",
      "  1.4684945e-03 -2.0207828e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " correcting solution estimate 0.01779794692993164 \n",
      " total:  0.2829000949859619 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " transforming scan2 0.002042055130004883 \n",
      " total:  0.2849545478820801 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " took  0.02407693862915039 seconds to get points in cluster\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " fit_gaussian for scan 2 0.03965425491333008 \n",
      " total:  0.32462191581726074 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 7.5810540e-01  8.3502354e-03  1.0037493e-02 -4.4571731e-04\n",
      "  1.7933178e-03  4.0010246e-04], shape=(6,), dtype=float32)\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " correcting solution estimate 0.017731428146362305 \n",
      " total:  0.34236621856689453 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " transforming scan2 0.002046346664428711 \n",
      " total:  0.34442567825317383 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " took  0.023723125457763672 seconds to get points in cluster\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " fit_gaussian for scan 2 0.039328575134277344 \n",
      " total:  0.38376855850219727 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 8.5116887e-01  9.4497018e-03  1.1090063e-02 -3.3274764e-04\n",
      "  1.9297987e-03  1.4813913e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " correcting solution estimate 0.017548084259033203 \n",
      " total:  0.4013364315032959 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " transforming scan2 0.002041339874267578 \n",
      " total:  0.40339064598083496 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " took  0.025409936904907227 seconds to get points in cluster\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " fit_gaussian for scan 2 0.04430723190307617 \n",
      " total:  0.44771289825439453 \n",
      " ~~~~~~~~~~~~~~\n",
      "\n",
      " estimated solution vector X: \n",
      " tf.Tensor(\n",
      "[ 8.9495701e-01  1.0515696e-02  1.1896235e-02 -3.2564672e-04\n",
      "  2.0113059e-03  2.1242215e-03], shape=(6,), dtype=float32)\n",
      "\n",
      " ~~~~~~~~~~~~~~ \n",
      " correcting solution estimate 0.017971277236938477 \n",
      " total:  0.4656996726989746 \n",
      " ~~~~~~~~~~~~~~\n",
      "pred_stds: \n",
      " tf.Tensor(\n",
      "[1.55315222e-03 5.43584989e-04 1.07104737e-04 2.71534518e-05\n",
      " 1.66262726e-05 1.18717064e-04], shape=(6,), dtype=float32)\n",
      " L2: \n",
      " tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]], shape=(6, 6), dtype=float32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ICET' object has no attribute 'plt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# fn1 = \"/home/derm/ASAR/v3/spherical_paper/MC_trajectories/scene1_scan13.txt\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# c1 = np.loadtxt(fn1)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# fn2 = \"/home/derm/ASAR/v3/spherical_paper/MC_trajectories/scene1_scan14.txt\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# c2 = np.loadtxt(fn2)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m it \u001b[38;5;241m=\u001b[39m ICET(cloud1 \u001b[38;5;241m=\u001b[39m c1, cloud2 \u001b[38;5;241m=\u001b[39m c2, fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m, niter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \n\u001b[1;32m     16\u001b[0m            draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, group \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, RM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, DNN_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m ViewInteractiveWidget(\u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mwindow)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ICET' object has no attribute 'plt'"
     ]
    }
   ],
   "source": [
    "basepath = '/media/derm/06EF-127D2/KITTI'\n",
    "# sequence = '03' #forest\n",
    "sequence = '09' #trees and small town\n",
    "dataset = pykitti.odometry(basepath, sequence)\n",
    "velo1 = dataset.get_velo(400)\n",
    "c1 = velo1[:,:3]\n",
    "velo2 = dataset.get_velo(401)\n",
    "c2 = velo2[:,:3]\n",
    "\n",
    "# fn1 = \"/home/derm/ASAR/v3/spherical_paper/MC_trajectories/scene1_scan13.txt\"\n",
    "# c1 = np.loadtxt(fn1)\n",
    "# fn2 = \"/home/derm/ASAR/v3/spherical_paper/MC_trajectories/scene1_scan14.txt\"\n",
    "# c2 = np.loadtxt(fn2)\n",
    "\n",
    "it = ICET(cloud1 = c1, cloud2 = c2, fid = 50, niter = 5, \n",
    "           draw = False, group = 2, RM = False, DNN_filter = False)\n",
    "ViewInteractiveWidget(it.plt.window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d4187",
   "metadata": {},
   "source": [
    "# get_points_in_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "ec4ddeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpc_old(cloud, occupied_spikes, bounds):\n",
    "    \"\"\" returns ragged tensor containing the indices of points in <cloud> in each cluster \n",
    "        cloud = point cloud tensor\n",
    "        occupied_spikes = tensor containing idx of spikes corresponding to bounds\n",
    "        bounds = tensor containing min and max radius for each occupied spike\n",
    "    \"\"\"\n",
    "    st = time.time()\n",
    "    fid_theta = 50\n",
    "    fid_phi = 50//3\n",
    "    thetamin = -np.pi\n",
    "    thetamax = np.pi #-  2*np.pi/self.fid_theta\n",
    "    phimin =  3*np.pi/8\n",
    "    phimax = 7*np.pi/8\n",
    "\n",
    "    edges_phi = tf.linspace(phimin, phimax, fid_phi) #was this for regular cells\n",
    "    bins_phi = tfp.stats.find_bins(cloud[:,2], edges_phi)\n",
    "\n",
    "    edges_theta = tf.linspace(thetamin, thetamax, fid_theta + 1)\n",
    "    bins_theta = tfp.stats.find_bins(cloud[:,1], edges_theta)\n",
    "\n",
    "    spike_idx = tf.cast(bins_theta*(fid_phi-1) + bins_phi, tf.int32)\n",
    "#     print(spike_idx[:50])\n",
    "    \n",
    "    #get idx of spike for each applicable point\n",
    "    cond1 = spike_idx == occupied_spikes[:,None] #match spike IDs\n",
    "    cond2 = cloud[:,0] < tf.cast(bounds[:,1][:,None], tf.float32) #closer than max bound\n",
    "    cond3 = cloud[:,0] > tf.cast(bounds[:,0][:,None], tf.float32) #further than min bound\n",
    "#     #this is the most computationally expensive part ---------\n",
    "#     before = time.time()\n",
    "#     cond1 = tf.math.equal(spike_idx, occupied_spikes[:,None])  #find where spike_idx matches spike ID\n",
    "# #     print(\"\\n took \", time.time() - before , \"seconds to do cond 1\" )\n",
    "#     #---------------------------------------------------------\n",
    "    # cond2 = tf.math.less(cloud[:,0], bounds[:,1][:,None]) #closer than max bound\n",
    "    # cond3 = tf.math.greater(cloud[:,0], bounds[:,0][:,None]) #further than min bound\n",
    "\n",
    "    inside1 = tf.where(tf.math.reduce_all(tf.Variable([cond1, cond2, cond3]), axis = 0))\n",
    "    numPtsPerCluster = tf.math.bincount(tf.cast(inside1[:,0], tf.int32))\n",
    "    inside1 = tf.RaggedTensor.from_value_rowids(inside1[:,1], inside1[:,0])\n",
    "\n",
    "    print(\"\\n took \", time.time() -st , \"seconds to get points in cluster with old method\" )\n",
    "    return(inside1, numPtsPerCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "9041663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpc_new(cloud, occupied_spikes, bounds):\n",
    "    \"\"\"New method of finding which voxel each point in a scan falls into\"\"\"\n",
    "    \n",
    "    st = time.time()\n",
    "\n",
    "    fid_theta = 50\n",
    "    fid_phi = 50//3\n",
    "    \n",
    "    thetamin = -np.pi\n",
    "    thetamax = np.pi\n",
    "    phimin =  3*np.pi/8\n",
    "    phimax = 7*np.pi/8\n",
    "\n",
    "    edges_phi = tf.linspace(phimin, phimax, fid_phi) #was this for regular cells\n",
    "    bins_phi = tfp.stats.find_bins(cloud[:,2], edges_phi)\n",
    "\n",
    "    edges_theta = tf.linspace(thetamin, thetamax, fid_theta + 1)\n",
    "    bins_theta = tfp.stats.find_bins(cloud[:,1], edges_theta)\n",
    "\n",
    "    spike_idx = tf.cast(bins_theta*(fid_phi-1) + bins_phi, tf.int32)\n",
    "    print(\"\\n spike_idx \\n\", spike_idx[:])\n",
    "    \n",
    "    #first get ragged tensor grouping all points by their respective horiz/vertial angular bins\n",
    "    #re-arrange all spike idx in ascending order\n",
    "    spike_idx_ascending_idx = tf.argsort(spike_idx)\n",
    "    spike_idx_ascending = tf.gather(spike_idx, spike_idx_ascending_idx)\n",
    "    print(\"\\n spike_idx_ascending \\n\", spike_idx_ascending)\n",
    "    \n",
    "    u, u_idx = tf.unique(spike_idx_ascending)\n",
    "#     print(\"\\n u \\n\", u[:10], \"\\n\", tf.shape(u))\n",
    "#     print(\"\\n u_idx \\n\", u_idx[:100], \"\\n\", tf.shape(u_idx))\n",
    "    \n",
    "    original_index = tf.range(len(u_idx))\n",
    "#     print(\"\\n original_index \\n\", original_index) \n",
    "    sorted_idx = tf.gather(original_index, spike_idx_ascending_idx)\n",
    "    print(\"\\n sorted_idx \\n\", sorted_idx) \n",
    "\n",
    "    #correclty clusterd by radial bounds (BUT in wrong order)\n",
    "    inside1 = tf.RaggedTensor.from_value_rowids(sorted_idx, u_idx)\n",
    "    \n",
    "    print(\"\\n occupied_spikes \\n\", occupied_spikes)\n",
    "    occupied_in_ascending_order_idx = tf.argsort(occupied_spikes)\n",
    "    print(\"\\n occupied_in_ascending_order_idx \\n\", occupied_in_ascending_order_idx )\n",
    "    \n",
    "    inside1 = tf.gather(inside1, occupied_in_ascending_order_idx)\n",
    "    \n",
    "    # then remove points that are too far inside or outside radial bounds of each cell    \n",
    "    print(\"\\n took \", time.time() -st , \"seconds to get points in cluster with new method\" )\n",
    "#     inside1 = None\n",
    "    numPtsPerCluster = None\n",
    "    return(inside1, numPtsPerCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "732ead2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " took  0.04666256904602051 seconds to get points in cluster with old method\n",
      "\n",
      " old_in1 \n",
      " tf.Tensor(\n",
      "[   25    93   526   610   629   647   668   686  1229  1850  2160  2182\n",
      "  2312  2453  2464  3386  3644  3730  4224  4474  5563  6136  6866  6915\n",
      "  7141  7331  7844  8392  8835  8998  9442  9467  9700  9786 10310 10427\n",
      " 10717 11405 11760 11848 12154 12369 12378 12617 12821 13034 13927 14437\n",
      " 14546 14815 17100 17295 17607 17691 18299 18303 18566 19482 19844 19876\n",
      " 20034 20398 20863 21944 22520 22894 23440 24024 24028 24076 24088 24517\n",
      " 25455 25703 26031 26068 26190 26304 26382 26510 26580 27113 27610 27632\n",
      " 27697 27821 28203 28337 28952 29708 30075 30109 30113 30721 30776 30886\n",
      " 31613 32265 32428 32597], shape=(100,), dtype=int64)\n",
      "\n",
      " ------------------------------ \n",
      "\n",
      "\n",
      " spike_idx \n",
      " tf.Tensor([199 574 440 ... 695 319 184], shape=(124598,), dtype=int32)\n",
      "\n",
      " spike_idx_ascending \n",
      " tf.Tensor([  3   3   3 ... 741 741 742], shape=(124598,), dtype=int32)\n",
      "\n",
      " sorted_idx \n",
      " tf.Tensor([    23     29     89 ... 124004 124251  56458], shape=(124598,), dtype=int32)\n",
      "\n",
      " occupied_spikes \n",
      " tf.Tensor(\n",
      "[741 335 468 697 516 305 155 320 470  78 441 141  94 469 214 649 380 411\n",
      "   3 244 275 423 291 457   5 547 288 274 184 530 499 562 456 292 273 379\n",
      " 198 319 140 725 409 215 501 666 258  93 619 592 169 651 678  64 589 138\n",
      " 393 277 695 455 290 123 636 663 394  63 336 532 517 561 577 576 681  81\n",
      " 573 109 472 591 531 738 546 588 708 199 502  21 650 515 127 201 168 709\n",
      " 483 154 108 185   4 710 605 693 424 500 635 634 514 680  19 408 170  80\n",
      " 395  79 125 230 259 348 590  50 333 558 694 260 618  67  95 438 306 544\n",
      "  49 425 665 172 696 187 740 350 349 364 648 664   6 606 667 381 156 321\n",
      " 303 711  82 724 454 453 484 351 318 334 439  48 620 139 682 513 726  66\n",
      " 397 289 304 378 243 543  33  34 202 485 111  96 498 200 276 574 153 322\n",
      " 126 171 110  65 337 739 365 412 529  36 528 307 621 575 261  35  97 633\n",
      " 604  51 183 246 186 603 622 637 545 679 157 559 427 426 124 229 228 216\n",
      "  18 366 363 487 262 396 486 607 442 723 242 471  52 213 142 245 367  20\n",
      " 382 247 560 440 410 112 352 217 232 227 231 652  37], shape=(247,), dtype=int32)\n",
      "\n",
      " occupied_in_ascending_order_idx \n",
      " tf.Tensor(\n",
      "[ 18  94  24 138 216 104 233  83 168 169 195 189 246 155 126 115 199 228\n",
      "  63  51 183 161 121   9 109 107  71 146  45  12 122 173 196  92  73 182\n",
      " 172 239  59 212 110 180  86  53 157  38  11 230 178  91   6 142 208  88\n",
      "  48 106 181 129 200  28  93 202 131  36  81 175  87 170 229  14  41 215\n",
      " 241 243 214 213 111 244 242 226 166  19 231 201 235  44 112 119 194 220\n",
      "  34  27  20 176  55  26 163  58  22  33 144 164   5 124 191 152  37   7\n",
      " 143 179 116 153   1  64 184 113 134 133 151 240 218 135 186 217 232 165\n",
      "  35  16 141 234  54  62 108 221 162 105  40 238  17 187  21  98 127 211\n",
      " 210 123 154 237  10 224 149 148  57  32  23   2  13   8 227  74  90 150\n",
      " 171 222 219 174  30  99  42  82 159 102  85   4  66 190 188  29  76  65\n",
      " 167 125 206  78  25 117 209 236  67  31  72 177 193  69  68  79  52 114\n",
      "  75  47 203 198  96 139 223 120  46 156 192 204 197 101 100  60 205 136\n",
      "  15  84  49 245  61 137 128  43 140  50 207 103  70 158  97 118  56 130\n",
      "   3  80  89  95 145 225 147  39 160  77 185 132   0], shape=(247,), dtype=int32)\n",
      "\n",
      " took  0.017075538635253906 seconds to get points in cluster with new method\n",
      "\n",
      " new_in1 \n",
      " tf.Tensor(\n",
      "[    35    702    980   4074   4774   5211   6109   8841  12386  15153\n",
      "  15907  19100  19311  20545  20760  24019  24104  25133  25891  28315\n",
      "  29801  30120  30262  30784  34591  35194  35415  36095  37103  37719\n",
      "  38620  39836  40631  40712  41493  41687  41771  44136  45171  48786\n",
      "  49995  51022  52185  52750  53517  54004  54490  56220  57506  58315\n",
      "  58544  62531  63878  64087  68359  70782  73647  75000  75079  76766\n",
      "  77546  77738  78571  79164  79431  80346  82698  83675  84222  85396\n",
      "  87532  90289  90794  90973  91001  91404  93282  94794  94921  95191\n",
      "  97008 103045 104155 104985 107971 109662 111109 113237 113423 115807\n",
      " 116883 118442 120797 121454], shape=(94,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#old method takes ~0.04s (slow)\n",
    "old_in1, old_npc = gpc_old(it.cloud2_tensor_spherical, it.occupied_spikes, it.bounds)\n",
    "# print(it.occupied_spikes)\n",
    "print(\"\\n old_in1 \\n\", old_in1[0][:100])\n",
    "# print(\"\\n old_in1 \\n\", tf.shape(old_in1))\n",
    "print(\"\\n ------------------------------ \\n\")\n",
    "\n",
    "#new method (should be significantly faster)\n",
    "new_in1, new_npc = gpc_new(it.cloud2_tensor_spherical, it.occupied_spikes, it.bounds)\n",
    "print(\"\\n new_in1 \\n\", new_in1[0][:100])\n",
    "# print(\"\\n new_in1 \\n\", tf.shape(new_in1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "0aa44a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54d137da74845b993910315f8679187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=1043, layout=Layout(height='auto', width='100%'), width=1280)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt1 = Plotter(N = 1, axes = 4, bg = (1, 1, 1), interactive = True)\n",
    "disp = []\n",
    "\n",
    "pts_old = tf.gather(it.cloud2_tensor_OG, old_in1[0][:100]).numpy()\n",
    "pts_new = tf.gather(it.cloud2_tensor_OG, new_in1[0][:100]).numpy()\n",
    "\n",
    "disp.append(Points(pts_old, c = 'red', r = 6, alpha = 1))\n",
    "disp.append(Points(pts_new, c = 'blue', r = 6, alpha = 1))\n",
    "disp.append(Points(it.cloud2_tensor_OG, c = 'black', r = 3, alpha = 1))\n",
    "\n",
    "plt1.show(disp, \"debug indexing for fast voxel search\")\n",
    "ViewInteractiveWidget(plt1.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "a930a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values \n",
      " tf.Tensor([0.29603457 0.7836784  0.6381947  0.5451517  0.35780048 0.7451891 ], shape=(6,), dtype=float32)\n",
      "tf.Tensor([0.29603457 0.6381947  0.5451517  0.35780048 0.7836784  0.7451891 ], shape=(6,), dtype=float32)\n",
      "\n",
      " rag \n",
      " <tf.RaggedTensor [[0.29603457], [], [0.6381947], [0.5451517], [0.35780048],\n",
      " [0.7836784, 0.7451891]]>\n",
      "\n",
      " test \n",
      " <tf.RaggedTensor [[0.6381947],\n",
      " [0.6381947],\n",
      " [0.5451517]]>\n"
     ]
    }
   ],
   "source": [
    "#test TF ragged from value row IDs\n",
    "values = tf.random.uniform([6])\n",
    "print(\"values \\n\", values)\n",
    "# row_ids = tf.constant([1,2,3,4,0,5], dtype = tf.int32)\n",
    "row_ids = tf.constant([0,5,2,3,4,5], dtype = tf.int32)\n",
    "\n",
    "row_id_indices_sorted = tf.argsort(row_ids)\n",
    "row_ids_sorted = tf.gather(row_ids, row_id_indices_sorted)\n",
    "values_sorted = tf.gather(values, row_id_indices_sorted)\n",
    "print(values_sorted)\n",
    "\n",
    "rag = tf.RaggedTensor.from_value_rowids(values_sorted, row_ids_sorted)\n",
    "print(\"\\n rag \\n\", rag)\n",
    "\n",
    "print(\"\\n test \\n\", tf.gather(rag, tf.constant([2, 2, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ff0c0",
   "metadata": {},
   "source": [
    "# get_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt2(rads, thresh = 0.5, mnp = 100):\n",
    "    \"\"\"testing new method of finding radial bins for spherical voxels\"\"\"\n",
    "    \n",
    "    before = time.time()\n",
    "\n",
    "    max_buffer = 0.2 \n",
    "\n",
    "    if len(tf.shape(rads)) < 2:\n",
    "        rads = rads[:,None]\n",
    "\n",
    "    OG_rads = rads #hold on to OG rads\n",
    "    #replace all zeros in rads (result of converting ragged -> standard tensor) with some arbitrarily large value\n",
    "    mask = tf.cast(tf.math.equal(rads, 0), tf.float32)*1000\n",
    "    rads = rads + mask\n",
    "    # print(rads)\n",
    "\n",
    "    #sort in ascending order for each column in tensor\n",
    "    top_k = tf.math.top_k(tf.transpose(rads), k = tf.shape(rads)[0])\n",
    "#     print(\"\\n top_k \\n\", top_k[1])\n",
    "    rads = tf.transpose(tf.gather(tf.transpose(rads), top_k[1], batch_dims = 1))\n",
    "    rads = tf.reverse(rads, axis = tf.constant([0]))\n",
    "#     print(\"rads \\n\", rads)\n",
    "\n",
    "    # calculate the forward difference between neighboring points\n",
    "    z = tf.zeros([1, tf.shape(rads)[1].numpy()])\n",
    "    shifted = tf.concat((rads[1:], z), axis = 0)\n",
    "    diff = shifted - rads\n",
    "\n",
    "    # #find where difference jumps\n",
    "    jumps = tf.where(diff > thresh)\n",
    "#     print(\"\\n jumps \\n\", jumps) #[idx of jump, which spike is jumping]\n",
    "\n",
    "    #----------------------------------------------------------------------\n",
    "    #not sure if actually needed...\n",
    "    #get indexes of all used spikes\n",
    "    used = jumps[:,1][None,:]\n",
    "    # print(\"used\", used)\n",
    "    biggest = tf.math.reduce_max(used, axis = 1).numpy()[0]\n",
    "    # print(\"biggest\", biggest)\n",
    "    all_spikes = tf.cast(tf.linspace(0,biggest,biggest+1), tf.int64)[None,:] #list all spikes total\n",
    "    # print(\"all_spikes\", all_spikes)\n",
    "\n",
    "    #find differnce\n",
    "    missing = tf.sets.difference(all_spikes, used).values[None,:]\n",
    "    # print(\"\\n missing\", missing)\n",
    "    # z = tf.zeros(tf.shape(missing), dtype = tf.int64) #wrong...\n",
    "    # z = 51*tf.ones(tf.shape(missing), dtype = tf.int64) #wrong...\n",
    "    # print(\"z\", z)\n",
    "\n",
    "    #z should be this...\n",
    "    # print(\"\\n OG_rads\", OG_rads)\n",
    "    # ends = tf.math.argmax(OG_rads, axis = 0) #wrong -> not max arg, last nonzero argument!!\n",
    "    zero = tf.constant(0, dtype = tf.float32)\n",
    "    ends = tf.math.reduce_sum(tf.cast(tf.not_equal(OG_rads, zero), tf.int64), axis = 0) #correct\n",
    "    # print(\"\\n ends\", ends)\n",
    "\n",
    "    test = tf.gather(ends, missing[0])  #get index of last element of missing jump section\n",
    "    # print(\"\\n test\", test)\n",
    "    z = test[None,:]\n",
    "    z -= 2 #fixes indexing bug\n",
    "    # print(\"z\", z)\n",
    "\n",
    "    missing = tf.transpose(tf.concat((z, missing), axis = 0))\n",
    "    # print(missing)\n",
    "\n",
    "    jumps = tf.concat((jumps, missing), axis = 0) #concat missing stuff back at the end of jumps\n",
    "#     print(\"\\n jumps after fix\", jumps)\n",
    "    #----------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n jumps: \\n\", jumps)\n",
    "    \n",
    "    #find where the first large cluster occurs in each spike\n",
    "   \n",
    "\n",
    "    \n",
    "    bounds = None\n",
    "\n",
    "    return(bounds, jumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f50c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_cluster, get_cluster_fast\n",
    "# print(\"rads: \\n\", it.rads)\n",
    "\n",
    "s = time.time()\n",
    "bounds_old = get_cluster(it.rads, mnp = it.min_num_pts)\n",
    "print(\"\\n took\", time.time() - s, \" s with old method \\n\")\n",
    "print(\"\\n bounds_old: \\n\", bounds_old[:10])\n",
    "print(np.shape(bounds_old))\n",
    "\n",
    "s = time.time()\n",
    "# bounds_new, jumps = gt2(it.rads, mnp = it.min_num_pts)\n",
    "bounds_new = get_cluster_fast(it.rads, mnp = it.min_num_pts)\n",
    "print(\"\\n bounds_new: \\n\", bounds_new[:10])\n",
    "print(\" \\n took\", time.time() - s, \" s with new method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7756ff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#identifying location of jumps without looping\n",
    "print(\"old slow soln shape: \\n\", tf.shape(bounds_old)) #want to produce this same shape!!!\n",
    "# print(\"\\n bounds_old: \\n\", bounds_old[:10])\n",
    "\n",
    "bounds_new, jumps = gt2(it.rads, mnp = it.min_num_pts)\n",
    "\n",
    "#get all radial measurements\n",
    "#(temp-- already done inside function)-----------------------------\n",
    "mask = tf.cast(tf.math.equal(it.rads, 0), tf.float32)*1000\n",
    "rads = it.rads + mask\n",
    "#sort in ascending order for each column in tensor\n",
    "top_k = tf.math.top_k(tf.transpose(rads), k = tf.shape(rads)[0])\n",
    "rads = tf.transpose(tf.gather(tf.transpose(rads), top_k[1], batch_dims = 1))\n",
    "rads = tf.reverse(rads, axis = tf.constant([0]))\n",
    "# print(\"\\n rads: \\n\", rads[:10])\n",
    "# print(\"\\n rads: \\n\", np.shape(rads))\n",
    "# print(\"\\n it.rads \\n\", it.rads)\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "# print(\"\\n jumps: \\n\", tf.shape(jumps))\n",
    "# print(\"\\n jumps: \\n\", jumps) #[idx of jump, which spike is jumping]\n",
    "\n",
    "# y, idx = tf.unique(jumps[:,0]) #was this\n",
    "jumps_temp = tf.gather(jumps, tf.argsort(jumps[:,1]), axis=0) #reorder based on index\n",
    "y, idx = tf.unique(jumps_temp[:,1]) #test\n",
    "print(\"\\n y \\n\", y[:15], \"\\n\", tf.shape(y), \"\\n \\n idx \\n\", idx[:15], \"\\n\", tf.shape(idx))\n",
    "# print(\"\\n jumps_temp \\n\", jumps_temp[:15])\n",
    "# print(\"\\n jumps[:,_]: \\n\", jumps[:,0])\n",
    "\n",
    "# get ragged tensor containing indices where jumps occur inside each wedge shaped voxel\n",
    "# jumps_rag = tf.RaggedTensor.from_value_rowids(jumps[:,1], idx) #WAS THIS --wrong!!\n",
    "\n",
    "jumps_rag = tf.RaggedTensor.from_value_rowids(jumps_temp[:,0], jumps_temp[:,1]) #TEST - should be this??\n",
    "# jumps_rag = tf.RaggedTensor.from_value_rowids(jumps[:,1], jumps[:,0]) #TEST\n",
    "# print(\"\\n jumps_rag \\n\", jumps_rag[:15])\n",
    "print(\"\\n rads[0,_] \\n\", rads[:30,0])\n",
    "\n",
    "\n",
    "# append 0 to beginning of each ragged elemet of jumps_rag\n",
    "zeros = tf.zeros(tf.shape(jumps_rag)[0])[:,None]\n",
    "zeros = tf.cast(tf.RaggedTensor.from_tensor(zeros), tf.int64)\n",
    "jumps_rag = tf.concat([zeros.with_row_splits_dtype(tf.int64), jumps_rag.with_row_splits_dtype(tf.int64)], axis = 1)\n",
    "print(\"\\n jumps_rag \\n\", jumps_rag[:15])\n",
    "# print(\"\\n jumps_rag \\n\", jumps_rag.to_tensor())\n",
    "\n",
    "#get num points between each jump \n",
    "npts_between_jumps = tf.experimental.numpy.diff(jumps_rag.to_tensor())\n",
    "# print(\"\\n npts_between_jumps:\\n \",npts_between_jumps[:10,:10])\n",
    "# print(\"\\n npts_between_jumps:\\n \",npts_between_jumps)\n",
    "\n",
    "#flag spikes where all npts_between_jumps are less than mnp\n",
    "biggest_jump = tf.math.reduce_max(npts_between_jumps, axis = 1)\n",
    "# print(\"\\n biggest_jump \\n\", biggest_jump)\n",
    "mnp = 100 #minumum number of points per cluster (defined in ICET class)\n",
    "good_clusters = tf.cast(tf.math.greater(biggest_jump, mnp), tf.int32)\n",
    "# good_clusters = tf.RaggedTensor.from_value_rowids(good_clusters, y).to_tensor()[:,0]  #fill in skipped indices\n",
    "print(\"\\n good_clusters (hold on to this for later) \\n\", good_clusters)\n",
    "\n",
    "#get idx within jumps_rag corresponding to first sufficiently large jump\n",
    "big_enough = tf.cast(tf.math.greater(npts_between_jumps, 100), tf.int32)\n",
    "# print(big_enough[:10])\n",
    "first_big_enough = tf.math.argmax(big_enough, axis = 1)\n",
    "print(\"\\n first_big_enough: \\n\", first_big_enough)\n",
    "# print(\"\\n first_big_enough: \\n\", first_big_enough)\n",
    "\n",
    "print(\"\\n everything looks good up to this point :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee64d08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#get inner and outer (simple way-- just use radial measurements of inner and outermost points in cluster)\n",
    "#get index of radial measurements that defines inner bounds of voxel \n",
    "inner_idx = tf.gather(jumps_rag.to_tensor(), first_big_enough, batch_dims=1) + 1\n",
    "# print(\"\\n inner_idx: \\n\", inner_idx, \"\\n\")\n",
    "inner  = tf.gather(tf.transpose(rads), inner_idx, batch_dims=1)\n",
    "# print(\"\\n inner: \\n\", inner)\n",
    "\n",
    "outer_idx = tf.gather(jumps_rag.to_tensor(), first_big_enough + 1, batch_dims=1)\n",
    "# print(\"\\n outer_idx: \\n\", outer_idx, \"\\n\")\n",
    "outer  = tf.gather(tf.transpose(rads), outer_idx, batch_dims=1)\n",
    "# print(\"\\n outer: \\n\", outer)\n",
    "\n",
    "# bounds = np.array([inner, outer]).T\n",
    "bounds = tf.concat((inner[:,None], outer[:,None]), axis = 1)\n",
    "bounds = tf.cast(good_clusters[:,None], tf.float32) * bounds\n",
    "print(bounds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a61db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get inner and outer as described in spherical paper\n",
    "#  (max half distance betweeen last in cluster and first point outside cluster)\n",
    "\n",
    "max_buffer = 3.0\n",
    "\n",
    "inner_idx = tf.gather(jumps_rag.to_tensor(), first_big_enough, batch_dims=1) # + 1 #DEBUG -- do we need +1 here??\n",
    "inner_radii  = tf.gather(tf.transpose(rads), inner_idx, batch_dims=1)\n",
    "#get radial distance of closest point on near side of cluster\n",
    "next_inner_idx = tf.gather(jumps_rag.to_tensor(), first_big_enough-1, batch_dims=1)\n",
    "next_inner_radii = tf.gather(tf.transpose(rads), next_inner_idx, batch_dims=1) \n",
    "\n",
    "# print(first_big_enough)\n",
    "# print(inner_idx[:15])\n",
    "# print(next_inner_idx[:15])\n",
    "# print(inner_radii[:15])\n",
    "# print(next_inner_radii[:15])\n",
    "# print(test[:15])\n",
    "\n",
    "#will be zero when inner idx occurs on first element of spike, otherwise correct soln\n",
    "inner_skip_dist = inner_radii - next_inner_radii\n",
    "# print(\"before: \\n\",inner_skip_dist[:15])\n",
    "#of these nonzero distances, some are smaller than max_buffer -> leave as is, all else set to max_buffer\n",
    "too_big = tf.cast(tf.math.less(inner_skip_dist*2, max_buffer), tf.float32)\n",
    "# print(too_big[:15])\n",
    "inner_skip_dist = inner_skip_dist*too_big + (1-too_big)*max_buffer\n",
    "# print(\"after: \\n\",inner_skip_dist[:15])\n",
    "temp = tf.cast(tf.math.equal(inner_skip_dist, 0), tf.float32)*max_buffer #set all others to max_buffer\n",
    "# print(temp[:15])\n",
    "# print(- inner_skip_dist - temp)\n",
    "inner = inner_radii - inner_skip_dist - temp\n",
    "\n",
    "#good up to here-----------------\n",
    "\n",
    "\n",
    "outer_idx = tf.gather(jumps_rag.to_tensor(), first_big_enough + 1, batch_dims=1) - 1\n",
    "outer_radii  = tf.gather(tf.transpose(rads), outer_idx, batch_dims=1)\n",
    "next_outer_idx = tf.gather(jumps_rag.to_tensor(), first_big_enough + 1, batch_dims=1) +1\n",
    "next_outer_radii = tf.gather(tf.transpose(rads), next_outer_idx, batch_dims=1) \n",
    "\n",
    "# print(outer_idx[:15])\n",
    "# print(next_outer_idx[:15])\n",
    "print(outer_radii[:15])\n",
    "print(next_outer_radii[:15])\n",
    "\n",
    "outer_skip_dist = next_outer_radii - outer_radii\n",
    "# print(outer_skip_dist[:15])\n",
    "too_big = tf.cast(tf.math.less(outer_skip_dist*2, max_buffer), tf.float32)\n",
    "print(too_big[:15])\n",
    "\n",
    "outer_skip_dist = outer_skip_dist*too_big + (1-too_big)*max_buffer\n",
    "print(outer_skip_dist[:15])\n",
    "\n",
    "outer = outer_radii + outer_skip_dist\n",
    "print(outer[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cd8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec14bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ebbe43f",
   "metadata": {},
   "source": [
    "# fit_gaussian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fg2(cloud, rag, npts):\n",
    "    \"\"\"new method of fitting gaussian to better handle ragged input data\"\"\"\n",
    "    numSamples = 3\n",
    "    \n",
    "    coords = tf.gather(cloud, rag)\n",
    "    mu = tf.math.reduce_mean(coords, axis = 1)[:,None]\n",
    "#     mu = tf.math.reduce_mean(coords, axis = 1) #old\n",
    "#     print(mu)\n",
    "\n",
    "#   TODO: try randomly sampling 30 points from each ragged cell, use reduced num pts to calculate covariance\n",
    "#     subsampled = tf.map_fn(sample, it.inside2) #works but SLOW\n",
    "#     subsampled = tf.map_fn(sample, it.inside2, parallel_iterations=True)\n",
    "#     subsampled = tf.gather(rag,tf.range(tf.shape(rag)[0]))[:numSamples] #wrong\n",
    "#     print(subsampled)\n",
    "\n",
    "    xpos = tf.gather(cloud[:,0], rag)\n",
    "    ypos = tf.gather(cloud[:,1], rag)\n",
    "    zpos = tf.gather(cloud[:,2], rag)\n",
    "#     c = tfp.stats.covariance(xpos.to_tensor(), ypos.to_tensor())\n",
    "\n",
    "#     print(xpos)\n",
    "    idx = tf.range(30)\n",
    "    xpos = tf.gather(xpos, idx, axis = 1)\n",
    "    ypos = tf.gather(ypos, idx, axis = 1)\n",
    "    zpos = tf.gather(zpos, idx, axis = 1)\n",
    "    print(xpos)\n",
    "\n",
    "    xx = tf.math.reduce_sum(tf.math.square(xpos - mu[:,:,0] ), axis = 1)/npts\n",
    "    yy = tf.math.reduce_sum(tf.math.square(ypos - mu[:,:,1] ), axis = 1)/npts\n",
    "    zz = tf.math.reduce_sum(tf.math.square(zpos - mu[:,:,2] ), axis = 1)/npts\n",
    "    xy = tf.math.reduce_sum( (xpos - mu[:,:,0])*(ypos - mu[:,:,1]), axis = 1)/npts  #+\n",
    "    xz = tf.math.reduce_sum( (xpos - mu[:,:,0])*(zpos - mu[:,:,2]), axis = 1)/npts #-\n",
    "    yz = tf.math.reduce_sum( (ypos - mu[:,:,1])*(zpos - mu[:,:,2]), axis = 1)/npts #-\n",
    "\n",
    "    sigma = tf.Variable([xx, xy, xz,\n",
    "                        xy, yy, yz,\n",
    "                        xz, yz, zz]) \n",
    "    sigma = tf.reshape(tf.transpose(sigma), (tf.shape(sigma)[1] ,3,3))\n",
    "        \n",
    "#     mu = None\n",
    "    return(mu, sigma)\n",
    "\n",
    "@tf.function\n",
    "def sample(x, samples=3):\n",
    "  \"\"\"https://stackoverflow.com/questions/71073873/sample-from-ragged-tensor\"\"\"  \n",
    "  length = tf.shape(x)[0]\n",
    "#   was this\n",
    "#   x = tf.cond(tf.less_equal(length, samples), lambda: x, lambda: tf.gather(x, tf.random.shuffle(tf.range(length))[:samples]))\n",
    " \n",
    "#   test\n",
    "#   x = tf.cond(tf.less_equal(length, samples), lambda: x, lambda: tf.gather(x, tf.range(length))[:samples])\n",
    "  x = tf.gather(x,tf.range(length))[:samples]\n",
    "\n",
    "    \n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "mu2, sigma2 = it.fit_gaussian(it.cloud2_tensor, it.inside2, tf.cast(it.npts2, tf.float32))\n",
    "print(\"\\n took\", time.time() - s, \" s with old method\")\n",
    "\n",
    "s = time.time()\n",
    "mu2, sigma2 = fg2(it.cloud2_tensor, it.inside2, tf.cast(it.npts2, tf.float32))\n",
    "print(\" \\n took\", time.time() - s, \" s with new method\")\n",
    "\n",
    "# print(it.npts2)\n",
    "# print(it.inside2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c39b54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# vect = it.inside2\n",
    "vect = tf.ragged.constant([[],[1,2,3,4],[5,4,3,2,1],[6],[99],[7,8,9,10,11,12,13]])\n",
    "# print(tf.shape(vect)[0])\n",
    "print(\"vect\", vect)\n",
    "c = tf.map_fn(sample, vect)\n",
    "# print(c)\n",
    "\n",
    "#wrong\n",
    "# test = tf.gather(vect,tf.range(tf.shape(vect)[0]))[:3]\n",
    "idx = tf.range(3)\n",
    "print(\"\\n idx\", idx)\n",
    "test = tf.gather(vect, idx , axis = 1)\n",
    "print(\"\\n test\", test) #NOTE: indices with too few elements produce unexpected behavior\n",
    "                        #that doesn't matter since they get suppressed anyways\n",
    "    \n",
    "vec2 = tf.random.categorical(vect, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987394e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608026c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
