{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "breathing-blair",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Derm\\anaconda3\\envs\\tf23\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Derm\\anaconda3\\envs\\tf23\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\Derm\\anaconda3\\envs\\tf23\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "#setup - rememeber to switch to tensorflow 2.3 kernel...\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import datetime\n",
    "import trimesh\n",
    "import time\n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "\n",
    "#need to have these two lines to work on my ancient 1060 3gb\n",
    "#  https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# %matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "# plt.rcParams['image.interpolation'] = 'nearest'\n",
    "# plt.rcParams['image.cmap'] = 'gray'\n",
    "%matplotlib notebook\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load OFF file from ModelNet10 dir\n",
    "start = time.time()\n",
    "fn = 'C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/toilet_0069.off'\n",
    "# fn = 'C:/Users/Derm/Desktop/big/ModelNet10/sofa/train/sofa_0370.off'\n",
    "# fn = 'C:/Users/Derm/Desktop/big/ModelNet10/bed/train/bed_0320.off'\n",
    "\n",
    "\n",
    "M = trimesh.load(fn)\n",
    "test = trimesh.sample.sample_surface(M, 100)\n",
    "print(\"took \", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Vedo to plot OG and subsampled surfaces\n",
    "plt1 = Plotter(N = 1, axes = 4, bg = (1, 1, 1), interactive = True)\n",
    "disp = []\n",
    "\n",
    "# disp.append(Points(M.vertices, c = 'blue', r = 4))\n",
    "disp.append(Points(test[0], c = 'red', r = 5))\n",
    "toilet = Mesh(M).c(\"gray\").alpha(0.2)\n",
    "disp.append(toilet)\n",
    "\n",
    "plt1.show(disp, \"surface sampling test\")\n",
    "ViewInteractiveWidget(plt1.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expected-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define rotation matrix used to transform point clouds\n",
    "def R_tf(angs):\n",
    "    if len(tf.shape(angs)) == 1:\n",
    "        angs = angs[None,:]\n",
    "    phi = angs[:,0]\n",
    "    theta = angs[:,1]\n",
    "    psi = angs[:,2]\n",
    "    mat = tf.Variable([[cos(theta)*cos(psi), sin(psi)*cos(phi) + sin(phi)*sin(theta)*cos(psi), sin(phi)*sin(psi) - sin(theta)*cos(phi)*cos(psi)],\n",
    "                       [-sin(psi)*cos(theta), cos(phi)*cos(psi) - sin(phi)*sin(theta)*sin(psi), sin(phi)*cos(psi) + sin(theta)*sin(psi)*cos(phi)],\n",
    "                       [sin(theta), -sin(phi)*cos(theta), cos(phi)*cos(theta)]\n",
    "                        ])\n",
    "    mat = tf.transpose(mat, [2, 0, 1])\n",
    "    mat = tf.squeeze(mat)\n",
    "    return mat\n",
    "\n",
    "# determine euler angles from rotation matrix\n",
    "def R2Euler(mat):\n",
    "    if len( tf.shape(mat) ) == 2:\n",
    "        mat = mat[None, :, :]\n",
    "    R_sum = np.sqrt(( mat[:,0,0]**2 + mat[:,0,1]**2 + mat[:,1,2]**2 + mat[:,2,2]**2 ) / 2)\n",
    "    phi = np.arctan2(-mat[:,1,2],mat[:,2,2])\n",
    "    theta = np.arctan2(mat[:,0,2], R_sum)\n",
    "    psi = np.arctan2(-mat[:,0,1], mat[:,0,0])\n",
    "    angs = np.array([phi, theta, psi])\n",
    "    return angs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-joshua",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate toy dataset using all of the toilets in the ModelNet10 repository\n",
    "numMeshes = 300 #344\n",
    "ptsPerCloud = 256 #was 25 in OG method \n",
    "iterPerMesh = 200 #100  #number of times to sample clouds from each mesh\n",
    "\n",
    "#init vector to store sampled point clouds\n",
    "x = np.zeros([numMeshes*iterPerMesh, ptsPerCloud*2, 3])\n",
    "#init vector to store transformations \n",
    "y = np.zeros([numMeshes*iterPerMesh, 6]) #rotation and translation\n",
    "# y = np.zeros([numMeshes*iterPerMesh, 3]) #if only considering translations\n",
    "\n",
    "#scale trans and rotation params so outputs are equally weighted\n",
    "trans_scale = 10.0 #2.0\n",
    "rot_scale = 0.2 #0.2\n",
    "\n",
    "for i in range(numMeshes):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    fn = 'C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/toilet_%04d.off' %(i+1) #loop through file names\n",
    "#     fn = 'C:/Users/Derm/Desktop/big/ModelNet10/bed/train/bed_0059.off'\n",
    "#     fn = 'C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/toilet_0069.off' #debug -> only use single toilet model\n",
    "    M = trimesh.load(fn)\n",
    "\n",
    "    #more efficient to sample all points at once and then just use some for each frame\n",
    "    sam1 = trimesh.sample.sample_surface(M, iterPerMesh*ptsPerCloud)[0] #get keyframe scan\n",
    "    sam2 = trimesh.sample.sample_surface(M, iterPerMesh*ptsPerCloud)[0] #get new scan\n",
    "#     sam2 = sam1 + 0.01*np.random.randn(np.shape(sam1)[0], 3) #copy point locations and add some noise\n",
    "    \n",
    "    for j in range(iterPerMesh):\n",
    "        #rotate keyframe\n",
    "        angs1 = 0.5*tf.random.normal([3])\n",
    "        rot1 = R_tf(angs1)\n",
    "        #rotate scan 2 relative to keyframe\n",
    "        angs2 = rot_scale*tf.random.normal([3])\n",
    "#         rot2 = R_tf(angs1 + angs2) #was this (wrong??)\n",
    "#         angs2 = tf.zeros([3]) # ~~~~~~~~~~~~~~~ zero out rotation (for debug) ~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         rot2 = tf.matmul(R_tf(angs1), R_tf(angs2)) #was this (wrong??)\n",
    "        rot2 = tf.matmul(R_tf(angs2), R_tf(angs1)) #test\n",
    "\n",
    "        \n",
    "        # randomly grow/shrink each point cloud before translation\n",
    "        scale = 1. + 0.2*tf.random.normal([1])[0]\n",
    "\n",
    "        x[i*iterPerMesh + j, :ptsPerCloud, :] = sam1[j*ptsPerCloud:(j+1)*ptsPerCloud].dot(rot1.numpy())*scale           \n",
    "            \n",
    "        trans = trans_scale*tf.random.normal([3])\n",
    "        #was this (incorrect for large angle deviation?)\n",
    "#         sam2_j = trans + sam2[j*ptsPerCloud:(j+1)*ptsPerCloud].dot(rot1.numpy()).dot(rot2.numpy())*scale \n",
    "        sam2_j = trans + sam2[j*ptsPerCloud:(j+1)*ptsPerCloud].dot(rot2.numpy())*scale \n",
    "        x[i*iterPerMesh + j, ptsPerCloud:, :] = sam2_j\n",
    "\n",
    "        #save transformation as y\n",
    "        y[i*iterPerMesh + j,:3] = trans.numpy()/trans_scale\n",
    "        y[i*iterPerMesh + j,3:] = angs2.numpy()/rot_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test sets, save to file\n",
    "split = 0.9\n",
    "x_train = x[:int(split*np.shape(x)[0])]\n",
    "# np.save('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/x_train', x_train)\n",
    "x_test = x[int(split*np.shape(x)[0]):]\n",
    "# np.save('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/x_test', x_test)\n",
    "y_train = y[:int(split*np.shape(y)[0])]\n",
    "# np.save('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/y_train', y_train)\n",
    "y_test = y[int(split*np.shape(y)[0]):]\n",
    "# np.save('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "still-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from memory\n",
    "numMeshes = 300 #344\n",
    "ptsPerCloud = 256 #was 25 in OG method \n",
    "iterPerMesh = 200 #100  #number of times to sample clouds from each mesh\n",
    "trans_scale = 10.0 #2.0\n",
    "rot_scale = 0.2 #0.2\n",
    "\n",
    "# x_train = np.load('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/x_train.npy')\n",
    "# y_train = np.load('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/y_train.npy')\n",
    "# x_test = np.load('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/x_test.npy')\n",
    "# y_test = np.load('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/y_test.npy')\n",
    "\n",
    "x_train = np.load('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/x_train_trans_only.npy')\n",
    "y_train = np.load('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/y_train_trans_only.npy')\n",
    "x_test = np.load('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/x_test_trans_only.npy')\n",
    "y_test = np.load('C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/y_test_trans_only.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chronic-centre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([54000   512     3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([54000     6], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.convert_to_tensor(x_train)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "x_test = tf.convert_to_tensor(x_test)\n",
    "y_test = tf.convert_to_tensor(y_test)\n",
    "print(tf.shape(x_train))\n",
    "print(tf.shape(y_train))\n",
    "\n",
    "# print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "after-receptor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512, 3)]          0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_ExpandDims (Tens [(None, 512, 3, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 512, 1, 64)        256       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512, 1, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 512, 1, 64)        4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512, 1, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 512, 1, 256)       16640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512, 1, 256)       1024      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorF [(None, 512, 256)]        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Transpose (Tenso [(None, 256, 2)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 63, 4)             68        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 63, 4)             16        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 63, 64)            320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 63, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 30, 2)             514       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 2)             8         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30, 64)            192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 30, 64)            256       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               491776    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowO [(None, 6)]               0         \n",
      "=================================================================\n",
      "Total params: 534,116\n",
      "Trainable params: 532,440\n",
      "Non-trainable params: 1,676\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F20CE2D3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001F20CE2D3A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      " 2/95 [..............................] - ETA: 16s - loss: 1.7954WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1264s vs `on_train_batch_end` time: 0.2347s). Check your callbacks.\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.3967WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F211598558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001F211598558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\Derm\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\Derm\\anaconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F2900E2C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F2900E2C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 37s 389ms/step - loss: 0.3967 - val_loss: 0.9501\n",
      "Epoch 2/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.1528WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A720318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A720318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 381ms/step - loss: 0.1528 - val_loss: 0.8988\n",
      "Epoch 3/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.1235WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A2175E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A2175E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 37s 386ms/step - loss: 0.1235 - val_loss: 0.7252\n",
      "Epoch 4/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.1105WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F290687168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F290687168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 378ms/step - loss: 0.1105 - val_loss: 0.4285\n",
      "Epoch 5/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.1055WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F208A09708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F208A09708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 376ms/step - loss: 0.1055 - val_loss: 0.2218\n",
      "Epoch 6/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0952WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F2906DD5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F2906DD5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 373ms/step - loss: 0.0952 - val_loss: 0.1426\n",
      "Epoch 7/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0906WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29AC021F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29AC021F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 376ms/step - loss: 0.0906 - val_loss: 0.1393\n",
      "Epoch 8/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0879WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A80A948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A80A948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 375ms/step - loss: 0.0879 - val_loss: 0.1114\n",
      "Epoch 9/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0826WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D746948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D746948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 375ms/step - loss: 0.0826 - val_loss: 0.1082\n",
      "Epoch 10/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0829WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29AB90798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29AB90798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 376ms/step - loss: 0.0829 - val_loss: 0.0880\n",
      "Epoch 11/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0800WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28DBD5B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28DBD5B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 373ms/step - loss: 0.0800 - val_loss: 0.0751\n",
      "Epoch 12/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0770 - val_loss: 0.0806\n",
      "Epoch 13/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0749WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D698D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D698D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 374ms/step - loss: 0.0749 - val_loss: 0.0724\n",
      "Epoch 14/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0738 - val_loss: 0.0876\n",
      "Epoch 15/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0726WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A984C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A984C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 376ms/step - loss: 0.0726 - val_loss: 0.0657\n",
      "Epoch 16/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0701 - val_loss: 0.0872\n",
      "Epoch 17/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0696WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F298DCF9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F298DCF9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 375ms/step - loss: 0.0696 - val_loss: 0.0647\n",
      "Epoch 18/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0679 - val_loss: 0.0651\n",
      "Epoch 19/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0677 - val_loss: 0.0653\n",
      "Epoch 20/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0672 - val_loss: 0.0706\n",
      "Epoch 21/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0652WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29074D4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29074D4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 35s 374ms/step - loss: 0.0652 - val_loss: 0.0562\n",
      "Epoch 22/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0651 - val_loss: 0.0608\n",
      "Epoch 23/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0642WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F2986105E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F2986105E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 374ms/step - loss: 0.0642 - val_loss: 0.0531\n",
      "Epoch 24/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0658WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29AAD2948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29AAD2948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 372ms/step - loss: 0.0658 - val_loss: 0.0515\n",
      "Epoch 25/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0624 - val_loss: 0.0531\n",
      "Epoch 26/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0618 - val_loss: 0.0533\n",
      "Epoch 27/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0623 - val_loss: 0.0615\n",
      "Epoch 28/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0620 - val_loss: 0.0572\n",
      "Epoch 29/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0616 - val_loss: 0.0577\n",
      "Epoch 30/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0603WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F297C11828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F297C11828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 373ms/step - loss: 0.0603 - val_loss: 0.0485\n",
      "Epoch 31/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0601 - val_loss: 0.0502\n",
      "Epoch 32/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0581WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29818B708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29818B708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 373ms/step - loss: 0.0581 - val_loss: 0.0463\n",
      "Epoch 33/300\n",
      "95/95 [==============================] - 34s 353ms/step - loss: 0.0584 - val_loss: 0.0497\n",
      "Epoch 34/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0584WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D6D4948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D6D4948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 373ms/step - loss: 0.0584 - val_loss: 0.0462\n",
      "Epoch 35/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0595 - val_loss: 0.0525\n",
      "Epoch 36/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0592WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F298223798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F298223798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 374ms/step - loss: 0.0592 - val_loss: 0.0460\n",
      "Epoch 37/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0576WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A29D318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A29D318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 374ms/step - loss: 0.0576 - val_loss: 0.0432\n",
      "Epoch 38/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0585 - val_loss: 0.0515\n",
      "Epoch 39/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0572 - val_loss: 0.0547\n",
      "Epoch 40/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0564 - val_loss: 0.0505\n",
      "Epoch 41/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0560 - val_loss: 0.0524\n",
      "Epoch 42/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0564 - val_loss: 0.0457\n",
      "Epoch 43/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0557 - val_loss: 0.0505\n",
      "Epoch 44/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0566WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F208B0E9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F208B0E9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 373ms/step - loss: 0.0566 - val_loss: 0.0420\n",
      "Epoch 45/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0559 - val_loss: 0.0575\n",
      "Epoch 46/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0572 - val_loss: 0.0493\n",
      "Epoch 47/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0550 - val_loss: 0.0504\n",
      "Epoch 48/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0558 - val_loss: 0.0488\n",
      "Epoch 49/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0554 - val_loss: 0.0449\n",
      "Epoch 50/300\n",
      "95/95 [==============================] - 34s 354ms/step - loss: 0.0549 - val_loss: 0.0477\n",
      "Epoch 51/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0549WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D68E828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D68E828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 373ms/step - loss: 0.0549 - val_loss: 0.0374\n",
      "Epoch 52/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0548 - val_loss: 0.0428\n",
      "Epoch 53/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0539 - val_loss: 0.0422\n",
      "Epoch 54/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0540 - val_loss: 0.0433\n",
      "Epoch 55/300\n",
      "95/95 [==============================] - 27s 282ms/step - loss: 0.0539 - val_loss: 0.0449\n",
      "Epoch 56/300\n",
      "95/95 [==============================] - 21s 225ms/step - loss: 0.0548 - val_loss: 0.0482\n",
      "Epoch 57/300\n",
      "95/95 [==============================] - 24s 253ms/step - loss: 0.0533 - val_loss: 0.0384\n",
      "Epoch 58/300\n",
      "95/95 [==============================] - 20s 209ms/step - loss: 0.0527 - val_loss: 0.0423\n",
      "Epoch 59/300\n",
      "95/95 [==============================] - 19s 202ms/step - loss: 0.0543 - val_loss: 0.0440\n",
      "Epoch 60/300\n",
      "95/95 [==============================] - 19s 202ms/step - loss: 0.0546 - val_loss: 0.0396\n",
      "Epoch 61/300\n",
      "95/95 [==============================] - 28s 299ms/step - loss: 0.0529 - val_loss: 0.0389\n",
      "Epoch 62/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0536 - val_loss: 0.0418\n",
      "Epoch 63/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0527WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29852B318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29852B318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 372ms/step - loss: 0.0527 - val_loss: 0.0373\n",
      "Epoch 64/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0520 - val_loss: 0.0406\n",
      "Epoch 65/300\n",
      "95/95 [==============================] - 34s 361ms/step - loss: 0.0514 - val_loss: 0.0441\n",
      "Epoch 66/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0510 - val_loss: 0.0421\n",
      "Epoch 67/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0520 - val_loss: 0.0468\n",
      "Epoch 68/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0514 - val_loss: 0.0460\n",
      "Epoch 69/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0518 - val_loss: 0.0412\n",
      "Epoch 70/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0529 - val_loss: 0.0405\n",
      "Epoch 71/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0523 - val_loss: 0.0379\n",
      "Epoch 72/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0505 - val_loss: 0.0421\n",
      "Epoch 73/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0534 - val_loss: 0.0468\n",
      "Epoch 74/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0509 - val_loss: 0.0423\n",
      "Epoch 75/300\n",
      "95/95 [==============================] - 33s 349ms/step - loss: 0.0506 - val_loss: 0.0420\n",
      "Epoch 76/300\n",
      "95/95 [==============================] - 34s 354ms/step - loss: 0.0509 - val_loss: 0.0379\n",
      "Epoch 77/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0528WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D698708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D698708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 36s 375ms/step - loss: 0.0528 - val_loss: 0.0364\n",
      "Epoch 78/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0507 - val_loss: 0.0406\n",
      "Epoch 79/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0515 - val_loss: 0.0379\n",
      "Epoch 80/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0523 - val_loss: 0.0601\n",
      "Epoch 81/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0511 - val_loss: 0.0471\n",
      "Epoch 82/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0501 - val_loss: 0.0417\n",
      "Epoch 83/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0517 - val_loss: 0.0414\n",
      "Epoch 84/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0492 - val_loss: 0.0466\n",
      "Epoch 85/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0503 - val_loss: 0.0386\n",
      "Epoch 86/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0506 - val_loss: 0.0388\n",
      "Epoch 87/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0501 - val_loss: 0.0430\n",
      "Epoch 88/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0497 - val_loss: 0.0371\n",
      "Epoch 89/300\n",
      "95/95 [==============================] - 33s 351ms/step - loss: 0.0513 - val_loss: 0.0450\n",
      "Epoch 90/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0496 - val_loss: 0.0445\n",
      "Epoch 91/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0492 - val_loss: 0.0416\n",
      "Epoch 92/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0487WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A7CB288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29A7CB288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 371ms/step - loss: 0.0487 - val_loss: 0.0339\n",
      "Epoch 93/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0495 - val_loss: 0.0383\n",
      "Epoch 94/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0497 - val_loss: 0.0404\n",
      "Epoch 95/300\n",
      "95/95 [==============================] - 33s 350ms/step - loss: 0.0491 - val_loss: 0.0341\n",
      "Epoch 96/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0495 - val_loss: 0.0381\n",
      "Epoch 97/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0492 - val_loss: 0.0390\n",
      "Epoch 98/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0497 - val_loss: 0.0406\n",
      "Epoch 99/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0475 - val_loss: 0.0397\n",
      "Epoch 100/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0489 - val_loss: 0.0411\n",
      "Epoch 101/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0490 - val_loss: 0.0386\n",
      "Epoch 102/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0485 - val_loss: 0.0357\n",
      "Epoch 103/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0489 - val_loss: 0.0363\n",
      "Epoch 104/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0476 - val_loss: 0.0414\n",
      "Epoch 105/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0489 - val_loss: 0.0445\n",
      "Epoch 106/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0470 - val_loss: 0.0410\n",
      "Epoch 107/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0490 - val_loss: 0.0416\n",
      "Epoch 108/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0479 - val_loss: 0.0473\n",
      "Epoch 109/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0487 - val_loss: 0.0353\n",
      "Epoch 110/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0485 - val_loss: 0.0488\n",
      "Epoch 111/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0488 - val_loss: 0.0345\n",
      "Epoch 112/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0479 - val_loss: 0.0399\n",
      "Epoch 113/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0492 - val_loss: 0.0369\n",
      "Epoch 114/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0470 - val_loss: 0.0370\n",
      "Epoch 115/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0491 - val_loss: 0.0454\n",
      "Epoch 116/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0468 - val_loss: 0.0366\n",
      "Epoch 117/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0472 - val_loss: 0.0345\n",
      "Epoch 118/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0477WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29850AEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F29850AEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 370ms/step - loss: 0.0477 - val_loss: 0.0323\n",
      "Epoch 119/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0483 - val_loss: 0.0388\n",
      "Epoch 120/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0468 - val_loss: 0.0366\n",
      "Epoch 121/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0472 - val_loss: 0.0445\n",
      "Epoch 122/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0467 - val_loss: 0.0396\n",
      "Epoch 123/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0470 - val_loss: 0.0426\n",
      "Epoch 124/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0503 - val_loss: 0.0533\n",
      "Epoch 125/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0483 - val_loss: 0.0394\n",
      "Epoch 126/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0483 - val_loss: 0.0435\n",
      "Epoch 127/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0466 - val_loss: 0.0446\n",
      "Epoch 128/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0463 - val_loss: 0.0452\n",
      "Epoch 129/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0468 - val_loss: 0.0468\n",
      "Epoch 130/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0478 - val_loss: 0.0345\n",
      "Epoch 131/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0469 - val_loss: 0.0429\n",
      "Epoch 132/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0473 - val_loss: 0.0373\n",
      "Epoch 133/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0484 - val_loss: 0.0414\n",
      "Epoch 134/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0476 - val_loss: 0.0502\n",
      "Epoch 135/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0477 - val_loss: 0.0446\n",
      "Epoch 136/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0469 - val_loss: 0.0379\n",
      "Epoch 137/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0475 - val_loss: 0.0351\n",
      "Epoch 138/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0466 - val_loss: 0.0453\n",
      "Epoch 139/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0479 - val_loss: 0.0392\n",
      "Epoch 140/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0474 - val_loss: 0.0372\n",
      "Epoch 141/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0458 - val_loss: 0.0471\n",
      "Epoch 142/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0454 - val_loss: 0.0403\n",
      "Epoch 143/300\n",
      "95/95 [==============================] - 33s 352ms/step - loss: 0.0462 - val_loss: 0.0380\n",
      "Epoch 144/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0452 - val_loss: 0.0379\n",
      "Epoch 145/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0473 - val_loss: 0.0723\n",
      "Epoch 146/300\n",
      "95/95 [==============================] - 33s 343ms/step - loss: 0.0488 - val_loss: 0.0398\n",
      "Epoch 147/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0463 - val_loss: 0.0349\n",
      "Epoch 148/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0458 - val_loss: 0.0402\n",
      "Epoch 149/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0454 - val_loss: 0.0425\n",
      "Epoch 150/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0458 - val_loss: 0.0484\n",
      "Epoch 151/300\n",
      "95/95 [==============================] - 33s 343ms/step - loss: 0.0454 - val_loss: 0.0476\n",
      "Epoch 152/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0455 - val_loss: 0.0448\n",
      "Epoch 153/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0463 - val_loss: 0.0400\n",
      "Epoch 154/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0452 - val_loss: 0.0467\n",
      "Epoch 155/300\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.0448WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D6CAD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F28D6CAD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: DermNet_ModelNet_benchmark_cp.kmod\\assets\n",
      "95/95 [==============================] - 35s 369ms/step - loss: 0.0448 - val_loss: 0.0308\n",
      "Epoch 156/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0445 - val_loss: 0.0396\n",
      "Epoch 157/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0446 - val_loss: 0.0358\n",
      "Epoch 158/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0461 - val_loss: 0.0419\n",
      "Epoch 159/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0440 - val_loss: 0.0424\n",
      "Epoch 160/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0450 - val_loss: 0.0373\n",
      "Epoch 161/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0440 - val_loss: 0.0374\n",
      "Epoch 162/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0451 - val_loss: 0.0316\n",
      "Epoch 163/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0457 - val_loss: 0.0374\n",
      "Epoch 164/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0453 - val_loss: 0.0381\n",
      "Epoch 165/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0459 - val_loss: 0.0424\n",
      "Epoch 166/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0450 - val_loss: 0.0421\n",
      "Epoch 167/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0442 - val_loss: 0.0369\n",
      "Epoch 168/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0446 - val_loss: 0.0490\n",
      "Epoch 169/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0439 - val_loss: 0.0360\n",
      "Epoch 170/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0441 - val_loss: 0.0322\n",
      "Epoch 171/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0457 - val_loss: 0.0562\n",
      "Epoch 172/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0451 - val_loss: 0.0437\n",
      "Epoch 173/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0450 - val_loss: 0.0381\n",
      "Epoch 174/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0446 - val_loss: 0.0416\n",
      "Epoch 175/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0450 - val_loss: 0.0427\n",
      "Epoch 176/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0448 - val_loss: 0.0327\n",
      "Epoch 177/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0427 - val_loss: 0.0404\n",
      "Epoch 178/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0446 - val_loss: 0.0519\n",
      "Epoch 179/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0440 - val_loss: 0.0517\n",
      "Epoch 180/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0447 - val_loss: 0.0355\n",
      "Epoch 181/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0443 - val_loss: 0.0386\n",
      "Epoch 182/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0444 - val_loss: 0.0374\n",
      "Epoch 183/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0446 - val_loss: 0.0438\n",
      "Epoch 184/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0435 - val_loss: 0.0379\n",
      "Epoch 185/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0466 - val_loss: 0.0356\n",
      "Epoch 186/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0447 - val_loss: 0.0375\n",
      "Epoch 187/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0471 - val_loss: 0.0341\n",
      "Epoch 188/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0427 - val_loss: 0.0450\n",
      "Epoch 189/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0428 - val_loss: 0.0397\n",
      "Epoch 190/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0438 - val_loss: 0.0413\n",
      "Epoch 191/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0439 - val_loss: 0.0357\n",
      "Epoch 192/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0431 - val_loss: 0.0341\n",
      "Epoch 193/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0434 - val_loss: 0.0413\n",
      "Epoch 194/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0430 - val_loss: 0.0465\n",
      "Epoch 195/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0421 - val_loss: 0.0373\n",
      "Epoch 196/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0430 - val_loss: 0.0416\n",
      "Epoch 197/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0423 - val_loss: 0.0438\n",
      "Epoch 198/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 33s 352ms/step - loss: 0.0439 - val_loss: 0.0431\n",
      "Epoch 199/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0478 - val_loss: 0.1418\n",
      "Epoch 200/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0512 - val_loss: 0.0758\n",
      "Epoch 201/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0455 - val_loss: 0.0503\n",
      "Epoch 202/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0461 - val_loss: 0.0499\n",
      "Epoch 203/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0461 - val_loss: 0.0372\n",
      "Epoch 204/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0444 - val_loss: 0.0470\n",
      "Epoch 205/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0440 - val_loss: 0.0362\n",
      "Epoch 206/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0440 - val_loss: 0.0374\n",
      "Epoch 207/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0430 - val_loss: 0.0373\n",
      "Epoch 208/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0434 - val_loss: 0.0404\n",
      "Epoch 209/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0443 - val_loss: 0.0435\n",
      "Epoch 210/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0425 - val_loss: 0.0357\n",
      "Epoch 211/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0440 - val_loss: 0.0356\n",
      "Epoch 212/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0440 - val_loss: 0.0388\n",
      "Epoch 213/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0429 - val_loss: 0.0365\n",
      "Epoch 214/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0418 - val_loss: 0.0380\n",
      "Epoch 215/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0428 - val_loss: 0.0370\n",
      "Epoch 216/300\n",
      "95/95 [==============================] - 33s 343ms/step - loss: 0.0427 - val_loss: 0.0397\n",
      "Epoch 217/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0416 - val_loss: 0.0373\n",
      "Epoch 218/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0429 - val_loss: 0.0391\n",
      "Epoch 219/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0431 - val_loss: 0.0439\n",
      "Epoch 220/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0439 - val_loss: 0.0469\n",
      "Epoch 221/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0431 - val_loss: 0.0462\n",
      "Epoch 222/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0431 - val_loss: 0.0413\n",
      "Epoch 223/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0416 - val_loss: 0.0410\n",
      "Epoch 224/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0422 - val_loss: 0.0369\n",
      "Epoch 225/300\n",
      "95/95 [==============================] - 33s 343ms/step - loss: 0.0430 - val_loss: 0.0435\n",
      "Epoch 226/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0420 - val_loss: 0.0424\n",
      "Epoch 227/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0409 - val_loss: 0.0371\n",
      "Epoch 228/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0423 - val_loss: 0.0557\n",
      "Epoch 229/300\n",
      "95/95 [==============================] - 33s 352ms/step - loss: 0.0415 - val_loss: 0.0355\n",
      "Epoch 230/300\n",
      "95/95 [==============================] - 33s 350ms/step - loss: 0.0425 - val_loss: 0.0387\n",
      "Epoch 231/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0419 - val_loss: 0.0404\n",
      "Epoch 232/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0408 - val_loss: 0.0335\n",
      "Epoch 233/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0418 - val_loss: 0.0365\n",
      "Epoch 234/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0433 - val_loss: 0.0651\n",
      "Epoch 235/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0444 - val_loss: 0.0460\n",
      "Epoch 236/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0409 - val_loss: 0.0432\n",
      "Epoch 237/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0417 - val_loss: 0.0381\n",
      "Epoch 238/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0417 - val_loss: 0.0408\n",
      "Epoch 239/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0409 - val_loss: 0.0400\n",
      "Epoch 240/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0436 - val_loss: 0.0392\n",
      "Epoch 241/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0408 - val_loss: 0.0496\n",
      "Epoch 242/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0409 - val_loss: 0.0432\n",
      "Epoch 243/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0426 - val_loss: 0.0471\n",
      "Epoch 244/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0435 - val_loss: 0.0601\n",
      "Epoch 245/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0412 - val_loss: 0.0382\n",
      "Epoch 246/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0417 - val_loss: 0.0369\n",
      "Epoch 247/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0410 - val_loss: 0.0389\n",
      "Epoch 248/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0416 - val_loss: 0.0363\n",
      "Epoch 249/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0413 - val_loss: 0.0347\n",
      "Epoch 250/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0417 - val_loss: 0.0402\n",
      "Epoch 251/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0420 - val_loss: 0.0450\n",
      "Epoch 252/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0417 - val_loss: 0.0392\n",
      "Epoch 253/300\n",
      "95/95 [==============================] - 33s 348ms/step - loss: 0.0401 - val_loss: 0.0381\n",
      "Epoch 254/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0417 - val_loss: 0.0403\n",
      "Epoch 255/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0425 - val_loss: 0.0350\n",
      "Epoch 256/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0421 - val_loss: 0.0411\n",
      "Epoch 257/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0417 - val_loss: 0.0389\n",
      "Epoch 258/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0404 - val_loss: 0.0348\n",
      "Epoch 259/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0420 - val_loss: 0.0360\n",
      "Epoch 260/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0411 - val_loss: 0.0416\n",
      "Epoch 261/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0407 - val_loss: 0.0379\n",
      "Epoch 262/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0419 - val_loss: 0.0381\n",
      "Epoch 263/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0413 - val_loss: 0.0399\n",
      "Epoch 264/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0405 - val_loss: 0.0367\n",
      "Epoch 265/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0419 - val_loss: 0.0382\n",
      "Epoch 266/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0411 - val_loss: 0.0471\n",
      "Epoch 267/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0398 - val_loss: 0.0457\n",
      "Epoch 268/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0416 - val_loss: 0.0359\n",
      "Epoch 269/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0405 - val_loss: 0.0463\n",
      "Epoch 270/300\n",
      "95/95 [==============================] - 33s 347ms/step - loss: 0.0413 - val_loss: 0.0364\n",
      "Epoch 271/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0414 - val_loss: 0.0500\n",
      "Epoch 272/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0457 - val_loss: 0.0362\n",
      "Epoch 273/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0430 - val_loss: 0.0488\n",
      "Epoch 274/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0430 - val_loss: 0.0390\n",
      "Epoch 275/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0409 - val_loss: 0.0345\n",
      "Epoch 276/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0412 - val_loss: 0.0382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0408 - val_loss: 0.0427\n",
      "Epoch 278/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0415 - val_loss: 0.0351\n",
      "Epoch 279/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0410 - val_loss: 0.0433\n",
      "Epoch 280/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0480 - val_loss: 0.0546\n",
      "Epoch 281/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0454 - val_loss: 0.0431\n",
      "Epoch 282/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0437 - val_loss: 0.0620\n",
      "Epoch 283/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0436 - val_loss: 0.0346\n",
      "Epoch 284/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0427 - val_loss: 0.0412\n",
      "Epoch 285/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0419 - val_loss: 0.0454\n",
      "Epoch 286/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0419 - val_loss: 0.0380\n",
      "Epoch 287/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0411 - val_loss: 0.0355\n",
      "Epoch 288/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0415 - val_loss: 0.0510\n",
      "Epoch 289/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0408 - val_loss: 0.0345\n",
      "Epoch 290/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0403 - val_loss: 0.0437\n",
      "Epoch 291/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0411 - val_loss: 0.0374\n",
      "Epoch 292/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0422 - val_loss: 0.0386\n",
      "Epoch 293/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0405 - val_loss: 0.0405\n",
      "Epoch 294/300\n",
      "95/95 [==============================] - 33s 344ms/step - loss: 0.0426 - val_loss: 0.0443\n",
      "Epoch 295/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0414 - val_loss: 0.0406\n",
      "Epoch 296/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 297/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0415 - val_loss: 0.0352\n",
      "Epoch 298/300\n",
      "95/95 [==============================] - 33s 346ms/step - loss: 0.0404 - val_loss: 0.0343\n",
      "Epoch 299/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0408 - val_loss: 0.0409\n",
      "Epoch 300/300\n",
      "95/95 [==============================] - 33s 345ms/step - loss: 0.0406 - val_loss: 0.0386\n"
     ]
    }
   ],
   "source": [
    "#train network\n",
    "from network import Net #mine\n",
    "# from pcrnet import Network as Net #PCR-Net baseline\n",
    "np.random.seed(1337)\n",
    "runLen =  300 #30\n",
    "\n",
    "def scheduler(epoch, learning_rate):\n",
    "    part1 = runLen//3\n",
    "    part2 = 2*runLen//3 #net2\n",
    "    if epoch < part1:\n",
    "        learning_rate = 0.01\n",
    "        return learning_rate\n",
    "    if epoch >= part1 and epoch < part2:\n",
    "        learning_rate = 0.005 #0.001\n",
    "        return learning_rate\n",
    "    if epoch >= part2:\n",
    "        learning_rate = 0.00025 #0.00025\n",
    "        return learning_rate\n",
    "\n",
    "model = Net()\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "              loss = tf.keras.losses.MeanAbsoluteError()) #was MeanAbsoluteError()\n",
    "\n",
    "summary = model.summary()\n",
    "print(summary)\n",
    "scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(\"DermNet_ModelNet_benchmark_cp.kmod\", monitor = 'val_loss', save_best_only = True) \n",
    "\n",
    "log_dir = \"runs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "trace = model.fit(x = x_train, y = y_train, batch_size = 512, epochs=runLen, verbose=1, \n",
    "                  validation_split = 0.1, shuffle=True, callbacks = [cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "minute-police",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox  6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_device_pixel_ratio', {\n",
       "                device_pixel_ratio: fig.ratio,\n",
       "            });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'dblclick',\n",
       "        on_mouse_event_closure('dblclick')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    fig.rubberband_canvas.style.cursor = msg['cursor'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            var img = evt.data;\n",
       "            if (img.type !== 'image/png') {\n",
       "                /* FIXME: We get \"Resource interpreted as Image but\n",
       "                 * transferred with MIME type text/plain:\" errors on\n",
       "                 * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "                 * to be part of the websocket stream */\n",
       "                img.type = 'image/png';\n",
       "            }\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                img\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from https://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * https://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.key === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.key;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.key !== 'Control') {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    else if (event.altKey && event.key !== 'Alt') {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    else if (event.shiftKey && event.key !== 'Shift') {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k' + event.key;\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.binaryType = comm.kernel.ws.binaryType;\n",
       "    ws.readyState = comm.kernel.ws.readyState;\n",
       "    function updateReadyState(_event) {\n",
       "        if (comm.kernel.ws) {\n",
       "            ws.readyState = comm.kernel.ws.readyState;\n",
       "        } else {\n",
       "            ws.readyState = 3; // Closed state.\n",
       "        }\n",
       "    }\n",
       "    comm.kernel.ws.addEventListener('open', updateReadyState);\n",
       "    comm.kernel.ws.addEventListener('close', updateReadyState);\n",
       "    comm.kernel.ws.addEventListener('error', updateReadyState);\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        var data = msg['content']['data'];\n",
       "        if (data['blob'] !== undefined) {\n",
       "            data = {\n",
       "                data: new Blob(msg['buffers'], { type: data['blob'] }),\n",
       "            };\n",
       "        }\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(data);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Mean Absolute Error')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Network Loss Plots\n",
    "from matplotlib import pyplot as plt\n",
    "fig0, ax0 = plt.subplots()\n",
    "ax0.plot(trace.history['loss'], '-')\n",
    "ax0.plot(trace.history['val_loss'], '-')\n",
    "ax0.legend(['train', 'val'], loc='upper left')\n",
    "ax0.set_xlabel('iteration')\n",
    "ax0.set_ylabel('Mean Absolute Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "wrapped-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.82179523 3.2741554  1.68902776], shape=(3,), dtype=float64)\n",
      "tf.Tensor([3.14381527 3.5712673  3.54666586], shape=(3,), dtype=float64)\n",
      "tf.Tensor([-1.32202004 -0.29711189 -1.8576381 ], shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "#temp- get center location of each point cloud to make sure net isn't just matching centroids\n",
    "idx = int(np.floor(100*np.random.randn()))\n",
    "mu1 = tf.math.reduce_mean(x_train[idx, :ptsPerCloud, :], axis = 0)\n",
    "mu2 = tf.math.reduce_mean(x_train[idx, ptsPerCloud:, :], axis = 0) - (y_train[idx,:3]*trans_scale)\n",
    "print(mu1)\n",
    "print(mu2)\n",
    "center_error = mu1 - mu2\n",
    "print(center_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "identical-forum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001F28D7389D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001F28D7389D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "#look at errors at never-before-seen test data generated from similar objects in ModelNet10\n",
    "guess = model.predict(x_train[:4])\n",
    "error = y_train[:4] - guess\n",
    "# print(guess)\n",
    "# print(y_train[:4])\n",
    "# print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "paperback-component",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  1 512   3], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#Generate special test data for this visualization (evenly sampled)\n",
    "#  (doing this so we can draw the underlying model from which points were sampled)\n",
    "fn = 'C:/Users/Derm/Desktop/big/ModelNet10/toilet/train/toilet_0310.off' #0310 looks best\n",
    "# fn = 'C:/Users/Derm/Desktop/big/ModelNet10/bed/train/bed_0199.off' \n",
    "\n",
    "M = trimesh.load(fn)\n",
    "\n",
    "n_tests = 1 #number of test samples to generate\n",
    "#init vector to store sampled point clouds\n",
    "x_test2 = np.zeros([n_tests, ptsPerCloud*2, 3])\n",
    "#init vector to store transformations \n",
    "y_test2 = np.zeros([n_tests, 6]) #rotation and translation\n",
    "\n",
    "sam1 = trimesh.sample.sample_surface(M, n_tests*ptsPerCloud)[0] #get keyframe scan\n",
    "sam2 = trimesh.sample.sample_surface(M, n_tests*ptsPerCloud)[0] #get new scan\n",
    "\n",
    "for j in range(n_tests):\n",
    "    angs1 = 0.5*tf.random.normal([3])    #rotate keyframe\n",
    "    rot1 = R_tf(angs1)\n",
    "    angs2 = rot_scale*tf.random.normal([3])     #rotate scan 2 relative to keyframe\n",
    "    angs2 = tf.zeros([3]) # ~~~~~~~~~~~~~~~ zero out rotation (for debug) ~~~~~~~~~~~~~~~~~~~~~~\n",
    "    rot2 = R_tf(angs2)\n",
    "    #     rot_combined = R_tf(angs1 + angs2) #was this\n",
    "    rot_combined = tf.matmul(R_tf(angs1), R_tf(angs2))\n",
    "    \n",
    "    x_test2[j, :ptsPerCloud, :] = sam1[j*ptsPerCloud:(j+1)*ptsPerCloud].dot(rot1.numpy())         \n",
    "\n",
    "    trans = trans_scale*tf.random.normal([3])\n",
    "    #was this\n",
    "    sam2_j = trans + sam2[j*ptsPerCloud:(j+1)*ptsPerCloud].dot(rot_combined.numpy()) #transform scan\n",
    "    #DEBUG\n",
    "#     sam2_j = (sam2[j*ptsPerCloud:(j+1)*ptsPerCloud]+trans.numpy()).dot(rot_combined.numpy()) #transform scan\n",
    "    x_test2[j, ptsPerCloud:, :] = sam2_j\n",
    "\n",
    "    #save transformation as y\n",
    "    y_test2[j,:3] = trans.numpy()/trans_scale\n",
    "    y_test2[j,3:] = angs2.numpy()/rot_scale\n",
    "print(tf.shape(x_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "normal-plastic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001F28DBD5558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001F28DBD5558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      " ground truth: [-6.01884902 -0.7251104  -2.98458666  0.          0.          0.        ]\n",
      "\n",
      " estimate from DNN after 1 iteration: [-6.0259414e+00 -6.8264991e-01 -3.3393970e+00 -3.1106273e-04\n",
      " -2.0551895e-04 -3.4484544e-04]\n",
      "------- \n",
      " Final Error: [-0.17270672 -0.12660234  0.46092492 -0.0007067  -0.00049253 -0.00055989]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c106201d7dd4995ae31e2595301ed56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=568, layout=Layout(height='auto', width='100%'), width=1706)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize network performance on evenly sampled data\n",
    "t = 0 #test number to draw\n",
    "niter = 5 #number of iterations to run network for\n",
    "\n",
    "plt2 = Plotter(N = 3, axes = 4, bg = (1, 1, 1), interactive = True)\n",
    "disp1 = [] #before estimated transformation (drawn on left)\n",
    "disp2 = [] #after 1 transformation (drawn in center)\n",
    "disp3 = [] #after niter transformations\n",
    "\n",
    "#draw first viz (untransformed set of scans)-------------------\n",
    "scan1 = Mesh(M).c(\"red\").alpha(0.1)#.rotate(90, axis = (0,0,1))\n",
    "scan1.applyTransform(rot1.numpy().T)\n",
    "disp1.append(scan1)\n",
    "disp1.append(Points(x_test2[0,:ptsPerCloud], c = 'red', r = 5))\n",
    "\n",
    "scan2 = Mesh(M).c(\"blue\").alpha(0.1)\n",
    "scan2.applyTransform(rot_combined.numpy().T)\n",
    "# scan2.pos(y_test2[t,0], y_test2[t,1], y_test2[t,2])\n",
    "scan2.pos(y_test2[t,0]*trans_scale, y_test2[t,1]*trans_scale, y_test2[t,2]*trans_scale)\n",
    "disp1.append(scan2)\n",
    "disp1.append(Points(x_test2[0,ptsPerCloud:], c = 'blue', r = 5))\n",
    "\n",
    "#FOR DEBUG - draw ground truth transformation in green so I can be sure which order is correct\n",
    "# correct = (x_test2[0,ptsPerCloud:] - y_test2[0,:3]*trans_scale).dot(R_tf(y_test2[0,3:]*rot_scale).numpy().T)\n",
    "# temp = Points(correct, c = 'green', r = 5)\n",
    "# disp1.append(temp)\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "#draw esatimated soln after 1 iteration ------------------------\n",
    "ans_cum = model.predict(x_test2)[t]\n",
    "ans_cum[:3] = ans_cum[:3]*trans_scale\n",
    "ans_cum[3:] = ans_cum[3:]*rot_scale\n",
    "\n",
    "#draw meshes\n",
    "soln_est_rot = R_tf(ans_cum[3:])\n",
    "scan2_transformed = Mesh(M).c(\"blue\").alpha(0.1)\n",
    "scan2_transformed.applyTransform(soln_est_rot.numpy().dot(rot_combined.numpy().T))\n",
    "scan2_transformed.pos(y_test2[t,0]*trans_scale - ans_cum[0], \n",
    "                      y_test2[t,1]*trans_scale - ans_cum[1], \n",
    "                      y_test2[t,2]*trans_scale - ans_cum[2])\n",
    "disp2.append(scan2_transformed)\n",
    "disp2.append(Mesh(M).c(\"red\").alpha(0.1).applyTransform(rot1.numpy().T)) #draw keyframe\n",
    "\n",
    "#add points\n",
    "scan2_pts_transformed = (x_test2[0,ptsPerCloud:] - ans_cum[:3]).dot(soln_est_rot.numpy().T)\n",
    "# scan2_pts_transformed = (x_test2[0,ptsPerCloud:]).dot(soln_est_rot.numpy().T) - ans_cum[:3]\n",
    "disp2.append(Points(scan2_pts_transformed, c = 'blue', r = 5))\n",
    "\n",
    "disp2.append(Points(x_test2[0,:ptsPerCloud], c = 'red', r = 5))\n",
    "gt = y_test2[t].copy()\n",
    "gt[:3] = gt[:3]*trans_scale\n",
    "gt[3:] = gt[3:]*rot_scale\n",
    "print(\"\\n ground truth:\", gt)\n",
    "print(\"\\n estimate from DNN after 1 iteration:\", ans_cum)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# draw estiamted soln after n interations-------------------------\n",
    "#TODO: need to figure out more compat way of representing sequential 6DOF transforms\n",
    "for i in range(niter):\n",
    "    #replace initial scan2 with transformed pc2 as input to network\n",
    "    inlayer = tf.concat([x_test2[0][:ptsPerCloud], scan2_pts_transformed], axis = 0)[None, :, :]\n",
    "    ans_i = model.predict(inlayer)[0]\n",
    "    ans_i[:3] = ans_i[:3]*trans_scale\n",
    "    ans_i[3:] = ans_i[3:]*rot_scale\n",
    "    \n",
    "#     soln_est_rot = tf.matmul(R_tf(ans_i[3:]), soln_est_rot)\n",
    "    soln_est_rot = R_tf(ans_i[3:]) #test\n",
    "    ans_cum[:3] = ans_cum[:3] + ans_i[:3]\n",
    "#     ans_cum[3:] = R2Euler(soln_est_rot)[:,0]\n",
    "    ans_cum[3:] = R2Euler(tf.matmul(R_tf(ans_i[3:]), soln_est_rot))[:,0]\n",
    "    scan2_pts_transformed = (x_test2[0,ptsPerCloud:] - ans_cum[:3]).dot(soln_est_rot.numpy().T)\n",
    "    \n",
    "# print(\"\\n estimate from DNN after\", niter, \"iterations: \", ans_cum) \n",
    "\n",
    "scan2_transformed_again = Mesh(M).c(\"blue\").alpha(0.1)\n",
    "scan2_transformed_again.applyTransform(soln_est_rot.numpy().dot(rot_combined.numpy().T)) #test\n",
    "scan2_transformed_again.pos(y_test2[t,0]*trans_scale - ans_cum[0], \n",
    "                            y_test2[t,1]*trans_scale - ans_cum[1], \n",
    "                            y_test2[t,2]*trans_scale - ans_cum[2])\n",
    "disp3.append(scan2_transformed_again)\n",
    "disp3.append(Points(scan2_pts_transformed, c = 'blue', r = 5))\n",
    "disp3.append(Points(x_test2[0,:ptsPerCloud], c = 'red', r = 5))\n",
    "disp3.append(Mesh(M).c(\"red\").alpha(0.1).applyTransform(rot1.numpy().T)) #keyframe\n",
    "\n",
    "print(\"------- \\n Final Error:\", gt - ans_cum)\n",
    "# #---------------------------------------------------------------\n",
    "\n",
    "    \n",
    "plt2.show(disp1, \"initial transformation\", at = 0)\n",
    "plt2.show(disp2, \"after 1 iteration\", at = 1)\n",
    "plt2.show(disp3, \"after 5 iterations\", at = 2)\n",
    "ViewInteractiveWidget(plt2.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(x_test2[:ptsPerCloud], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "composed-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"DermNet_ModelNet_benchmark.kmod\") #256 pts per cloud, MAE =~ 0.3177\n",
    "# model.save(\"DermNet_ModelNet_benchmark.h5\") #allows viz with Netron\n",
    "# model = tf.keras.models.load_model(\"DermNet_ModelNet_benchmark.kmod\")\n",
    "\n",
    "# model.save(\"DermNet_ModelNet_trans_only.kmod\") #256 pts per cloud, MAE = 0.0308\n",
    "model = tf.keras.models.load_model(\"DermNet_ModelNet_trans_only.kmod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can't just add euler angles in 3D...\n",
    "a = tf.constant([[1., 2., 3.]])\n",
    "A = R_tf(a)\n",
    "b = tf.constant([[0.3, 0.2, 0.1]])\n",
    "B = R_tf(b)\n",
    "print(tf.matmul(A, B))\n",
    "print(R_tf(a + b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.numpy().T)\n",
    "print(np.linalg.pinv(A.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(R_tf(tf.constant([0.,0.,0.])), R_tf(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-smoke",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
