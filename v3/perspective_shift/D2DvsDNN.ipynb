{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rental-ordinary",
   "metadata": {},
   "source": [
    "### Monte-Carlo sims for comparing D2D and DNN solution vectors\n",
    "\n",
    "\n",
    "\n",
    "#### TODO:\n",
    "\n",
    "\n",
    "Bring cartesian representation of corners back to training data generation file\n",
    "\n",
    "Break test data into \"easy, medium, and hard\" scans with varying degrees of perspective shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacterial-nerve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 22:45:47.839187: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-27 22:45:47.926268: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-27 22:45:48.248672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-27 22:45:48.248705: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-27 22:45:48.248710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 22:45:49.067560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 22:45:49.091278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 22:45:49.091422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import datetime\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "print(tf.__version__) #requires tensorflow 2.3\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "utility-burst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12260, 100, 3)\n",
      "(12260, 100, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 22:50:31.276640: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-27 22:50:31.277236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 22:50:31.277374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 22:50:31.277467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 22:50:31.572427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 22:50:31.572561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 22:50:31.572664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-27 22:50:31.572745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "#load test data\n",
    "d1 = np.load(\"training_data/compact_scan1.npy\")\n",
    "d2 = np.load(\"training_data/compact_scan2.npy\")\n",
    "gt = np.load(\"training_data/ground_truth.npy\")\n",
    "cgt = np.load(\"training_data/compact_ground_truth.npy\")\n",
    "LUT = np.load(\"training_data/LUT.npy\")\n",
    "L = np.load(\"training_data/L.npy\")\n",
    "U = np.load(\"training_data/U.npy\")\n",
    "corn = np.load(\"training_data/corn.npy\")\n",
    "\n",
    "#reshape but don't convert to tensor\n",
    "points_per_sample = 50          #poitns sammpled from each voxel\n",
    "tsplit = 0.5 #0.95                   #this fraction goes into training\n",
    "\n",
    "scan1 = np.reshape(d1, [-1, points_per_sample, 3])\n",
    "scan2 = np.reshape(d2, [-1, points_per_sample, 3])\n",
    "ntrain = int(tsplit*tf.shape(scan1)[0].numpy())\n",
    "\n",
    "x_train = np.append(scan1[:ntrain], scan2[:ntrain], axis = 1)\n",
    "x_test = np.append(scan1[ntrain:], scan2[ntrain:], axis = 1)\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(x_test))\n",
    "\n",
    "y_train = gt[:ntrain] #for standard training/ test data\n",
    "y_test = gt[ntrain:]\n",
    "\n",
    "# y_train = gt[:ntrain][:,:,0] #when using compact data\n",
    "# y_test = gt[ntrain:][:,:,0]\n",
    "LUT = tf.convert_to_tensor(LUT)[ntrain:]\n",
    "U = tf.convert_to_tensor(U)[ntrain:]\n",
    "L = tf.convert_to_tensor(L)[ntrain:]\n",
    "corn_train = corn[:ntrain]\n",
    "corn_test = corn[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "significant-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"KITTInet.kmod\")\n",
    "# model = tf.keras.models.load_model(\"CP.kmod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "floppy-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN = model.predict(x_test[:10])\n",
    "# print(DNN)\n",
    "# print(tf.shape(y_test))\n",
    "# print(tf.shape(x_test))\n",
    "# print(tf.shape(LUT))\n",
    "# print(tf.shape(U))\n",
    "# print(tf.shape(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "protective-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions to convert between spherical and cartesian coordinate representations\n",
    "def c2s(pts):\n",
    "    \"\"\" converts points from cartesian coordinates to spherical coordinates \"\"\"\n",
    "    r = tf.sqrt(pts[:,0]**2 + pts[:,1]**2 + pts[:,2]**2)\n",
    "    phi = tf.math.acos(pts[:,2]/r)\n",
    "    theta = tf.math.atan2(pts[:,1], pts[:,0])\n",
    "\n",
    "    out = tf.transpose(tf.Variable([r, theta, phi]))\n",
    "    return(out)\n",
    "def s2c(pts):\n",
    "    \"\"\"converts spherical -> cartesian\"\"\"\n",
    "\n",
    "    x = pts[:,0]*tf.math.sin(pts[:,2])*tf.math.cos(pts[:,1])\n",
    "    y = pts[:,0]*tf.math.sin(pts[:,2])*tf.math.sin(pts[:,1]) \n",
    "    z = pts[:,0]*tf.math.cos(pts[:,2])\n",
    "\n",
    "    out = tf.transpose(tf.Variable([x, y, z]))\n",
    "    # out = tf.Variable([x, y, z])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-burst",
   "metadata": {},
   "source": [
    "### Iterative solution for single test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "quiet-philip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "\n",
      " correct soln [1.6382637  0.47738773 1.5255338 ]\n",
      "\n",
      " estiamted soln: [[1.67729303 0.46707976 1.27829373]]\n",
      "\n",
      " error from DNN: [[-0.03902933  0.01030796  0.24724007]]\n",
      "\n",
      " error in means [-0.02254307  0.02739984  0.02640271]\n"
     ]
    }
   ],
   "source": [
    "n = int(np.floor(500*np.random.rand()))\n",
    "# n = 0\n",
    "print(n)\n",
    "\n",
    "c1 = np.array([x_test[n,:points_per_sample,0], x_test[n,:points_per_sample,1], x_test[n,:points_per_sample,2]])\n",
    "c2 = np.array([x_test[n,points_per_sample:,0], x_test[n,points_per_sample:,1], x_test[n,points_per_sample:,2]])\n",
    "\n",
    "inputs = x_test[n][None,:]\n",
    "runlen = 2\n",
    "corr_sum = np.zeros([1,3]) #init var to store correction contributions\n",
    "for i in range(runlen):\n",
    "    correction = model.predict(inputs)[0] #show what the network thinks\n",
    "#     correction = correction*0.1 #for synthetic matab data only??\n",
    "#     correction = y_test[n] #show actual solution\n",
    "    corr_sum += correction\n",
    "    c1 = np.array([c1[0,:] + correction[0], c1[1,:] + correction[1], c1[2,:] + correction[2]])\n",
    "    inputs = np.append(c1, c2, axis = 1).T[None,:,:]\n",
    "\n",
    "print(\"\\n correct soln\", y_test[n])\n",
    "print(\"\\n estiamted soln:\", corr_sum)\n",
    "print(\"\\n error from DNN:\", y_test[n] - corr_sum)\n",
    "mean1 = np.mean(x_test[n,:points_per_sample], axis = 0)\n",
    "mean2 = np.mean(x_test[n,points_per_sample:], axis = 0)\n",
    "print(\"\\n error in means\",  y_test[n] + (mean1 - mean2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad039ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "active-liverpool",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soln_compact_xyz: \n",
      " tf.Tensor([ 0.0718215   0.40911397 -0.07713326], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89130f8382ed4468ba9a585895735e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=1043, layout=Layout(height='auto', width='100%'), width=1397)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use Vedo to plot inital and transformed point clouds in 3D \n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "\n",
    "plt1 = Plotter(N = 1, axes = 13, bg = (1, 1, 1), interactive = True)\n",
    "disp = []\n",
    "\n",
    "#draw scan1 \n",
    "# disp.append(Points(x_test[n,:points_per_sample].numpy(), c = 'green', r = 5))\n",
    "disp.append(Points(x_test[n,:points_per_sample], c = 'red', r = 5))\n",
    "\n",
    "#draw initial scan2\n",
    "# disp.append(Points(x_test[n,points_per_sample:].numpy(), c = 'red', r = 5))\n",
    "disp.append(Points(x_test[n,points_per_sample:], c = 'blue', r = 5))\n",
    "\n",
    "#Draw arrow for ground truth soln vec\n",
    "# disp.append(Arrow(mean1 + y_test[n], mean1, c = 'y4', s = 0.002, res = 100)) #arbitrarily start arrow from scan1 center\n",
    "disp.append(Arrow(mean2, mean2 - y_test[n], c = 'y4', s = 0.002, res = 100))\n",
    "\n",
    "#draw ground truth arrow cut short by U and L\n",
    "soln_compact = tf.matmul(LUT[n], y_test[n][:, None])\n",
    "soln_compact_xyz = tf.matmul(U[n], soln_compact)[:,0]\n",
    "soln_compact_xyz = tf.matmul(tf.transpose(U[n]), soln_compact)[:,0]\n",
    "print(\"soln_compact_xyz: \\n\", soln_compact_xyz)\n",
    "disp.append(Arrow( mean2, mean2 - soln_compact_xyz, c = 'p4', s = 0.002, res = 100)) #arbitrarily start arrow from scan1 center\n",
    "\n",
    "#draw the set of 8 points that defined the voxel boundaries for the keyframe scan\n",
    "corn_cart = s2c(corn_test[n])\n",
    "# disp.append(Points(corn_cart, c = 'black', r = 10))\n",
    "\n",
    "#draw box instead of individual points______________________________\n",
    "p1, p2, p3, p4, p5, p6, p7, p8 = corn_cart.numpy()\n",
    "# print(p1)\n",
    "# print(p2)\n",
    "lineWidth = 2\n",
    "c1 = \"black\"\n",
    "\n",
    "arc1 = shapes.Line(p1, p2, c = c1, lw = lineWidth) \n",
    "disp.append(arc1)\n",
    "arc2 = shapes.Line(p3, p4, c = c1, lw = lineWidth) #debug\n",
    "disp.append(arc2)\n",
    "line1 = shapes.Line(p1, p3, c = c1, lw = lineWidth)\n",
    "disp.append(line1)\n",
    "line2 = shapes.Line(p2, p4, c = c1, lw = lineWidth) #problem here\n",
    "disp.append(line2)\n",
    "arc3 = shapes.Line(p5, p6, c = c1, lw = lineWidth) #debug\n",
    "disp.append(arc3)\n",
    "arc4 = shapes.Line(p7, p8, c = c1, lw = lineWidth) #debug\n",
    "disp.append(arc4)\n",
    "line3 = shapes.Line(p5, p7, c = c1, lw = lineWidth)\n",
    "disp.append(line3)\n",
    "line4 = shapes.Line(p6, p8, c = c1, lw = lineWidth)\n",
    "disp.append(line4)\n",
    "disp.append(shapes.Line(p1,p5, c = c1, lw = lineWidth))\n",
    "disp.append(shapes.Line(p2,p6, c = c1, lw = lineWidth))\n",
    "disp.append(shapes.Line(p3,p7, c = c1, lw = lineWidth))\n",
    "disp.append(shapes.Line(p4,p8, c = c1, lw = lineWidth))\n",
    "#_____________________________________________________________________\n",
    "\n",
    "#draw transformed scan2\n",
    "# disp.append(Points(c1, c = 'blue', r = 5))\n",
    "\n",
    "plt1.show(disp, \"Network Performance Test\")\n",
    "ViewInteractiveWidget(plt1.window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-christopher",
   "metadata": {},
   "source": [
    "### Run network on all test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "proud-puzzle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test tf.Tensor([12260   100     3], shape=(3,), dtype=int32)\n",
      "384/384 [==============================] - 1s 1ms/step\n",
      "\n",
      " correct soln \n",
      " [[ 0.9814831  -0.17602861 -1.5250226 ]\n",
      " [-0.37280306  0.71115476 -0.943462  ]\n",
      " [ 0.4256593   0.3306437   0.25062585]\n",
      " ...\n",
      " [ 3.7600217  -1.3633529  -0.09132093]\n",
      " [ 0.03050299  0.63155997  0.33192906]\n",
      " [ 0.00528116  2.518143    0.2139193 ]]\n",
      "\n",
      " estiamted soln: \n",
      " [[ 8.62124562e-01 -9.21205580e-02 -8.83652925e-01]\n",
      " [-4.14431393e-01  4.34967041e-01 -5.34572423e-01]\n",
      " [ 3.96568835e-01  3.50146323e-01  2.62679994e-01]\n",
      " ...\n",
      " [ 3.82962704e+00 -1.22965384e+00  2.40137195e-03]\n",
      " [ 2.89848484e-02  5.92733860e-01  3.24466348e-01]\n",
      " [ 5.58916554e-02  2.53413630e+00  2.19279960e-01]]\n",
      "\n",
      " error from DNN: \n",
      " [[ 0.11935854 -0.08390805 -0.6413697 ]\n",
      " [ 0.04162833  0.27618772 -0.40888959]\n",
      " [ 0.02909046 -0.01950261 -0.01205415]\n",
      " ...\n",
      " [-0.06960535 -0.13369906 -0.0937223 ]\n",
      " [ 0.00151814  0.03882611  0.00746271]\n",
      " [-0.0506105  -0.01599336 -0.00536066]]\n",
      "\n",
      " mean raw DNN error: \n",
      " [0.20788168 0.22087188 0.41234964]\n",
      "\n",
      " mean raw D2D error \n",
      " [0.04239465 0.04467432 0.02369623]\n"
     ]
    }
   ],
   "source": [
    "# for n in range(numToTest):\n",
    "c1 = np.array([x_test[:,:points_per_sample,0], x_test[:,:points_per_sample,1], x_test[:,:points_per_sample,2]])\n",
    "c2 = np.array([x_test[:,points_per_sample:,0], x_test[:,points_per_sample:,1], x_test[:,points_per_sample:,2]])\n",
    "c1 = np.transpose(c1, (1,2,0))\n",
    "c2 = np.transpose(c2, (1,2,0))\n",
    "\n",
    "inputs = x_test\n",
    "# print(\"c1\" , tf.shape(c1))\n",
    "print(\"x_test\" , tf.shape(x_test))\n",
    "runlen = 1\n",
    "corr_sum = np.zeros([tf.shape(x_test)[0].numpy(),1,3]) #init var to store correction contributions\n",
    "for i in range(runlen):\n",
    "    correction = model.predict(inputs)[:,None,:] #show what the network thinks\n",
    "#     correction = correction*0.1 #for synthetic matab data only??\n",
    "#     correction = y_test[n] #show actual solution\n",
    "    corr_sum += correction\n",
    "#     print(\"corr_sum\", tf.shape(corr_sum))\n",
    "    c1 += correction\n",
    "#     print(\"after correction\", tf.shape(c1))\n",
    "    inputs = np.append(c1, c2, axis = 1)#.T\n",
    "#     print(\"\\n new inputs\", tf.shape(inputs))\n",
    "    \n",
    "dnn_estimates = corr_sum[:,0,:]\n",
    "print(\"\\n correct soln \\n\", y_test)\n",
    "print(\"\\n estiamted soln: \\n\", dnn_estimates)\n",
    "print(\"\\n error from DNN: \\n\", y_test - dnn_estimates)\n",
    "\n",
    "# print(\"\\n mean raw DNN error: \\n\", np.sqrt(np.sum(np.mean(np.abs(y_test - dnn_estimates), axis = 0)**2)))\n",
    "print(\"\\n mean raw DNN error: \\n\", np.sqrt(np.mean(np.abs(y_test - dnn_estimates), axis = 0)**2))\n",
    "\n",
    "D2D_distance = np.mean(x_test[:,:points_per_sample], axis = 1) - np.mean(x_test[:,points_per_sample:], axis = 1)\n",
    "# print(tf.shape(D2D_distance))\n",
    "# print(tf.shape(y_test))\n",
    "# print(\"\\n mean raw D2D error \\n\", np.sqrt(np.sum(np.mean(np.abs(y_test + D2D_distance), axis = 0)**2 )))\n",
    "print(\"\\n mean raw D2D error \\n\", np.sqrt(np.mean(np.abs(y_test + D2D_distance), axis = 0)**2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "understanding-gibson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " mean compact error DNN: \n",
      " [0.1183895  0.13934372 0.14059488]\n",
      "\n",
      " mean compact error D2D: \n",
      " [0.01307738 0.01476391 0.00778655]\n"
     ]
    }
   ],
   "source": [
    "#just considering the residuals doesn't tell the whole story \n",
    "#   we need to consider error in COMPACT only directions\n",
    "\n",
    "#use LUT to get compact axis of DNN solution vec for each trial\n",
    "dnn_compact = tf.matmul(LUT, dnn_estimates[:,:,None])\n",
    "dnn_compact_xyz = tf.matmul(U, dnn_compact)\n",
    "\n",
    "#for distrubution means distance\n",
    "d2d_compact = tf.matmul(LUT, D2D_distance[:,:,None])\n",
    "d2d_compact_xyz = tf.matmul(U, d2d_compact)\n",
    "\n",
    "truth_compact = tf.matmul(LUT, y_test[:,:,None])\n",
    "truth_compact_xyz = tf.matmul(U, truth_compact)\n",
    "\n",
    "# error_DNN_compact = np.sqrt(np.sum( np.mean(np.abs(truth_compact_xyz - dnn_compact_xyz), axis = 0)**2 ))\n",
    "error_DNN_compact = np.sqrt( np.mean(np.abs(truth_compact_xyz - dnn_compact_xyz), axis = 0)**2 )[:,0]\n",
    "print(\"\\n mean compact error DNN: \\n\", error_DNN_compact)\n",
    "\n",
    "# error_D2D_compact = np.sqrt(np.sum( np.mean(np.abs(truth_compact_xyz + d2d_compact_xyz), axis = 0)**2 ))\n",
    "error_D2D_compact = np.sqrt(np.mean(np.abs(truth_compact_xyz + d2d_compact_xyz), axis = 0)**2 )[:,0]\n",
    "print(\"\\n mean compact error D2D: \\n\", error_D2D_compact)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2c(corn_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-celebration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
