{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rental-ordinary",
   "metadata": {},
   "source": [
    "### Monte-Carlo sims for comparing D2D and DNN solution vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacterial-nerve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 17:50:54.670669: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-10 17:50:54.764538: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-10 17:50:55.089333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-10 17:50:55.089375: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-10 17:50:55.089379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 17:50:55.867002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-10 17:50:55.885243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-10 17:50:55.885373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#need to set this to allow eager execution within TF backend (i.e. my custom loss function)\n",
    "# tf.config.run_functions_eagerly(True) #debug\n",
    "# tf.data.experimental.enable_debug_mode() \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import datetime\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.sans-serif\": [\"Times\"],\n",
    "    \"font.size\": 12})\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 4*1024\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "print(tf.__version__) #requires tensorflow 2.3\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48fc9469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\begin{aligned}\n",
       "& \\text {Table 1.1. Performance on Normalized Training Data }\\\\\n",
       "&\\begin{array}{cccc}\n",
       "\\hline \\hline \\text { } & \\text { Mean Absolute Error } & \\text { Batch Size } & \\text { Epochs } & \\text {Notes} \\\\\n",
       "\\hline \n",
       "\n",
       "\\text{ModelNet40 320k LIDAR (100pts)} & 0.0017 \\text { MSE} & 1024 & 100 & \\text {Net.kmod } \\\\\n",
       "\n",
       "\\text{ModelNet40 LIDAR MAIN (100pts)} & 0.0321 & 1024 & 64 & \\text {SmallNet.kmod } \\\\\n",
       "\n",
       "\\text{KITTI 91 skip3 (100pts)} & 0.050 & 128 & 32 & \\text {KITTINet100.kmod } \\\\\n",
       "\n",
       "\\text{KITTI_CARLA 1,3 KITTI 91, 95 (100pts)} & 0.0403 & 1024 & 16 & \\text {KITTICARLA100.kmod } \\\\\n",
       "\n",
       "\\text{combined data (100pts)} & 0.0413 & 1024 & 64 & \\text {combinedNet.kmod} \\\\\n",
       "\n",
       "\\text{Big Compact (100pts)} & 0.0285 \\text { MAE} & 1024 & 64 & \\text {CompactNet.kmod} \\\\\n",
       "\n",
       "\\text{ 5, 27, 28 regular (100pts)} & 0.0521 \\text { MAE} & 1024 & 16 & \\text {ForestNet.kmod} \\\\\n",
       "\n",
       "\\hline\n",
       "\\end{array}\n",
       "\\end{aligned}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text {Table 1.1. Performance on Normalized Training Data }\\\\\n",
    "&\\begin{array}{cccc}\n",
    "\\hline \\hline \\text { } & \\text { Mean Absolute Error } & \\text { Batch Size } & \\text { Epochs } & \\text {Notes} \\\\\n",
    "\\hline \n",
    "\n",
    "\\text{ModelNet40 320k LIDAR (100pts)} & 0.0017 \\text { MSE} & 1024 & 100 & \\text {Net.kmod } \\\\\n",
    "\n",
    "\\text{ModelNet40 LIDAR MAIN (100pts)} & 0.0321 & 1024 & 64 & \\text {SmallNet.kmod } \\\\\n",
    "\n",
    "\\text{KITTI 91 skip3 (100pts)} & 0.050 & 128 & 32 & \\text {KITTINet100.kmod } \\\\\n",
    "\n",
    "\\text{KITTI_CARLA 1,3 KITTI 91, 95 (100pts)} & 0.0403 & 1024 & 16 & \\text {KITTICARLA100.kmod } \\\\\n",
    "\n",
    "\\text{combined data (100pts)} & 0.0413 & 1024 & 64 & \\text {combinedNet.kmod} \\\\\n",
    "\n",
    "\\text{Big Compact (100pts)} & 0.0285 \\text { MAE} & 1024 & 64 & \\text {CompactNet.kmod} \\\\\n",
    "\n",
    "\\text{ 5, 27, 28 regular (100pts)} & 0.0521 \\text { MAE} & 1024 & 16 & \\text {ForestNet.kmod} \\\\\n",
    "\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "significant-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(\"ForestNet.kmod\") #test\n",
    "model = tf.keras.models.load_model(\"CompactNet.kmod\", custom_objects={'compact_loss': compact_loss}) # trained with custom loss function using ULU^T \n",
    "# model = tf.keras.models.load_model(\"CompactNet.kmod\", compile = False)\n",
    "# model = tf.keras.models.load_model(\"KITTICARLA100.kmod\") #best for KITTI CARLA\n",
    "# model = tf.keras.models.load_model(\"KITTINet100.kmod\") #best KITTI (trained on combined)\n",
    "# model = tf.keras.models.load_model(\"Net.kmod\") # best so far for shadowed ModelNet40 point clouds\n",
    "# model = tf.keras.models.load_model(\"SmallNet.kmod\") # best for uniform sampling dataset\n",
    "# model = tf.keras.models.load_model(\"combinedNet.kmod\") #KITTI + KITTI_CARLA + ModelNet40\n",
    "# model = tf.keras.models.load_model(\"KITTInet.kmod\") #50pts, 3cm MAE, trained a while ago..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "utility-burst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151810, 200, 3)\n",
      "(7990, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "#load COMPACT test data\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_compact_scan1.npy\")\n",
    "d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_compact_scan2.npy\")\n",
    "gt = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_ground_truth.npy\")\n",
    "cgt = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_compact_ground_truth.npy\")\n",
    "LUT = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_LUT.npy\")\n",
    "L = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_L.npy\")\n",
    "U = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_U.npy\")\n",
    "corn = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_corn.npy\")\n",
    "\n",
    "# d1 = np.load(\"training_data/compact_scan1.npy\")\n",
    "# d2 = np.load(\"training_data/compact_scan2.npy\")\n",
    "# gt = np.load(\"training_data/ground_truth.npy\")\n",
    "# cgt = np.load(\"training_data/compact_ground_truth.npy\")\n",
    "# LUT = np.load(\"training_data/LUT.npy\")\n",
    "# L = np.load(\"training_data/L.npy\")\n",
    "# U = np.load(\"training_data/U.npy\")\n",
    "# corn = np.load(\"training_data/corn.npy\")\n",
    "# d1 = np.append(d1, d1_7, axis = 0)\n",
    "# d2 = np.append(d2, d2_7, axis = 0)\n",
    "# gt = np.append(gt, gt_7, axis = 0)\n",
    "# cgt = np.append(cgt, cgt_7, axis = 0)\n",
    "# LUT = np.append(LUT, LUT_7, axis = 0)\n",
    "# L = np.append(L, L_7, axis = 0)\n",
    "# U = np.append(U, U_7, axis = 0)\n",
    "# corn = np.append(corn, corn_7, axis = 0)\n",
    "# d1_7 = d2_7 = gt_7 = cgt_7 = LUT_7 = L_7 = U_7 = corn_7 = None\n",
    "\n",
    "# d1_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_compact_scan1.npy\")\n",
    "# d2_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_compact_scan2.npy\")\n",
    "# gt_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_ground_truth.npy\")\n",
    "# cgt_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_compact_ground_truth.npy\")\n",
    "# LUT_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_LUT.npy\")\n",
    "# L_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_L.npy\")\n",
    "# U_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_U.npy\")\n",
    "# corn_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_corn.npy\")\n",
    "# d1 = np.append(d1, d1_3, axis = 0)\n",
    "# d2 = np.append(d2, d2_3, axis = 0)\n",
    "# gt = np.append(gt, gt_3, axis = 0)\n",
    "# cgt = np.append(cgt, cgt_3, axis = 0)\n",
    "# LUT = np.append(LUT, LUT_3, axis = 0)\n",
    "# L = np.append(L, L_3, axis = 0)\n",
    "# U = np.append(U, U_3, axis = 0)\n",
    "# corn = np.append(corn, corn_3, axis = 0)\n",
    "# d1_3 = d2_3 = gt_3 = cgt_3 = LUT_3 = L_3 = U_3 = corn_3 = None\n",
    "\n",
    "\n",
    "# d1_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown2_compact_scan1.npy\")\n",
    "# d2_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown2_compact_scan2.npy\")\n",
    "# gt_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown2_ground_truth.npy\")\n",
    "# cgt_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown2_compact_ground_truth.npy\")\n",
    "# LUT_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown2_LUT.npy\")\n",
    "# L_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown2_L.npy\")\n",
    "# U_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown2_U.npy\")\n",
    "# corn_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown2_corn.npy\")\n",
    "# d1 = np.append(d1, d1_5, axis = 0)\n",
    "# d2 = np.append(d2, d2_5, axis = 0)\n",
    "# gt = np.append(gt, gt_5, axis = 0)\n",
    "# cgt = np.append(cgt, cgt_5, axis = 0)\n",
    "# LUT = np.append(LUT, LUT_5, axis = 0)\n",
    "# L = np.append(L, L_5, axis = 0)\n",
    "# U = np.append(U, U_5, axis = 0)\n",
    "# corn = np.append(corn, corn_5, axis = 0)\n",
    "# d1_5 = d2_5 = gt_5 = cgt_5 = LUT_5 = L_5 = U_5 = corn_5 = None\n",
    "\n",
    "# d1_8 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0117_compact_scan1.npy\")\n",
    "# d2_8 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0117_compact_scan2.npy\")\n",
    "# gt_8 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0117_ground_truth.npy\")\n",
    "# cgt_8 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0117_compact_ground_truth.npy\")\n",
    "# LUT_8 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0117_LUT.npy\")\n",
    "# L_8 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0117_L.npy\")\n",
    "# U_8 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0117_U.npy\")\n",
    "# corn_8 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0117_corn.npy\")\n",
    "# d1 = np.append(d1, d1_8, axis = 0)\n",
    "# d2 = np.append(d2, d2_8, axis = 0)\n",
    "# gt = np.append(gt, gt_8, axis = 0)\n",
    "# cgt = np.append(cgt, cgt_8, axis = 0)\n",
    "# LUT = np.append(LUT, LUT_8, axis = 0)\n",
    "# L = np.append(L, L_8, axis = 0)\n",
    "# U = np.append(U, U_8, axis = 0)\n",
    "# corn = np.append(corn, corn_8, axis = 0)\n",
    "# d1_8 = d2_8 = gt_8 = cgt_8 = LUT_8 = L_8 = U_8 = corn_8 = None\n",
    "\n",
    "# #test: add in synthetic ModelNet40 training data\n",
    "# d1_6 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_scan1_320k.npy\")\n",
    "# d2_6 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_scan2_320k.npy\")\n",
    "# gt_6 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_ground_truth_320k.npy\")\n",
    "# LUT_6 = tf.eye(3,3, batch_shape = [len(gt_6)])\n",
    "# L_6 = tf.eye(3,3, batch_shape = [len(gt_6)])\n",
    "# U_6 = tf.eye(3,3, batch_shape = [len(gt_6)])\n",
    "# d1 = np.append(d1, d1_6, axis = 0)\n",
    "# d2 = np.append(d2, d2_6, axis = 0)\n",
    "# gt = np.append(gt, gt_6, axis = 0)\n",
    "# LUT = np.append(LUT, LUT_6, axis = 0)\n",
    "# L = np.append(L, L_6, axis = 0)\n",
    "# U = np.append(U, U_6, axis = 0)\n",
    "# d1_6= d2_6 = gt_6 = cgt_6 = LUT_6 = L_6 = U_6 = corn_6 = None\n",
    "\n",
    "# d1_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_compact_scan1.npy\")\n",
    "# d2_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_compact_scan2.npy\")\n",
    "# gt_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_ground_truth.npy\")\n",
    "# cgt_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_compact_ground_truth.npy\")\n",
    "# LUT_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_LUT.npy\")\n",
    "# L_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_L.npy\")\n",
    "# U_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_U.npy\")\n",
    "# corn_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_corn.npy\")\n",
    "# d1 = np.append(d1, d1_4, axis = 0)\n",
    "# d2 = np.append(d2, d2_4, axis = 0)\n",
    "# gt = np.append(gt, gt_4, axis = 0)\n",
    "# cgt = np.append(cgt, cgt_4, axis = 0)\n",
    "# LUT = np.append(LUT, LUT_4, axis = 0)\n",
    "# L = np.append(L, L_4, axis = 0)\n",
    "# U = np.append(U, U_4, axis = 0)\n",
    "# corn = np.append(corn, corn_4, axis = 0)\n",
    "# d1_4 = d2_4 = gt_4 = cgt_4 = LUT_4 = L_4 = U_4 = corn_4 = None\n",
    "\n",
    "# d1_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_compact_scan1.npy\")\n",
    "# d2_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_compact_scan2.npy\")\n",
    "# gt_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_ground_truth.npy\")\n",
    "# cgt_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_compact_ground_truth.npy\")\n",
    "# LUT_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_LUT.npy\")\n",
    "# L_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_L.npy\")\n",
    "# U_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_U.npy\")\n",
    "# corn_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_corn.npy\")\n",
    "# d1 = np.append(d1, d1_2, axis = 0)\n",
    "# d2 = np.append(d2, d2_2, axis = 0)\n",
    "# gt = np.append(gt, gt_2, axis = 0)\n",
    "# cgt = np.append(cgt, cgt_2, axis = 0)\n",
    "# LUT = np.append(LUT, LUT_2, axis = 0)\n",
    "# L = np.append(L, L_2, axis = 0)\n",
    "# U = np.append(U, U_2, axis = 0)\n",
    "# corn = np.append(corn, corn_2, axis = 0)\n",
    "# d1_2= d2_2 = gt_2 = cgt_2 = LUT_2 = L_2 = U_2 = corn_2 = None\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "#reshape but don't convert to tensor\n",
    "points_per_sample = 100          #poitns sammpled from each voxel\n",
    "tsplit = 0.95 #0.95                   #this fraction goes into training\n",
    "\n",
    "scan1 = np.reshape(d1, [-1, points_per_sample, 3])\n",
    "scan2 = np.reshape(d2, [-1, points_per_sample, 3])\n",
    "ntrain = int(tsplit*tf.shape(scan1)[0].numpy())\n",
    "\n",
    "x_train = np.append(scan1[:ntrain], scan2[:ntrain], axis = 1)\n",
    "x_test = np.append(scan1[ntrain:], scan2[ntrain:], axis = 1)\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(x_test))\n",
    "\n",
    "y_train = gt[:ntrain] #for standard training/ test data\n",
    "y_test = gt[ntrain:]\n",
    "\n",
    "# y_train = gt[:ntrain][:,:,0] #when using compact data\n",
    "# y_test = gt[ntrain:][:,:,0]\n",
    "LUT_train = tf.convert_to_tensor(LUT)[:ntrain] \n",
    "ULUT_train = tf.matmul(U[:ntrain], LUT_train)\n",
    "\n",
    "LUT = tf.convert_to_tensor(LUT)[ntrain:]\n",
    "ULUT_test = tf.matmul(U[ntrain:], LUT)\n",
    "\n",
    "\n",
    "U = tf.convert_to_tensor(U)[ntrain:]\n",
    "L = tf.convert_to_tensor(L)[ntrain:]\n",
    "corn_train = corn[:ntrain]\n",
    "corn_test = corn[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae6493",
   "metadata": {},
   "source": [
    "## Test training with custom loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "340cd13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 200, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3, 4), dtype=tf.float32, name=None))>\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 200, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3, 4), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# tf.data.experimental.enable_debug_mode()\n",
    "def augment(points, gt):\n",
    "    \n",
    "#     dt = tf.float64\n",
    "    dt = tf.float32\n",
    "    \n",
    "    # jitter points\n",
    "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=dt)\n",
    "\n",
    "    #shuffle and randomly translate cloud 2 and ground truth\n",
    "    shift_scale = 0.2 #don't want to make this too large for PC to reach in 1 iter(?)\n",
    "    shift = tf.cast(tf.concat([shift_scale*tf.random.normal([3])], axis=0) , dt) #may need float64\n",
    "    points = tf.concat([tf.random.shuffle(points[:100]), tf.random.shuffle(points[100:]) + shift[:3]], axis = 0)\n",
    "#     gt = tf.concat([(gt[:,0] + shift)[:,None], gt[:,1:] ], axis = -1) #was this\n",
    "    gt = tf.concat([(gt[:,0] + tf.cast(shift,tf.float32))[:,None], gt[:,1:] ], axis = -1) #when working with modelnet40\n",
    "\n",
    "    #no shift\n",
    "#     points = tf.concat([tf.random.shuffle(points[:100]), tf.random.shuffle(points[100:])], axis = 0)\n",
    "    \n",
    "    return points, gt\n",
    "\n",
    "BATCH_SIZE =  1024\n",
    "\n",
    "y_train_extra = tf.concat([y_train[:, :, None], ULUT_train], 2)\n",
    "# print(y_train_extra)\n",
    "# print(x_train)\n",
    "# y_train_extra = np.append(y_train[:, :, None], ULUT_train, 2)\n",
    "y_test_extra = tf.concat([y_test[:, :, None], ULUT_test], 2)\n",
    "# y_test_extra = np.append(y_test[:, :, None], ULUT_test, 2)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_extra))\n",
    "train_dataset = train_dataset.shuffle(len(x_train)).map(augment).batch(BATCH_SIZE)\n",
    "print(train_dataset)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test_extra))\n",
    "val_dataset = val_dataset.shuffle(len(x_test)).map(augment).batch(BATCH_SIZE)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cdaa28e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "Tensor(\"compact_loss/MatMul:0\", shape=(None, 3, 1), dtype=float32)\n",
      "Tensor(\"compact_loss/strided_slice_5:0\", shape=(None, 3, 1), dtype=float32)\n",
      "Tensor(\"compact_loss/MatMul:0\", shape=(None, 3, 1), dtype=float32)\n",
      "Tensor(\"compact_loss/strided_slice_5:0\", shape=(None, 3, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 17:51:18.936983: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n",
      "2022-11-10 17:51:19.220276: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-11-10 17:51:19.355320: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-11-10 17:51:29.355721: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 800.00MiB (rounded to 838860800)requested by op model/batch_normalization_3/FusedBatchNormV3\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-11-10 17:51:29.355758: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc\n",
      "2022-11-10 17:51:29.355770: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): \tTotal Chunks: 59, Chunks in use: 58. 14.8KiB allocated for chunks. 14.5KiB in use in bin. 309B client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355777: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355784: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): \tTotal Chunks: 38, Chunks in use: 38. 39.0KiB allocated for chunks. 39.0KiB in use in bin. 38.0KiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355790: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): \tTotal Chunks: 41, Chunks in use: 41. 88.5KiB allocated for chunks. 88.5KiB in use in bin. 88.0KiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355796: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): \tTotal Chunks: 37, Chunks in use: 37. 148.0KiB allocated for chunks. 148.0KiB in use in bin. 148.0KiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355802: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): \tTotal Chunks: 3, Chunks in use: 2. 39.0KiB allocated for chunks. 24.0KiB in use in bin. 24.0KiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355808: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): \tTotal Chunks: 4, Chunks in use: 4. 108.0KiB allocated for chunks. 108.0KiB in use in bin. 107.6KiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355814: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): \tTotal Chunks: 2, Chunks in use: 2. 72.0KiB allocated for chunks. 72.0KiB in use in bin. 71.9KiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355819: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355824: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355829: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): \tTotal Chunks: 5, Chunks in use: 5. 1.90MiB allocated for chunks. 1.90MiB in use in bin. 1.75MiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355835: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): \tTotal Chunks: 8, Chunks in use: 8. 4.33MiB allocated for chunks. 4.33MiB in use in bin. 4.33MiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355840: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): \tTotal Chunks: 4, Chunks in use: 4. 5.63MiB allocated for chunks. 5.63MiB in use in bin. 4.75MiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355846: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): \tTotal Chunks: 7, Chunks in use: 7. 14.34MiB allocated for chunks. 14.34MiB in use in bin. 14.34MiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355852: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): \tTotal Chunks: 3, Chunks in use: 3. 12.00MiB allocated for chunks. 12.00MiB in use in bin. 12.00MiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355858: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): \tTotal Chunks: 3, Chunks in use: 3. 28.00MiB allocated for chunks. 28.00MiB in use in bin. 24.00MiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355863: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355869: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): \tTotal Chunks: 1, Chunks in use: 1. 33.27MiB allocated for chunks. 33.27MiB in use in bin. 33.27MiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355873: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355882: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): \tTotal Chunks: 4, Chunks in use: 4. 800.00MiB allocated for chunks. 800.00MiB in use in bin. 800.00MiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355887: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): \tTotal Chunks: 5, Chunks in use: 4. 3.12GiB allocated for chunks. 2.34GiB in use in bin. 2.34GiB client-requested in use in bin.\n",
      "2022-11-10 17:51:29.355893: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 800.00MiB was 256.00MiB, Chunk State: \n",
      "2022-11-10 17:51:29.355904: I tensorflow/core/common_runtime/bfc_allocator.cc:1062]   Size: 796.03MiB | Requested Size: 400.00MiB | in_use: 0 | bin_num: 20, prev:   Size: 800.00MiB | Requested Size: 800.00MiB | in_use: 1 | bin_num: -1\n",
      "2022-11-10 17:51:29.355908: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 4294967296\n",
      "2022-11-10 17:51:29.355915: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344000000 of size 27648 next 7\n",
      "2022-11-10 17:51:29.355920: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344006c00 of size 27648 next 8\n",
      "2022-11-10 17:51:29.355924: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134400d800 of size 27648 next 10\n",
      "2022-11-10 17:51:29.355928: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344014400 of size 256 next 14\n",
      "2022-11-10 17:51:29.355932: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344014500 of size 256 next 15\n",
      "2022-11-10 17:51:29.355936: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344014600 of size 256 next 16\n",
      "2022-11-10 17:51:29.355939: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344014700 of size 256 next 17\n",
      "2022-11-10 17:51:29.355943: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344014800 of size 256 next 19\n",
      "2022-11-10 17:51:29.355947: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344014900 of size 256 next 20\n",
      "2022-11-10 17:51:29.355951: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344014a00 of size 256 next 22\n",
      "2022-11-10 17:51:29.355955: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344014b00 of size 256 next 24\n",
      "2022-11-10 17:51:29.355959: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344014c00 of size 1024 next 25\n",
      "2022-11-10 17:51:29.355963: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344015000 of size 256 next 23\n",
      "2022-11-10 17:51:29.355967: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344015100 of size 1024 next 26\n",
      "2022-11-10 17:51:29.355971: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344015500 of size 1024 next 28\n",
      "2022-11-10 17:51:29.355975: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344015900 of size 1024 next 213\n",
      "2022-11-10 17:51:29.355978: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344015d00 of size 1024 next 30\n",
      "2022-11-10 17:51:29.355982: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344016100 of size 256 next 31\n",
      "2022-11-10 17:51:29.355986: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344016200 of size 256 next 32\n",
      "2022-11-10 17:51:29.355990: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344016300 of size 1280 next 9\n",
      "2022-11-10 17:51:29.355994: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344016800 of size 36864 next 12\n",
      "2022-11-10 17:51:29.355998: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134401f800 of size 3072 next 27\n",
      "2022-11-10 17:51:29.356003: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344020400 of size 418816 next 3\n",
      "2022-11-10 17:51:29.356007: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344086800 of size 523264 next 4\n",
      "2022-11-10 17:51:29.356012: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344106400 of size 523264 next 5\n",
      "2022-11-10 17:51:29.356016: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344186000 of size 27648 next 6\n",
      "2022-11-10 17:51:29.356020: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134418cc00 of size 697856 next 11\n",
      "2022-11-10 17:51:29.356024: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344237200 of size 1836032 next 18\n",
      "2022-11-10 17:51:29.356028: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13443f7600 of size 697856 next 21\n",
      "2022-11-10 17:51:29.356032: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a1c00 of size 1024 next 33\n",
      "2022-11-10 17:51:29.356035: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a2000 of size 1024 next 34\n",
      "2022-11-10 17:51:29.356039: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a2400 of size 2048 next 38\n",
      "2022-11-10 17:51:29.356043: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a2c00 of size 256 next 39\n",
      "2022-11-10 17:51:29.356047: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a2d00 of size 256 next 40\n",
      "2022-11-10 17:51:29.356051: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a2e00 of size 2048 next 43\n",
      "2022-11-10 17:51:29.356055: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a3600 of size 2048 next 41\n",
      "2022-11-10 17:51:29.356059: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a3e00 of size 2048 next 42\n",
      "2022-11-10 17:51:29.356063: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a4600 of size 4096 next 47\n",
      "2022-11-10 17:51:29.356067: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a5600 of size 256 next 48\n",
      "2022-11-10 17:51:29.356071: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a5700 of size 256 next 49\n",
      "2022-11-10 17:51:29.356075: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a5800 of size 4096 next 52\n",
      "2022-11-10 17:51:29.356078: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a6800 of size 4096 next 50\n",
      "2022-11-10 17:51:29.356082: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a7800 of size 4096 next 51\n",
      "2022-11-10 17:51:29.356086: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a8800 of size 4096 next 55\n",
      "2022-11-10 17:51:29.356090: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444a9800 of size 4096 next 56\n",
      "2022-11-10 17:51:29.356094: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444aa800 of size 256 next 57\n",
      "2022-11-10 17:51:29.356098: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444aa900 of size 256 next 58\n",
      "2022-11-10 17:51:29.356102: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444aaa00 of size 4096 next 61\n",
      "2022-11-10 17:51:29.356105: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444aba00 of size 4096 next 59\n",
      "2022-11-10 17:51:29.356110: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444aca00 of size 4096 next 60\n",
      "2022-11-10 17:51:29.356113: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444ada00 of size 4096 next 64\n",
      "2022-11-10 17:51:29.356117: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444aea00 of size 4096 next 65\n",
      "2022-11-10 17:51:29.356121: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444afa00 of size 256 next 66\n",
      "2022-11-10 17:51:29.356125: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444afb00 of size 256 next 67\n",
      "2022-11-10 17:51:29.356129: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444afc00 of size 4096 next 68\n",
      "2022-11-10 17:51:29.356133: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b0c00 of size 4096 next 69\n",
      "2022-11-10 17:51:29.356137: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b1c00 of size 4096 next 71\n",
      "2022-11-10 17:51:29.356141: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b2c00 of size 4096 next 72\n",
      "2022-11-10 17:51:29.356145: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b3c00 of size 4096 next 73\n",
      "2022-11-10 17:51:29.356149: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b4c00 of size 2048 next 76\n",
      "2022-11-10 17:51:29.356153: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b5400 of size 2048 next 74\n",
      "2022-11-10 17:51:29.356156: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b5c00 of size 2048 next 75\n",
      "2022-11-10 17:51:29.356160: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b6400 of size 2048 next 78\n",
      "2022-11-10 17:51:29.356164: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b6c00 of size 2048 next 79\n",
      "2022-11-10 17:51:29.356168: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b7400 of size 256 next 80\n",
      "2022-11-10 17:51:29.356172: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b7500 of size 256 next 81\n",
      "2022-11-10 17:51:29.356176: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b7600 of size 2048 next 82\n",
      "2022-11-10 17:51:29.356180: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b7e00 of size 2048 next 83\n",
      "2022-11-10 17:51:29.356184: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b8600 of size 2048 next 84\n",
      "2022-11-10 17:51:29.356187: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b8e00 of size 2048 next 86\n",
      "2022-11-10 17:51:29.356191: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b9600 of size 2048 next 87\n",
      "2022-11-10 17:51:29.356195: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444b9e00 of size 1024 next 90\n",
      "2022-11-10 17:51:29.356199: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444ba200 of size 1024 next 88\n",
      "2022-11-10 17:51:29.356203: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444ba600 of size 1024 next 89\n",
      "2022-11-10 17:51:29.356207: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444baa00 of size 1024 next 92\n",
      "2022-11-10 17:51:29.356211: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bae00 of size 1024 next 93\n",
      "2022-11-10 17:51:29.356214: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bb200 of size 256 next 94\n",
      "2022-11-10 17:51:29.356218: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bb300 of size 256 next 95\n",
      "2022-11-10 17:51:29.356222: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bb400 of size 256 next 97\n",
      "2022-11-10 17:51:29.356226: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bb500 of size 256 next 98\n",
      "2022-11-10 17:51:29.356230: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bb600 of size 256 next 96\n",
      "2022-11-10 17:51:29.356234: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bb700 of size 256 next 99\n",
      "2022-11-10 17:51:29.356238: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bb800 of size 256 next 102\n",
      "2022-11-10 17:51:29.356241: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bb900 of size 256 next 103\n",
      "2022-11-10 17:51:29.356245: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bba00 of size 256 next 104\n",
      "2022-11-10 17:51:29.356249: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bbb00 of size 256 next 105\n",
      "2022-11-10 17:51:29.356253: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bbc00 of size 256 next 106\n",
      "2022-11-10 17:51:29.356257: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bbd00 of size 256 next 107\n",
      "2022-11-10 17:51:29.356261: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bbe00 of size 256 next 108\n",
      "2022-11-10 17:51:29.356265: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bbf00 of size 256 next 109\n",
      "2022-11-10 17:51:29.356269: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bc000 of size 256 next 110\n",
      "2022-11-10 17:51:29.356273: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bc100 of size 3584 next 100\n",
      "2022-11-10 17:51:29.356277: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bcf00 of size 3072 next 101\n",
      "2022-11-10 17:51:29.356281: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bdb00 of size 1024 next 111\n",
      "2022-11-10 17:51:29.356285: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bdf00 of size 1024 next 112\n",
      "2022-11-10 17:51:29.356289: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444be300 of size 1024 next 113\n",
      "2022-11-10 17:51:29.356292: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444be700 of size 1024 next 114\n",
      "2022-11-10 17:51:29.356296: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444beb00 of size 1024 next 115\n",
      "2022-11-10 17:51:29.356300: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bef00 of size 1024 next 116\n",
      "2022-11-10 17:51:29.356304: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bf300 of size 2048 next 117\n",
      "2022-11-10 17:51:29.356308: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444bfb00 of size 2048 next 118\n",
      "2022-11-10 17:51:29.356312: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c0300 of size 2048 next 119\n",
      "2022-11-10 17:51:29.356316: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c0b00 of size 4096 next 121\n",
      "2022-11-10 17:51:29.356320: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c1b00 of size 4096 next 122\n",
      "2022-11-10 17:51:29.356324: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c2b00 of size 4096 next 123\n",
      "2022-11-10 17:51:29.356328: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c3b00 of size 4096 next 124\n",
      "2022-11-10 17:51:29.356332: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c4b00 of size 4096 next 125\n",
      "2022-11-10 17:51:29.356336: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c5b00 of size 4096 next 126\n",
      "2022-11-10 17:51:29.356339: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c6b00 of size 4096 next 128\n",
      "2022-11-10 17:51:29.356343: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c7b00 of size 4096 next 129\n",
      "2022-11-10 17:51:29.356347: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c8b00 of size 4096 next 130\n",
      "2022-11-10 17:51:29.356351: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444c9b00 of size 2048 next 131\n",
      "2022-11-10 17:51:29.356355: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444ca300 of size 2048 next 132\n",
      "2022-11-10 17:51:29.356359: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444cab00 of size 2048 next 133\n",
      "2022-11-10 17:51:29.356363: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444cb300 of size 2048 next 134\n",
      "2022-11-10 17:51:29.356367: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444cbb00 of size 2048 next 135\n",
      "2022-11-10 17:51:29.356371: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444cc300 of size 2048 next 136\n",
      "2022-11-10 17:51:29.356375: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444ccb00 of size 1024 next 138\n",
      "2022-11-10 17:51:29.356379: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444ccf00 of size 1024 next 139\n",
      "2022-11-10 17:51:29.356383: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444cd300 of size 1024 next 140\n",
      "2022-11-10 17:51:29.356387: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444cd700 of size 3072 next 141\n",
      "2022-11-10 17:51:29.356391: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444ce300 of size 256 next 142\n",
      "2022-11-10 17:51:29.356395: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444ce400 of size 3072 next 143\n",
      "2022-11-10 17:51:29.356399: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444cf000 of size 1024 next 144\n",
      "2022-11-10 17:51:29.356403: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444cf400 of size 1024 next 145\n",
      "2022-11-10 17:51:29.356407: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444cf800 of size 1024 next 146\n",
      "2022-11-10 17:51:29.356411: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444cfc00 of size 1024 next 148\n",
      "2022-11-10 17:51:29.356415: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d0000 of size 1024 next 149\n",
      "2022-11-10 17:51:29.356418: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d0400 of size 1024 next 150\n",
      "2022-11-10 17:51:29.356422: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d0800 of size 2048 next 152\n",
      "2022-11-10 17:51:29.356426: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d1000 of size 2048 next 153\n",
      "2022-11-10 17:51:29.356430: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d1800 of size 2048 next 154\n",
      "2022-11-10 17:51:29.356434: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d2000 of size 4096 next 156\n",
      "2022-11-10 17:51:29.356438: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d3000 of size 4096 next 157\n",
      "2022-11-10 17:51:29.356442: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d4000 of size 4096 next 158\n",
      "2022-11-10 17:51:29.356446: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d5000 of size 4096 next 160\n",
      "2022-11-10 17:51:29.356450: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d6000 of size 4096 next 161\n",
      "2022-11-10 17:51:29.356454: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d7000 of size 4096 next 162\n",
      "2022-11-10 17:51:29.356458: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d8000 of size 4096 next 164\n",
      "2022-11-10 17:51:29.356462: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444d9000 of size 4096 next 165\n",
      "2022-11-10 17:51:29.356466: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444da000 of size 4096 next 166\n",
      "2022-11-10 17:51:29.356470: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444db000 of size 2048 next 168\n",
      "2022-11-10 17:51:29.356474: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444db800 of size 2048 next 169\n",
      "2022-11-10 17:51:29.356478: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dc000 of size 2048 next 170\n",
      "2022-11-10 17:51:29.356481: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dc800 of size 2048 next 172\n",
      "2022-11-10 17:51:29.356485: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dd000 of size 2048 next 173\n",
      "2022-11-10 17:51:29.356489: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dd800 of size 2048 next 174\n",
      "2022-11-10 17:51:29.356493: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444de000 of size 1024 next 176\n",
      "2022-11-10 17:51:29.356497: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444de400 of size 1024 next 177\n",
      "2022-11-10 17:51:29.356501: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444de800 of size 1024 next 178\n",
      "2022-11-10 17:51:29.356505: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dec00 of size 3072 next 179\n",
      "2022-11-10 17:51:29.356509: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444df800 of size 256 next 180\n",
      "2022-11-10 17:51:29.356513: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444df900 of size 256 next 181\n",
      "2022-11-10 17:51:29.356517: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dfa00 of size 256 next 182\n",
      "2022-11-10 17:51:29.356521: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dfb00 of size 256 next 183\n",
      "2022-11-10 17:51:29.356525: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dfc00 of size 256 next 184\n",
      "2022-11-10 17:51:29.356529: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dfd00 of size 256 next 185\n",
      "2022-11-10 17:51:29.356533: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dfe00 of size 256 next 186\n",
      "2022-11-10 17:51:29.356537: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444dff00 of size 256 next 187\n",
      "2022-11-10 17:51:29.356541: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0000 of size 256 next 188\n",
      "2022-11-10 17:51:29.356545: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0100 of size 256 next 189\n",
      "2022-11-10 17:51:29.356549: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0200 of size 256 next 190\n",
      "2022-11-10 17:51:29.356553: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0300 of size 256 next 191\n",
      "2022-11-10 17:51:29.356557: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0400 of size 256 next 192\n",
      "2022-11-10 17:51:29.356561: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0500 of size 256 next 193\n",
      "2022-11-10 17:51:29.356565: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0600 of size 256 next 194\n",
      "2022-11-10 17:51:29.356568: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0700 of size 256 next 195\n",
      "2022-11-10 17:51:29.356572: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0800 of size 256 next 196\n",
      "2022-11-10 17:51:29.356576: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0900 of size 256 next 197\n",
      "2022-11-10 17:51:29.356580: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0a00 of size 256 next 198\n",
      "2022-11-10 17:51:29.356584: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0b00 of size 256 next 199\n",
      "2022-11-10 17:51:29.356588: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7f13444e0c00 of size 256 next 202\n",
      "2022-11-10 17:51:29.356592: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0d00 of size 256 next 204\n",
      "2022-11-10 17:51:29.356596: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e0e00 of size 1024 next 209\n",
      "2022-11-10 17:51:29.356600: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e1200 of size 1024 next 210\n",
      "2022-11-10 17:51:29.356604: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e1600 of size 1536 next 36\n",
      "2022-11-10 17:51:29.356608: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13444e1c00 of size 262144 next 35\n",
      "2022-11-10 17:51:29.356613: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344521c00 of size 1048576 next 45\n",
      "2022-11-10 17:51:29.356617: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344621c00 of size 524288 next 44\n",
      "2022-11-10 17:51:29.356621: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13446a1c00 of size 2097152 next 120\n",
      "2022-11-10 17:51:29.356625: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13448a1c00 of size 2097152 next 54\n",
      "2022-11-10 17:51:29.356629: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344aa1c00 of size 2097152 next 53\n",
      "2022-11-10 17:51:29.356633: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344ca1c00 of size 2097152 next 77\n",
      "2022-11-10 17:51:29.356637: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344ea1c00 of size 524288 next 91\n",
      "2022-11-10 17:51:29.356641: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344f21c00 of size 524288 next 85\n",
      "2022-11-10 17:51:29.356645: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1344fa1c00 of size 1968896 next 1\n",
      "2022-11-10 17:51:29.356649: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1345182700 of size 1280 next 2\n",
      "2022-11-10 17:51:29.356653: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1345182c00 of size 34884096 next 13\n",
      "2022-11-10 17:51:29.356658: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13472c7600 of size 4194304 next 70\n",
      "2022-11-10 17:51:29.356662: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13476c7600 of size 12582912 next 63\n",
      "2022-11-10 17:51:29.356666: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13482c7600 of size 8388608 next 62\n",
      "2022-11-10 17:51:29.356670: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1348ac7600 of size 4194304 next 127\n",
      "2022-11-10 17:51:29.356674: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1348ec7600 of size 524288 next 137\n",
      "2022-11-10 17:51:29.356678: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1348f47600 of size 262144 next 147\n",
      "2022-11-10 17:51:29.356682: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1348f87600 of size 524288 next 151\n",
      "2022-11-10 17:51:29.356686: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1349007600 of size 2097152 next 155\n",
      "2022-11-10 17:51:29.356690: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1349207600 of size 8388608 next 159\n",
      "2022-11-10 17:51:29.356694: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1349a07600 of size 4194304 next 163\n",
      "2022-11-10 17:51:29.356698: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1349e07600 of size 2097152 next 167\n",
      "2022-11-10 17:51:29.356702: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a007600 of size 1048576 next 171\n",
      "2022-11-10 17:51:29.356706: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a107600 of size 524288 next 175\n",
      "2022-11-10 17:51:29.356710: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a187600 of size 2457600 next 200\n",
      "2022-11-10 17:51:29.356714: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3df600 of size 12288 next 207\n",
      "2022-11-10 17:51:29.356718: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3e2600 of size 1024 next 211\n",
      "2022-11-10 17:51:29.356722: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3e2a00 of size 1024 next 214\n",
      "2022-11-10 17:51:29.356726: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3e2e00 of size 1024 next 215\n",
      "2022-11-10 17:51:29.356730: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3e3200 of size 2048 next 217\n",
      "2022-11-10 17:51:29.356734: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3e3a00 of size 2048 next 218\n",
      "2022-11-10 17:51:29.356738: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3e4200 of size 2048 next 219\n",
      "2022-11-10 17:51:29.356742: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3e4a00 of size 4096 next 221\n",
      "2022-11-10 17:51:29.356746: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3e5a00 of size 4096 next 222\n",
      "2022-11-10 17:51:29.356750: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3e6a00 of size 4096 next 223\n",
      "2022-11-10 17:51:29.356754: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7f134a3e7a00 of size 15360 next 201\n",
      "2022-11-10 17:51:29.356758: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3eb600 of size 12288 next 203\n",
      "2022-11-10 17:51:29.356762: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f134a3ee600 of size 209715200 next 205\n",
      "2022-11-10 17:51:29.356767: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1356bee600 of size 36864 next 206\n",
      "2022-11-10 17:51:29.356771: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f1356bf7600 of size 209715200 next 208\n",
      "2022-11-10 17:51:29.356775: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13633f7600 of size 209715200 next 29\n",
      "2022-11-10 17:51:29.356779: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f136fbf7600 of size 209715200 next 212\n",
      "2022-11-10 17:51:29.356783: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f137c3f7600 of size 419430400 next 37\n",
      "2022-11-10 17:51:29.356788: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13953f7600 of size 419430400 next 216\n",
      "2022-11-10 17:51:29.356792: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13ae3f7600 of size 838860800 next 46\n",
      "2022-11-10 17:51:29.356796: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f13e03f7600 of size 838860800 next 220\n",
      "2022-11-10 17:51:29.356800: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7f14123f7600 of size 834701824 next 18446744073709551615\n",
      "2022-11-10 17:51:29.356804: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: \n",
      "2022-11-10 17:51:29.356810: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 58 Chunks of size 256 totalling 14.5KiB\n",
      "2022-11-10 17:51:29.356815: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 35 Chunks of size 1024 totalling 35.0KiB\n",
      "2022-11-10 17:51:29.356821: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 1280 totalling 2.5KiB\n",
      "2022-11-10 17:51:29.356825: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 1536 totalling 1.5KiB\n",
      "2022-11-10 17:51:29.356830: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 35 Chunks of size 2048 totalling 70.0KiB\n",
      "2022-11-10 17:51:29.356835: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 5 Chunks of size 3072 totalling 15.0KiB\n",
      "2022-11-10 17:51:29.356839: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 3584 totalling 3.5KiB\n",
      "2022-11-10 17:51:29.356844: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 37 Chunks of size 4096 totalling 148.0KiB\n",
      "2022-11-10 17:51:29.356848: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 12288 totalling 24.0KiB\n",
      "2022-11-10 17:51:29.356853: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 27648 totalling 108.0KiB\n",
      "2022-11-10 17:51:29.356858: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 36864 totalling 72.0KiB\n",
      "2022-11-10 17:51:29.356862: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 262144 totalling 512.0KiB\n",
      "2022-11-10 17:51:29.356867: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 418816 totalling 409.0KiB\n",
      "2022-11-10 17:51:29.356872: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 523264 totalling 1022.0KiB\n",
      "2022-11-10 17:51:29.356876: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 6 Chunks of size 524288 totalling 3.00MiB\n",
      "2022-11-10 17:51:29.356881: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 697856 totalling 1.33MiB\n",
      "2022-11-10 17:51:29.356885: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 1048576 totalling 2.00MiB\n",
      "2022-11-10 17:51:29.356890: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 1836032 totalling 1.75MiB\n",
      "2022-11-10 17:51:29.356894: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 1968896 totalling 1.88MiB\n",
      "2022-11-10 17:51:29.356899: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 6 Chunks of size 2097152 totalling 12.00MiB\n",
      "2022-11-10 17:51:29.356903: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 2457600 totalling 2.34MiB\n",
      "2022-11-10 17:51:29.356908: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 4194304 totalling 12.00MiB\n",
      "2022-11-10 17:51:29.356912: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 8388608 totalling 16.00MiB\n",
      "2022-11-10 17:51:29.356917: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 12582912 totalling 12.00MiB\n",
      "2022-11-10 17:51:29.356922: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 34884096 totalling 33.27MiB\n",
      "2022-11-10 17:51:29.356926: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 209715200 totalling 800.00MiB\n",
      "2022-11-10 17:51:29.356931: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 419430400 totalling 800.00MiB\n",
      "2022-11-10 17:51:29.356935: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 838860800 totalling 1.56GiB\n",
      "2022-11-10 17:51:29.356940: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 3.22GiB\n",
      "2022-11-10 17:51:29.356944: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 4294967296 memory_limit_: 4294967296 available bytes: 0 curr_region_allocation_bytes_: 8589934592\n",
      "2022-11-10 17:51:29.356953: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: \n",
      "Limit:                      4294967296\n",
      "InUse:                      3460249856\n",
      "MaxInUse:                   3460249856\n",
      "NumAllocs:                         309\n",
      "MaxAllocSize:                838860800\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-11-10 17:51:29.356965: W tensorflow/core/common_runtime/bfc_allocator.cc:491] *********************************************************************************___________________\n",
      "2022-11-10 17:51:29.356989: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at fused_batch_norm_op.cc:872 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1024,1024,200,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m cp \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCP.kmod\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# trace = model.fit(x = x_train, y = y_train_extra, batch_size = BS, \u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#                   epochs=runLen, verbose=1, validation_split = 0.2,\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#                   shuffle=True, callbacks = [scheduler, cp])\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#using tf.data() pipeline instead of simply feeding in np arrays\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m trace \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrunLen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from network import Net\n",
    "# from network import bestNet as Net\n",
    "\n",
    "runLen = 128\n",
    "BS = 1024\n",
    "\n",
    "y_train_extra = tf.concat([y_train[:, :, None], ULUT_train], 2)\n",
    "\n",
    "def compact_loss(y_true_extra, y_pred):\n",
    "    #Here, we take in LUT as part of y_true that we disect inside this loss function\n",
    "    y_true = y_true_extra[:,:,0] #ahahahahahah messed up my indexing here!\n",
    "    ULUT = y_true_extra[:,:,1:]\n",
    "    \n",
    "    compact_dnn = tf.matmul(ULUT, y_pred[:,:,None])\n",
    "    compact_true = tf.matmul(ULUT, y_true[:,:,None])\n",
    "    loss_compact = (tf.math.reduce_mean(tf.math.abs(compact_dnn - compact_true), axis = 0)**2 )[:,0]    \n",
    "    loss_compact = tf.math.sqrt(loss_compact)\n",
    "    \n",
    "    print(compact_dnn)\n",
    "    print(y_true[:,:,None])\n",
    "    \n",
    "    #take average of compact loss and total loss to try and balance out training\n",
    "    loss_total = (tf.math.reduce_mean(tf.math.abs(y_pred[:,:,None] - y_true[:,:,None]), axis = 0)**2 )[:,0]\n",
    "    loss_total = tf.math.sqrt(loss_total)\n",
    "\n",
    "    loss = (loss_compact + loss_total) / 2\n",
    "#     loss = loss_compact\n",
    "#     loss = loss_compact*0.25 + loss_total*(0.75)    \n",
    "    return loss\n",
    "\n",
    "def scheduler(epoch, learning_rate):\n",
    "    part1 = runLen//4\n",
    "    part2 = 2*runLen//4\n",
    "    part3 = 3*runLen//4\n",
    "    if epoch < part1:\n",
    "        learning_rate = 0.0001\n",
    "        return learning_rate\n",
    "    if epoch >= part1 and epoch < part2:\n",
    "        learning_rate = 0.00005       \n",
    "        return learning_rate\n",
    "    if epoch >= part2 and epoch < part3:\n",
    "        learning_rate = 0.00001     \n",
    "        return learning_rate\n",
    "    if epoch >= part3:\n",
    "        learning_rate = 0.000001\n",
    "        return learning_rate\n",
    "\n",
    "model = Net() #comment out to re-train existing network\n",
    "model.compile(loss = compact_loss, optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005))\n",
    "scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(\"CP.kmod\", monitor = 'val_loss', save_best_only = True) \n",
    "\n",
    "# trace = model.fit(x = x_train, y = y_train_extra, batch_size = BS, \n",
    "#                   epochs=runLen, verbose=1, validation_split = 0.2,\n",
    "#                   shuffle=True, callbacks = [scheduler, cp])\n",
    "\n",
    "#using tf.data() pipeline instead of simply feeding in np arrays\n",
    "trace = model.fit(train_dataset, epochs=runLen, validation_data = val_dataset, \n",
    "                  verbose=1, callbacks = [scheduler, cp])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, ax0 = plt.subplots()\n",
    "ax0.plot(trace.history['loss'], '-')\n",
    "ax0.plot(trace.history['val_loss'], '-')\n",
    "ax0.legend(['train', 'val'], loc='upper left')\n",
    "ax0.set_xlabel('iteration')\n",
    "ax0.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ModelNet40 shadowed data\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_scan1_320k.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_scan2_320k.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_ground_truth_320k.npy\")\n",
    "\n",
    "#load 100pts KITTI skip3 data\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_scan1_100pts_skip3.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_scan2_100pts_skip3.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_ground_truth_100pts_skip3.npy\")\n",
    "\n",
    "#load 50pts KITTI noskip \n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091v2_scan1_50pts.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091v2_scan2_50pts.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091v2_ground_truth_50pts.npy\")\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071v2_scan1_50pts.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071v2_scan2_50pts.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071v2_ground_truth_50pts.npy\")\n",
    "\n",
    "#KITTI + KITTI-CARLA + ModelNet normal training data\n",
    "# #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_01_scan1_100pts.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_01_scan2_100pts.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_01_ground_truth_100pts.npy\")\n",
    "d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan1_100pts.npy\")\n",
    "d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan2_100pts.npy\")\n",
    "gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_ground_truth_100pts.npy\")\n",
    "\n",
    "# d1_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_03_scan1_100pts.npy\")\n",
    "# d2_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_03_scan2_100pts.npy\")\n",
    "# gt_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_03_ground_truth_100pts.npy\")\n",
    "# d1 = np.append(d1, d1_2, axis = 0)\n",
    "# d2 = np.append(d2, d2_2, axis = 0)\n",
    "# gt = np.append(gt, gt_2, axis = 0)\n",
    "\n",
    "# d1_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_scan1_100pts_skip3.npy\")\n",
    "# d2_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_scan2_100pts_skip3.npy\")\n",
    "# gt_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_ground_truth_100pts_skip3.npy\")\n",
    "# d1 = np.append(d1, d1_4, axis = 0)\n",
    "# d2 = np.append(d2, d2_4, axis = 0)\n",
    "# gt = np.append(gt, gt_4, axis = 0)\n",
    "\n",
    "# d1_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_scan1_320k.npy\")\n",
    "# d2_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_scan2_320k.npy\")\n",
    "# gt_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_ground_truth_320k.npy\")\n",
    "# d1 = np.append(d1, d1_5, axis = 0)\n",
    "# d2 = np.append(d2, d2_5, axis = 0)\n",
    "# gt = np.append(gt, gt_5, axis = 0)\n",
    "\n",
    "# # d1_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071_scan1_100pts.npy\")\n",
    "# # d2_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071_scan2_100pts.npy\")\n",
    "# # gt_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071_ground_truth_100pts.npy\")\n",
    "# d1_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0095_scan1_100pts_skip3.npy\")\n",
    "# d2_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0095_scan2_100pts_skip3.npy\")\n",
    "# gt_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0095_ground_truth_100pts_skip3.npy\")\n",
    "# d1 = np.append(d1, d1_3, axis = 0)\n",
    "# d2 = np.append(d2, d2_3, axis = 0)\n",
    "# gt = np.append(gt, gt_3, axis = 0)\n",
    "\n",
    "# d1_6 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan1_100pts_skip3.npy\")\n",
    "# d2_6 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan2_100pts_skip3.npy\")\n",
    "# gt_6 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_ground_truth_100pts_skip3.npy\")\n",
    "# d1 = np.append(d1, d1_6, axis = 0)\n",
    "# d2 = np.append(d2, d2_6, axis = 0)\n",
    "# gt = np.append(gt, gt_6, axis = 0)\n",
    "\n",
    "#forest (straight road)\n",
    "d1_7 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0027_scan1_100pts.npy\")\n",
    "d2_7 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0027_scan2_100pts.npy\")\n",
    "gt_7 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0027_ground_truth_100pts.npy\")\n",
    "d1 = np.append(d1, d1_7, axis = 0)\n",
    "d2 = np.append(d2, d2_7, axis = 0)\n",
    "gt = np.append(gt, gt_7, axis = 0)\n",
    "\n",
    "#forest (curved road)\n",
    "d1_7 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0028_scan1_100pts.npy\")\n",
    "d2_7 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0028_scan2_100pts.npy\")\n",
    "gt_7 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0028_ground_truth_100pts.npy\")\n",
    "d1 = np.append(d1, d1_7, axis = 0)\n",
    "d2 = np.append(d2, d2_7, axis = 0)\n",
    "gt = np.append(gt, gt_7, axis = 0)\n",
    "# #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan1_100pts.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan2_100pts.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_ground_truth_100pts.npy\")\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan1_100pts_skip3.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan2_100pts_skip3.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_ground_truth_100pts_skip3.npy\")\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0027_scan1_100pts.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0027_scan2_100pts.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0027_ground_truth_100pts.npy\")\n",
    "\n",
    "points_per_sample = 100 #100          #points sampled from each voxel\n",
    "tsplit = 0.8 #0.95                   #this fraction goes into training\n",
    "\n",
    "scan1 = np.reshape(d1, [-1, points_per_sample, 3])\n",
    "scan2 = np.reshape(d2, [-1, points_per_sample, 3])\n",
    "ntrain = int(tsplit*tf.shape(scan1)[0].numpy())\n",
    "\n",
    "#randomly shuffle train and test data _______________\n",
    "np.random.seed(10)\n",
    "randy = np.linspace(0,np.shape(gt)[0]-1,np.shape(gt)[0]).astype(int)\n",
    "np.random.shuffle(randy)\n",
    "scan1 = scan1[randy]\n",
    "scan2 = scan2[randy]\n",
    "gt = gt[randy]\n",
    "#____________________________________________________\n",
    "\n",
    "x_train = np.append(scan1[:ntrain], scan2[:ntrain], axis = 1)\n",
    "x_test = np.append(scan1[ntrain:], scan2[ntrain:], axis = 1)\n",
    "print(np.shape(x_train))\n",
    "# print(np.shape(x_test))\n",
    "\n",
    "y_train = gt[:ntrain] #for standard training/ test data\n",
    "y_test = gt[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load uniformly sampled ModelNet40 data\n",
    "points_per_sample = 100\n",
    "# x_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_ModelNet40_x_train.npy')\n",
    "# y_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_ModelNet40_y_train.npy')[:,:3]\n",
    "# x_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_ModelNet40_x_test.npy')\n",
    "# y_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_ModelNet40_y_test.npy')[:,:3]\n",
    "\n",
    "x_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_simple_ModelNet40_x_train.npy')\n",
    "y_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_simple_ModelNet40_y_train.npy')[:,:3]\n",
    "x_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_simple_ModelNet40_x_test.npy')\n",
    "y_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_simple_ModelNet40_y_test.npy')[:,:3]\n",
    "\n",
    "# x_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/single_ModelNet40_x_train.npy')\n",
    "# y_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/single_ModelNet40_y_train.npy')[:,:3]\n",
    "# x_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/single_ModelNet40_x_test.npy')\n",
    "# y_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/single_ModelNet40_y_test.npy')[:,:3]\n",
    "\n",
    "print(\"x_test\", np.shape(x_test))\n",
    "print(\"y_test\", np.shape(y_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "protective-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions to convert between spherical and cartesian coordinate representations\n",
    "def c2s(pts):\n",
    "    \"\"\" converts points from cartesian coordinates to spherical coordinates \"\"\"\n",
    "    r = tf.sqrt(pts[:,0]**2 + pts[:,1]**2 + pts[:,2]**2)\n",
    "    phi = tf.math.acos(pts[:,2]/r)\n",
    "    theta = tf.math.atan2(pts[:,1], pts[:,0])\n",
    "\n",
    "    out = tf.transpose(tf.Variable([r, theta, phi]))\n",
    "    return(out)\n",
    "def s2c(pts):\n",
    "    \"\"\"converts spherical -> cartesian\"\"\"\n",
    "\n",
    "    x = pts[:,0]*tf.math.sin(pts[:,2])*tf.math.cos(pts[:,1])\n",
    "    y = pts[:,0]*tf.math.sin(pts[:,2])*tf.math.sin(pts[:,1]) \n",
    "    z = pts[:,0]*tf.math.cos(pts[:,2])\n",
    "\n",
    "    out = tf.transpose(tf.Variable([x, y, z]))\n",
    "    # out = tf.Variable([x, y, z])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-burst",
   "metadata": {},
   "source": [
    "### Iterative solution for single test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "quiet-philip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2070\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "\n",
      " correct soln [-1.1873461   0.57330257 -0.02789416]\n",
      "\n",
      " estiamted soln: [[-1.12606668  0.56880295  0.056997  ]]\n",
      "\n",
      " error from DNN: [[-0.06127942  0.00449961 -0.08489116]]\n",
      "\n",
      " error in means [ 0.0005945  -0.10827035 -0.06485149]\n"
     ]
    }
   ],
   "source": [
    "n = int(np.floor(3000*np.random.rand()))\n",
    "# n = 100\n",
    "print(n)\n",
    "\n",
    "c1 = np.array([x_test[n,:points_per_sample,0], x_test[n,:points_per_sample,1], x_test[n,:points_per_sample,2]])\n",
    "c2 = np.array([x_test[n,points_per_sample:,0], x_test[n,points_per_sample:,1], x_test[n,points_per_sample:,2]])\n",
    "\n",
    "inputs = x_test[n][None,:]\n",
    "runlen = 1\n",
    "corr_sum = np.zeros([1,3]) #init var to store correction contributions\n",
    "for i in range(runlen):\n",
    "    correction = model.predict(inputs)[0] #show what the network thinks\n",
    "#     correction = correction*0.1 #for synthetic matab data only??\n",
    "#     correction = y_test[n] #show actual solution\n",
    "    corr_sum += correction\n",
    "    c1 = np.array([c1[0,:] + correction[0], c1[1,:] + correction[1], c1[2,:] + correction[2]])\n",
    "    inputs = np.append(c1, c2, axis = 1).T[None,:,:]\n",
    "\n",
    "print(\"\\n correct soln\", y_test[n])\n",
    "print(\"\\n estiamted soln:\", corr_sum)\n",
    "print(\"\\n error from DNN:\", y_test[n] - corr_sum)\n",
    "mean1 = np.mean(x_test[n,:points_per_sample], axis = 0)\n",
    "mean2 = np.mean(x_test[n,points_per_sample:], axis = 0)\n",
    "print(\"\\n error in means\",  y_test[n] + (mean1 - mean2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "active-liverpool",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14a0f63da804667a41f87692cb64638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=1043, layout=Layout(height='auto', width='100%'), width=1280)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use Vedo to plot inital and transformed point clouds in 3D \n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "\n",
    "plt1 = Plotter(N = 1, axes = 13, bg = (1, 1, 1), interactive = True)\n",
    "disp = []\n",
    "\n",
    "#draw scan1 \n",
    "# disp.append(Points(x_test[n,:points_per_sample].numpy(), c = 'green', r = 5))\n",
    "disp.append(Points(x_test[n,:points_per_sample], c = 'red', r = 10, alpha = 0.7))\n",
    "\n",
    "#draw initial scan2\n",
    "# disp.append(Points(x_test[n,points_per_sample:].numpy(), c = 'red', r = 5))\n",
    "# disp.append(Points(x_test[n,points_per_sample:], c = 'green', r = 10, alpha = 0.7))\n",
    "\n",
    "#draw scan2 corrected by DNN\n",
    "# disp.append(Points(x_test[n, points_per_sample:] - corr_sum, c = \"blue\", r =10, alpha = 0.7 ))\n",
    "\n",
    "#Draw arrow for ground truth soln vec\n",
    "# disp.append(Arrow(mean1 + y_test[n], mean1, c = 'y4', s = 0.002, res = 100)) #arbitrarily start arrow from scan1 center\n",
    "# disp.append(Arrow(mean2, mean2 - y_test[n], c = 'y4', s = 0.005, res = 100))\n",
    "\n",
    "#draw ground truth arrow cut short by U and L\n",
    "# soln_compact = tf.matmul(LUT[n], y_test[n][:, None])\n",
    "# soln_compact_xyz = tf.matmul(U[n], soln_compact)[:,0]\n",
    "# soln_compact_xyz = tf.matmul(tf.transpose(U[n]), soln_compact)[:,0]\n",
    "# print(\"soln_compact_xyz: \\n\", soln_compact_xyz)\n",
    "# disp.append(Arrow( mean2, mean2 - soln_compact_xyz, c = 'p4', s = 0.005, res = 100)) #arbitrarily start arrow from scan1 center\n",
    "\n",
    "#draw the set of 8 points that defined the voxel boundaries for the keyframe scan\n",
    "corn_cart = s2c(corn_test[n])\n",
    "# disp.append(Points(corn_cart, c = 'black', r = 10))\n",
    "\n",
    "#draw box instead of individual points______________________________\n",
    "p1, p2, p3, p4, p5, p6, p7, p8 = corn_cart.numpy()\n",
    "# print(p1)\n",
    "# print(p2)\n",
    "lineWidth = 2\n",
    "c1 = \"black\"\n",
    "\n",
    "arc1 = shapes.Line(p1, p2, c = c1, lw = lineWidth) \n",
    "disp.append(arc1)\n",
    "arc2 = shapes.Line(p3, p4, c = c1, lw = lineWidth) #debug\n",
    "disp.append(arc2)\n",
    "line1 = shapes.Line(p1, p3, c = c1, lw = lineWidth)\n",
    "disp.append(line1)\n",
    "line2 = shapes.Line(p2, p4, c = c1, lw = lineWidth) #problem here\n",
    "disp.append(line2)\n",
    "arc3 = shapes.Line(p5, p6, c = c1, lw = lineWidth) #debug\n",
    "disp.append(arc3)\n",
    "arc4 = shapes.Line(p7, p8, c = c1, lw = lineWidth) #debug\n",
    "disp.append(arc4)\n",
    "line3 = shapes.Line(p5, p7, c = c1, lw = lineWidth)\n",
    "disp.append(line3)\n",
    "line4 = shapes.Line(p6, p8, c = c1, lw = lineWidth)\n",
    "disp.append(line4)\n",
    "disp.append(shapes.Line(p1,p5, c = c1, lw = lineWidth))\n",
    "disp.append(shapes.Line(p2,p6, c = c1, lw = lineWidth))\n",
    "disp.append(shapes.Line(p3,p7, c = c1, lw = lineWidth))\n",
    "disp.append(shapes.Line(p4,p8, c = c1, lw = lineWidth))\n",
    "#_____________________________________________________________________\n",
    "\n",
    "#draw transformed scan2\n",
    "# disp.append(Points(c1, c = 'blue', r = 5))\n",
    "\n",
    "plt1.show(disp, \"Network Performance Test\")\n",
    "ViewInteractiveWidget(plt1.window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-christopher",
   "metadata": {},
   "source": [
    "### Run network on all test data \n",
    "# RUN TESTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(numToTest):\n",
    "c1 = np.array([x_test[:,:points_per_sample,0], x_test[:,:points_per_sample,1], x_test[:,:points_per_sample,2]])\n",
    "c2 = np.array([x_test[:,points_per_sample:,0], x_test[:,points_per_sample:,1], x_test[:,points_per_sample:,2]])\n",
    "c1 = np.transpose(c1, (1,2,0))\n",
    "c2 = np.transpose(c2, (1,2,0))\n",
    "\n",
    "inputs = x_test\n",
    "# print(\"c1\" , tf.shape(c1))\n",
    "print(\"x_test\" , tf.shape(x_test))\n",
    "print(\"y_test\" , tf.shape(y_test))\n",
    "\n",
    "runlen = 2\n",
    "corr_sum = np.zeros([tf.shape(x_test)[0].numpy(),1,3]) #init var to store correction contributions\n",
    "for i in range(runlen):\n",
    "    correction = model.predict(inputs)[:,None,:] #show what the network thinks\n",
    "#     correction = correction*0.1 #for synthetic matab data only??\n",
    "#     correction = y_test[n] #show actual solution\n",
    "    corr_sum += correction\n",
    "#     print(\"corr_sum\", tf.shape(corr_sum))\n",
    "    c1 += correction\n",
    "#     print(\"after correction\", tf.shape(c1))\n",
    "    inputs = np.append(c1, c2, axis = 1)#.T\n",
    "#     print(\"\\n new inputs\", tf.shape(inputs))\n",
    "    \n",
    "dnn_estimates = corr_sum[:,0,:]\n",
    "# print(\"\\n correct soln \\n\", y_test)\n",
    "# print(\"\\n estiamted soln: \\n\", dnn_estimates)\n",
    "# print(\"\\n error from DNN: \\n\", y_test - dnn_estimates)\n",
    "\n",
    "# print(\"\\n mean raw DNN error: \\n\", np.sqrt(np.sum(np.mean(np.abs(y_test - dnn_estimates), axis = 0)**2)))\n",
    "# print(\"\\n mean raw DNN error: \\n\", np.sqrt(np.mean(np.abs(y_test - dnn_estimates), axis = 0)**2)) #old\n",
    "print(\"\\n mean raw DNN error: \\n\", np.sqrt(np.mean((y_test - dnn_estimates)**2, axis = 0)))\n",
    "\n",
    "D2D_distance = np.mean(x_test[:,:points_per_sample], axis = 1) - np.mean(x_test[:,points_per_sample:], axis = 1)\n",
    "\n",
    "print(\"\\n test \\n\", y_test + D2D_distance)\n",
    "# print(tf.shape(y_test))\n",
    "# print(\"\\n mean raw D2D error \\n\", np.sqrt(np.sum(np.mean(np.abs(y_test + D2D_distance), axis = 0)**2 )))\n",
    "# print(\"\\n mean raw D2D error \\n\", np.sqrt(np.mean(np.abs(y_test + D2D_distance), axis = 0)**2 )) #old\n",
    "print(\"\\n mean raw D2D error \\n\", np.sqrt(np.mean((y_test + D2D_distance)**2, axis = 0) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c2867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of distribution of errors from DNN (is it even gaussian??)\n",
    "\n",
    "dnn_error = y_test - dnn_estimates\n",
    "D2D_error = y_test + D2D_distance\n",
    "mag_D2D = np.sqrt(D2D_error[:,0]**2 + D2D_error[:,1]**2 + D2D_error[:,2]**2)\n",
    "mag_DNN = np.sqrt(dnn_error[:,0]**2 + dnn_error[:,1]**2 + dnn_error[:,2]**2)\n",
    "\n",
    "#get rid of non-converging DNN solns\n",
    "goodidx = mag_DNN < 1\n",
    "mag_DNN = mag_DNN[goodidx]\n",
    "mag_D2D = mag_D2D[goodidx]\n",
    "print(len(dnn_error) - sum(goodidx), \"of\", len(dnn_error), \"test clouds did not converge\")\n",
    "# print(np.shape(goodidx))\n",
    "\n",
    "num_bins = 100\n",
    "\n",
    "#plot on same axis ---------------------------------------\n",
    "fig, ax = plt.subplots()\n",
    "# ax.set_title(\"x component of registration error\")\n",
    "# R = [-0.5,0.5] #range\n",
    "# ax.hist(dnn_error[:,0], num_bins, R, histtype='step', fill=False, color = (0,0,1), label = 'DNN (ours)')\n",
    "# ax.hist(D2D_error[:,0], num_bins, R, histtype='step', fill=False, color = (1,0,0), label = 'D2D')\n",
    "ax.set_title(\"Registration Error, KITTI data (skip 3)\")\n",
    "# ax.set_title(\"Registration Error, Shadowed ModelNet40 Dataset\")\n",
    "# ax.set_title(\"Registration Error, Uniformly Sampled ModelNet40 Dataset\")\n",
    "R = [0, 0.7]\n",
    "ax.hist(mag_DNN, num_bins, R, histtype='step', fill=False, color = (0,0,1), label = 'DNN (ours)')\n",
    "ax.hist(mag_D2D, num_bins, R, histtype='step', fill=False, color = (1,0,0), label = 'D2D')\n",
    "# ax.set_xlim([0,0.5])\n",
    "ax.set_ylabel('frequency')\n",
    "ax.set_xlabel('magnitude error (m)')\n",
    "ax.legend(loc = 'best')\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# #seprate plots------------------------------------------\n",
    "# fig, ax = plt.subplots(2,1)\n",
    "# ax[0].set_ylabel('frequency')\n",
    "# ax[0].set_xlabel('x component of translation error (m)')\n",
    "# ax[0].set_title('DNN (ours)')\n",
    "# ax[0].hist(dnn_error[:,0], num_bins);\n",
    "# ax[0].set_xlim([-0.5,0.5])\n",
    "\n",
    "# ax[1].set_title('D2D')\n",
    "# ax[1].set_ylabel('frequency')\n",
    "# ax[1].set_xlabel('x component of translation error (m)')\n",
    "# ax[1].hist(D2D_error[:,0], num_bins*3)\n",
    "# ax[1].set_xlim([-0.5,0.5])\n",
    "# #-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d7d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot relationship between error in D2D and error in DNN\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('Magnitude error D2D')\n",
    "# ax.set_xlabel('$\\Delta_{D2D}$') #nope\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylabel('Magnitude error DNN (ours)')\n",
    "ax.set_title(\"KITTI, skip3\")\n",
    "\n",
    "ax.scatter(mag_D2D, mag_DNN, alpha = 1, s = 0.2)\n",
    "\n",
    "c = np.cov(mag_D2D, mag_DNN)\n",
    "\n",
    "print(\"\\n mean magnitude D2D:\", np.mean(mag_D2D))\n",
    "print(\"\\n mean magnitude DNN\", np.mean(mag_DNN))\n",
    "print(\"\\n cov mag D2D:\", np.sqrt(c)[0,0])\n",
    "print(\"\\n cov mag DNN:\", np.sqrt(c)[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstrate using |D2D-DNN| as a monitor statistic\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('Actual D2D Error (unobservable)')\n",
    "ax.set_ylabel('Magnitude Difference Between D2D and DNN')\n",
    "# ax.set_xlabel('$\\Delta_{D2D}$') #nope\n",
    "# ax.set_xlim([0,1])\n",
    "ax.set_title(\"KITTI data\")\n",
    "\n",
    "# print(D2D_distance[:10])\n",
    "# print(dnn_estimates[:10])\n",
    "Diff= dnn_estimates  +  D2D_distance\n",
    "mag_Diff = np.sqrt(Diff[:,0]**2 + Diff[:,1]**2 + Diff[:,2]**2)\n",
    "\n",
    "D2D_error = y_test + D2D_distance\n",
    "mag_D2D = np.sqrt(D2D_error[:,0]**2 + D2D_error[:,1]**2 + D2D_error[:,2]**2)\n",
    "# print(y_test)\n",
    "# print(D2D_distance)\n",
    "# print(D2D_error)\n",
    "\n",
    "ax.scatter(mag_D2D, mag_Diff, alpha = 1, s = 0.2)\n",
    "# ax.plot(np.linspace(0,1,50),np.linspace(0,1,50), color = (1,0,0)) #plot y=x (for debug)\n",
    "\n",
    "\n",
    "c = np.cov(mag_D2D, mag_Diff)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider magnitude of error projected in compact directions\n",
    "\n",
    "#use LUT to get compact axis of DNN solution vec for each trial\n",
    "#was this\n",
    "# dnn_compact = tf.matmul(LUT, dnn_estimates[:,:,None])\n",
    "# dnn_compact_xyz = tf.matmul(U, dnn_compact)\n",
    "# truth_compact = tf.matmul(LUT, y_test[:,:,None])\n",
    "# truth_compact_xyz = tf.matmul(U, truth_compact) \n",
    "#new\n",
    "dnn_compact_xyz = tf.matmul(ULUT_test, dnn_estimates[:,:,None])\n",
    "# print(tf.shape(dnn_compact))\n",
    "truth_compact_xyz = tf.matmul(ULUT_test, y_test[:,:,None])\n",
    "# print(tf.shape(truth_compact_xyz))\n",
    "\n",
    "# # #old\n",
    "# #consider all\n",
    "# error_DNN_compact = np.sqrt( np.mean(np.abs(truth_compact_xyz - dnn_compact_xyz), axis = 0)**2 )[:,0] #wrong(?)\n",
    "# # ignore cases where DNN explodes...\n",
    "# # error_DNN_compact = np.sqrt( np.mean(np.abs(truth_compact_xyz[goodidx] - dnn_compact_xyz[goodidx]), axis = 0)**2 )[:,0]\n",
    "# print(\"\\n mean compact error DNN: \\n\", error_DNN_compact)\n",
    "# DNN_total = np.sqrt(error_DNN_compact[0]**2 + error_DNN_compact[1]**2 + error_DNN_compact[2]**2)\n",
    "# print(\"total mag DNN\", DNN_total)\n",
    "\n",
    "#RMSE\n",
    "d_dnn = (truth_compact_xyz - dnn_compact_xyz)[:,:,0] #get difference for each xyz component for each trial\n",
    "# print(d_dnn)\n",
    "mag_d_dnn = np.sqrt(d_dnn[:,0]**2 + d_dnn[:,1]**2 + d_dnn[:,2]**2) #get magnitude of error in solution\n",
    "# print(mag_d_dnn)\n",
    "# print(np.mean(mag_d_dnn))\n",
    "RMSE_DNN_compact = np.sqrt(np.mean(mag_d_dnn**2))\n",
    "print(\"\\n mean compact error DNN:\", RMSE_DNN_compact)\n",
    "\n",
    "# #old\n",
    "# # error_D2D_compact = np.sqrt(np.sum( np.mean(np.abs(truth_compact_xyz + d2d_compact_xyz), axis = 0)**2 ))\n",
    "# #for distrubution means distance\n",
    "# d2d_compact = tf.matmul(LUT, D2D_distance[:,:,None])\n",
    "# d2d_compact_xyz = tf.matmul(U, d2d_compact)\n",
    "# truth_compact = tf.matmul(LUT, y_test[:,:,None])\n",
    "# truth_compact_xyz = tf.matmul(U, truth_compact)\n",
    "# error_D2D_compact = np.sqrt(np.mean(np.abs(truth_compact_xyz + d2d_compact_xyz), axis = 0)**2 )[:,0]\n",
    "# # error_D2D_compact = np.sqrt(np.mean(np.abs(truth_compact_xyz[goodidx] + d2d_compact_xyz[goodidx]), axis = 0)**2 )[:,0] #ignore DNN exploding\n",
    "# print(\"\\n mean compact error D2D: \\n\", error_D2D_compact)\n",
    "# D2D_total = np.sqrt(error_D2D_compact[0]**2 + error_D2D_compact[1]**2 + error_D2D_compact[2]**2)\n",
    "# print(\"total mag D2D\", D2D_total)\n",
    "\n",
    "#RMSE\n",
    "d2d_compact_xyz = tf.matmul(ULUT_test, D2D_distance[:,:,None])\n",
    "d_d2d = (truth_compact_xyz + d2d_compact_xyz)[:,:,0]\n",
    "mag_d_d2d = np.sqrt(d_d2d[:,0]**2 + d_d2d[:,1]**2 + d_d2d[:,2]**2)\n",
    "RMSE_D2D_compact = np.sqrt(np.mean(mag_d_d2d**2))\n",
    "print(\"\\n mean compact error D2D:\", RMSE_D2D_compact)\n",
    "\n",
    "\n",
    "fig, ax  = plt.subplots()\n",
    "ax.scatter(mag_d_d2d, mag_d_dnn, s = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2c(corn_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 10\n",
    "print(y_train[I])\n",
    "print(ULUT_train[I] @ y_train[I][:,None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceade216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
