{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rental-ordinary",
   "metadata": {},
   "source": [
    "### Monte-Carlo sims for comparing D2D and DNN solution vectors\n",
    "\n",
    "\n",
    "\n",
    "#### TODO:\n",
    "\n",
    "\n",
    "Bring cartesian representation of corners back to training data generation file\n",
    "\n",
    "Make new network that has parallel channel (outside PointNet) for boundary coordinates\n",
    "\n",
    "Break test data into \"easy, medium, and hard\" scans with varying degrees of perspective shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacterial-nerve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import datetime\n",
    "\n",
    "#need to have these two lines to work on my ancient 1060 3gb\n",
    "#  https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "print(tf.__version__) #requires tensorflow 2.3\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "insured-phase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1471, 100, 3)\n",
      "(1472, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "#Load data directly from numpy files-- KEEP IN RAM (DO NOT CONVERT TO TF Tensor!!!)\n",
    "# #_________________________________________________________________\n",
    "# #load individual data numpy files\n",
    "# d1_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v1_scan1.npy\")\n",
    "# d2_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v1_scan2.npy\")\n",
    "# gt_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v1_ground_truth.npy\")\n",
    "# # d1_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v3_scan1.npy\")\n",
    "# # d2_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v3_scan2.npy\")\n",
    "# # gt_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v3_ground_truth.npy\")\n",
    "\n",
    "# # d1_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/scan1_300k_50_samples.npy\")\n",
    "# # d2_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/scan2_300k_50_samples.npy\")\n",
    "# # gt_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ground_truth_300k_50_samples.npy\")\n",
    "# # gt_1 = gt_1*0.1 #scale to match real-world data (vel-> pos)\n",
    "\n",
    "# # d1_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_scan1.npy\")\n",
    "# # d2_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_scan2.npy\")\n",
    "# # gt_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_ground_truth.npy\")\n",
    "# d1_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v4_scan1.npy\")\n",
    "# d2_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v4_scan2.npy\")\n",
    "# gt_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v4_ground_truth.npy\")\n",
    "\n",
    "\n",
    "# d1 = np.append(d1_1, d1_2, axis = 0)\n",
    "# d2 = np.append(d2_1, d2_2, axis = 0)\n",
    "# gt = np.append(gt_1, gt_2, axis = 0)\n",
    "# #_________________________________________________________________\n",
    "\n",
    "#Single datset\n",
    "# d1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_scan1.npy\")\n",
    "# d2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_scan2.npy\")\n",
    "# gt = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_ground_truth.npy\")\n",
    "\n",
    "# d1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/scan1_300k_50_samples.npy\")\n",
    "# d2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/scan2_300k_50_samples.npy\")\n",
    "# gt = np.load(\"C:/Users/Derm/Desktop/big/pshift/ground_truth_300k_50_samples.npy\")\n",
    "# gt = gt*0.1 #scale to match real-world data (vel-> pos)\n",
    "#_________________________________________________________________\n",
    "\n",
    "d1 = np.load(\"training_data/compact_scan1.npy\")\n",
    "d2 = np.load(\"training_data/compact_scan2.npy\")\n",
    "gt = np.load(\"training_data/compact_ground_truth.npy\")\n",
    "LUT = np.load(\"training_data/LUT.npy\")\n",
    "L = np.load(\"training_data/L.npy\")\n",
    "U = np.load(\"training_data/U.npy\")\n",
    "corn = np.load(\"training_data/corn.npy\")\n",
    "#_________________________________________________________________\n",
    "\n",
    "\n",
    "#reshape but don't convert to tensor\n",
    "points_per_sample = 50          #poitns sammpled from each voxel\n",
    "tsplit = 0.5 #0.95                   #this fraction goes into training\n",
    "\n",
    "scan1 = np.reshape(d1, [-1, points_per_sample, 3])\n",
    "scan2 = np.reshape(d2, [-1, points_per_sample, 3])\n",
    "ntrain = int(tsplit*tf.shape(scan1)[0].numpy())\n",
    "\n",
    "x_train = np.append(scan1[:ntrain], scan2[:ntrain], axis = 1)\n",
    "x_test = np.append(scan1[ntrain:], scan2[ntrain:], axis = 1)\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(x_test))\n",
    "\n",
    "y_train = gt[:ntrain] #for standard training/ test data\n",
    "y_test = gt[ntrain:]\n",
    "# y_train = gt[:ntrain][:,:,0] #when using compact data\n",
    "# y_test = gt[ntrain:][:,:,0]\n",
    "LUT = tf.convert_to_tensor(LUT)[ntrain:]\n",
    "U = tf.convert_to_tensor(U)[ntrain:]\n",
    "L = tf.convert_to_tensor(L)[ntrain:]\n",
    "corn_train = corn[:ntrain]\n",
    "corn_test = corn[ntrain:]\n",
    "\n",
    "# print(y_train)\n",
    "# print(np.shape(y_train))\n",
    "# print(np.shape(y_test))\n",
    "# print(np.shape(gt))\n",
    "# print(np.shape(corn))\n",
    "# print(corn)\n",
    "#-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "trained-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"FORDNetV3.kmod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "descending-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN = model.predict(x_test[:10])\n",
    "# print(DNN)\n",
    "# print(tf.shape(y_test))\n",
    "# print(tf.shape(x_test))\n",
    "# print(tf.shape(LUT))\n",
    "# print(tf.shape(U))\n",
    "# print(tf.shape(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "motivated-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions to convert between spherical and cartesian coordinate representations\n",
    "def c2s(pts):\n",
    "    \"\"\" converts points from cartesian coordinates to spherical coordinates \"\"\"\n",
    "    r = tf.sqrt(pts[:,0]**2 + pts[:,1]**2 + pts[:,2]**2)\n",
    "    phi = tf.math.acos(pts[:,2]/r)\n",
    "    theta = tf.math.atan2(pts[:,1], pts[:,0])\n",
    "\n",
    "    out = tf.transpose(tf.Variable([r, theta, phi]))\n",
    "    return(out)\n",
    "def s2c(pts):\n",
    "    \"\"\"converts spherical -> cartesian\"\"\"\n",
    "\n",
    "    x = pts[:,0]*tf.math.sin(pts[:,2])*tf.math.cos(pts[:,1])\n",
    "    y = pts[:,0]*tf.math.sin(pts[:,2])*tf.math.sin(pts[:,1]) \n",
    "    z = pts[:,0]*tf.math.cos(pts[:,2])\n",
    "\n",
    "    out = tf.transpose(tf.Variable([x, y, z]))\n",
    "    # out = tf.Variable([x, y, z])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-portugal",
   "metadata": {},
   "source": [
    "### Iterative solution for single test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "altered-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427\n",
      "\n",
      " correct soln [ 0.5703471   0.88483715 -0.00714883]\n",
      "\n",
      " estiamted soln: [[ 0.60412293  0.85561393 -0.00882648]]\n",
      "\n",
      " error from DNN: [[-0.03377586  0.02922322  0.00167765]]\n",
      "\n",
      " error in means [-0.01638865 -0.00734806 -0.03584408]\n"
     ]
    }
   ],
   "source": [
    "n = int(np.floor(500*np.random.rand()))\n",
    "# n = 0\n",
    "print(n)\n",
    "\n",
    "c1 = np.array([x_test[n,:points_per_sample,0], x_test[n,:points_per_sample,1], x_test[n,:points_per_sample,2]])\n",
    "c2 = np.array([x_test[n,points_per_sample:,0], x_test[n,points_per_sample:,1], x_test[n,points_per_sample:,2]])\n",
    "\n",
    "inputs = x_test[n][None,:]\n",
    "runlen = 10\n",
    "corr_sum = np.zeros([1,3]) #init var to store correction contributions\n",
    "for i in range(runlen):\n",
    "    correction = model.predict(inputs)[0] #show what the network thinks\n",
    "#     correction = correction*0.1 #for synthetic matab data only??\n",
    "#     correction = y_test[n] #show actual solution\n",
    "    corr_sum += correction\n",
    "    c1 = np.array([c1[0,:] + correction[0], c1[1,:] + correction[1], c1[2,:] + correction[2]])\n",
    "    inputs = np.append(c1, c2, axis = 1).T[None,:,:]\n",
    "\n",
    "print(\"\\n correct soln\", y_test[n])\n",
    "print(\"\\n estiamted soln:\", corr_sum)\n",
    "print(\"\\n error from DNN:\", y_test[n] - corr_sum)\n",
    "mean1 = np.mean(x_test[n,:points_per_sample], axis = 0)\n",
    "mean2 = np.mean(x_test[n,points_per_sample:], axis = 0)\n",
    "print(\"\\n error in means\",  y_test[n] + (mean1 - mean2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ranging-decision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0.5644437  -0.00287378 -0.03175014], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e75211f5634cb1a2b59b27d990d726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=960, layout=Layout(height='auto', width='100%'), width=960)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use Vedo to plot inital and transformed point clouds in 3D \n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "\n",
    "plt1 = Plotter(N = 1, axes = 13, bg = (1, 1, 1), interactive = True)\n",
    "disp = []\n",
    "\n",
    "#draw scan1 \n",
    "# disp.append(Points(x_test[n,:points_per_sample].numpy(), c = 'green', r = 5))\n",
    "disp.append(Points(x_test[n,:points_per_sample], c = 'green', r = 5))\n",
    "\n",
    "#draw initial scan2\n",
    "# disp.append(Points(x_test[n,points_per_sample:].numpy(), c = 'red', r = 5))\n",
    "disp.append(Points(x_test[n,points_per_sample:], c = 'red', r = 5))\n",
    "\n",
    "#Draw arrow for ground truth soln vec\n",
    "disp.append(Arrow(mean1, mean1 + y_test[n], c = 'r4', res = 50)) #arbitrarily start arrow from scan1 center\n",
    "\n",
    "#draw ground truth arrow cut short by U and L\n",
    "soln_compact = tf.matmul(LUT[n], y_test[n][:, None])\n",
    "soln_compact_xyz = tf.matmul(U[n], soln_compact)[:,0]\n",
    "print(soln_compact_xyz)\n",
    "disp.append(Arrow(mean1, mean1 + soln_compact_xyz, c = 'p4', res = 50)) #arbitrarily start arrow from scan1 center\n",
    "\n",
    "#draw the set of 8 points that defined the voxel boundaries for the keyframe scan\n",
    "corn_cart = s2c(corn_test[n])\n",
    "disp.append(Points(corn_cart, c = 'black', r = 10))\n",
    "\n",
    "#draw transformed scan2\n",
    "disp.append(Points(c1, c = 'blue', r = 5))\n",
    "\n",
    "plt1.show(disp, \"Network Performance Test\")\n",
    "ViewInteractiveWidget(plt1.window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-domestic",
   "metadata": {},
   "source": [
    "### Run network on all test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(numToTest):\n",
    "c1 = np.array([x_test[:,:points_per_sample,0], x_test[:,:points_per_sample,1], x_test[:,:points_per_sample,2]])\n",
    "c2 = np.array([x_test[:,points_per_sample:,0], x_test[:,points_per_sample:,1], x_test[:,points_per_sample:,2]])\n",
    "c1 = np.transpose(c1, (1,2,0))\n",
    "c2 = np.transpose(c2, (1,2,0))\n",
    "\n",
    "inputs = x_test\n",
    "# print(\"c1\" , tf.shape(c1))\n",
    "print(\"x_test\" , tf.shape(x_test))\n",
    "runlen = 10\n",
    "corr_sum = np.zeros([tf.shape(x_test)[0].numpy(),1,3]) #init var to store correction contributions\n",
    "for i in range(runlen):\n",
    "    correction = model.predict(inputs)[:,None,:] #show what the network thinks\n",
    "#     correction = correction*0.1 #for synthetic matab data only??\n",
    "#     correction = y_test[n] #show actual solution\n",
    "    corr_sum += correction\n",
    "#     print(\"corr_sum\", tf.shape(corr_sum))\n",
    "    c1 += correction\n",
    "#     print(\"after correction\", tf.shape(c1))\n",
    "    inputs = np.append(c1, c2, axis = 1)#.T\n",
    "#     print(\"\\n new inputs\", tf.shape(inputs))\n",
    "    \n",
    "dnn_estimates = corr_sum[:,0,:]\n",
    "print(\"\\n correct soln \\n\", y_test)\n",
    "print(\"\\n estiamted soln: \\n\", dnn_estimates)\n",
    "print(\"\\n error from DNN: \\n\", y_test - dnn_estimates)\n",
    "\n",
    "print(\"\\n mean raw DNN error: \\n\", np.sqrt(np.sum(np.mean(np.abs(y_test - dnn_estimates), axis = 0)**2)))\n",
    "\n",
    "D2D_distance = np.mean(x_test[:,:points_per_sample], axis = 1) - np.mean(x_test[:,points_per_sample:], axis = 1)\n",
    "# print(tf.shape(D2D_distance))\n",
    "# print(tf.shape(y_test))\n",
    "print(\"\\n mean raw D2D error \\n\", np.sqrt(np.sum(np.mean(np.abs(y_test + D2D_distance), axis = 0)**2 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just considering the residuals doesn't tell the whole story \n",
    "#   we need to consider error in COMPACT only directions\n",
    "\n",
    "#use LUT to get compact axis of DNN solution vec for each trial\n",
    "dnn_compact = tf.matmul(LUT, dnn_estimates[:,:,None])\n",
    "dnn_compact_xyz = tf.matmul(U, dnn_compact)\n",
    "\n",
    "#for distrubution means distance\n",
    "d2d_compact = tf.matmul(LUT, D2D_distance[:,:,None])\n",
    "d2d_compact_xyz = tf.matmul(U, d2d_compact)\n",
    "\n",
    "truth_compact = tf.matmul(LUT, y_test[:,:,None])\n",
    "truth_compact_xyz = tf.matmul(U, truth_compact)\n",
    "\n",
    "error_DNN_compact = np.sqrt(np.sum( np.mean(np.abs(truth_compact_xyz - dnn_compact_xyz), axis = 0)**2 ))\n",
    "print(\"\\n mean compact error DNN: \\n\", error_DNN_compact)\n",
    "\n",
    "error_D2D_compact = np.sqrt(np.sum( np.mean(np.abs(truth_compact_xyz + d2d_compact_xyz), axis = 0)**2 ))\n",
    "print(\"\\n mean compact error D2D: \\n\", error_D2D_compact)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-birthday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-richards",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
