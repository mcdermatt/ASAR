{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rental-ordinary",
   "metadata": {},
   "source": [
    "### Monte-Carlo sims for comparing D2D and DNN solution vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacterial-nerve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 17:05:13.221518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-30 17:05:13.318729: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-30 17:05:13.631550: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-30 17:05:13.631592: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-30 17:05:13.631596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 17:05:14.423873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 17:05:14.443655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 17:05:14.443790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#need to set this to allow eager execution within TF backend (i.e. my custom loss function)\n",
    "# tf.config.run_functions_eagerly(True) #debug\n",
    "# tf.data.experimental.enable_debug_mode() \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import datetime\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.sans-serif\": [\"Times\"],\n",
    "    \"font.size\": 12})\n",
    "\n",
    "#limit GPU memory ------------------------------------------------\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    memlim = 12*1024\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memlim)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "print(tf.__version__) #requires tensorflow 2.3\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"CP.kmod\") #temp\n",
    "# model = tf.keras.models.load_model(\"KITTICARLA100.kmod\") #best for KITTI CARLA\n",
    "# model = tf.keras.models.load_model(\"KITTINet100.kmod\") #best KITTI (trained on combined)\n",
    "# model = tf.keras.models.load_model(\"Net.kmod\") # best so far for shadowed ModelNet40 point clouds\n",
    "# model = tf.keras.models.load_model(\"SmallNet.kmod\") # best for uniform sampling dataset\n",
    "# model = tf.keras.models.load_model(\"combinedNet.kmod\") #KITTI + KITTI_CARLA + ModelNet40\n",
    "# model = tf.keras.models.load_model(\"KITTInet.kmod\") #50pts, 3cm MAE, trained a while ago..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "utility-burst",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 17:05:15.642213: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-30 17:05:15.642798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 17:05:15.642946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 17:05:15.643045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 17:05:16.002441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 17:05:16.002592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 17:05:16.002698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-30 17:05:16.002807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(941194, 200, 3)\n",
      "(49537, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "#load COMPACT test data\n",
    "# d1 = np.load(\"training_data/compact_scan1.npy\")\n",
    "# d2 = np.load(\"training_data/compact_scan2.npy\")\n",
    "# gt = np.load(\"training_data/ground_truth.npy\")\n",
    "# cgt = np.load(\"training_data/compact_ground_truth.npy\")\n",
    "# LUT = np.load(\"training_data/LUT.npy\")\n",
    "# L = np.load(\"training_data/L.npy\")\n",
    "# U = np.load(\"training_data/U.npy\")\n",
    "# corn = np.load(\"training_data/corn.npy\")\n",
    "\n",
    "d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_compact_scan1.npy\")\n",
    "d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_compact_scan2.npy\")\n",
    "gt = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_ground_truth.npy\")\n",
    "cgt = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_compact_ground_truth.npy\")\n",
    "LUT = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_LUT.npy\")\n",
    "L = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_L.npy\")\n",
    "U = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_U.npy\")\n",
    "corn = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0095_corn.npy\")\n",
    "\n",
    "d1_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_compact_scan1.npy\")\n",
    "d2_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_compact_scan2.npy\")\n",
    "gt_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_ground_truth.npy\")\n",
    "cgt_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_compact_ground_truth.npy\")\n",
    "LUT_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_LUT.npy\")\n",
    "L_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_L.npy\")\n",
    "U_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_U.npy\")\n",
    "corn_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown3_corn.npy\")\n",
    "d1 = np.append(d1, d1_3, axis = 0)\n",
    "d2 = np.append(d2, d2_3, axis = 0)\n",
    "gt = np.append(gt, gt_3, axis = 0)\n",
    "cgt = np.append(cgt, cgt_3, axis = 0)\n",
    "LUT = np.append(LUT, LUT_3, axis = 0)\n",
    "L = np.append(L, L_3, axis = 0)\n",
    "U = np.append(U, U_3, axis = 0)\n",
    "corn = np.append(corn, corn_3, axis = 0)\n",
    "\n",
    "d1_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_compact_scan1.npy\")\n",
    "d2_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_compact_scan2.npy\")\n",
    "gt_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_ground_truth.npy\")\n",
    "cgt_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_compact_ground_truth.npy\")\n",
    "LUT_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_LUT.npy\")\n",
    "L_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_L.npy\")\n",
    "U_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_U.npy\")\n",
    "corn_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/KCtown1_corn.npy\")\n",
    "d1 = np.append(d1, d1_4, axis = 0)\n",
    "d2 = np.append(d2, d2_4, axis = 0)\n",
    "gt = np.append(gt, gt_4, axis = 0)\n",
    "cgt = np.append(cgt, cgt_4, axis = 0)\n",
    "LUT = np.append(LUT, LUT_4, axis = 0)\n",
    "L = np.append(L, L_4, axis = 0)\n",
    "U = np.append(U, U_4, axis = 0)\n",
    "corn = np.append(corn, corn_4, axis = 0)\n",
    "\n",
    "\n",
    "d1_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_compact_scan1.npy\")\n",
    "d2_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_compact_scan2.npy\")\n",
    "gt_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_ground_truth.npy\")\n",
    "cgt_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_compact_ground_truth.npy\")\n",
    "LUT_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_LUT.npy\")\n",
    "L_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_L.npy\")\n",
    "U_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_U.npy\")\n",
    "corn_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/compact/0091_corn.npy\")\n",
    "d1 = np.append(d1, d1_2, axis = 0)\n",
    "d2 = np.append(d2, d2_2, axis = 0)\n",
    "gt = np.append(gt, gt_2, axis = 0)\n",
    "cgt = np.append(cgt, cgt_2, axis = 0)\n",
    "LUT = np.append(LUT, LUT_2, axis = 0)\n",
    "L = np.append(L, L_2, axis = 0)\n",
    "U = np.append(U, U_2, axis = 0)\n",
    "corn = np.append(corn, corn_2, axis = 0)\n",
    "\n",
    "#reshape but don't convert to tensor\n",
    "points_per_sample = 100          #poitns sammpled from each voxel\n",
    "tsplit = 0.95 #0.95                   #this fraction goes into training\n",
    "\n",
    "scan1 = np.reshape(d1, [-1, points_per_sample, 3])\n",
    "scan2 = np.reshape(d2, [-1, points_per_sample, 3])\n",
    "ntrain = int(tsplit*tf.shape(scan1)[0].numpy())\n",
    "\n",
    "x_train = np.append(scan1[:ntrain], scan2[:ntrain], axis = 1)\n",
    "x_test = np.append(scan1[ntrain:], scan2[ntrain:], axis = 1)\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(x_test))\n",
    "\n",
    "y_train = gt[:ntrain] #for standard training/ test data\n",
    "y_test = gt[ntrain:]\n",
    "\n",
    "# y_train = gt[:ntrain][:,:,0] #when using compact data\n",
    "# y_test = gt[ntrain:][:,:,0]\n",
    "LUT_train = tf.convert_to_tensor(LUT)[:ntrain] \n",
    "ULUT_train = tf.matmul(U[:ntrain], LUT_train)\n",
    "\n",
    "LUT = tf.convert_to_tensor(LUT)[ntrain:]\n",
    "ULUT_test = tf.matmul(U[ntrain:], LUT)\n",
    "\n",
    "\n",
    "U = tf.convert_to_tensor(U)[ntrain:]\n",
    "L = tf.convert_to_tensor(L)[ntrain:]\n",
    "corn_train = corn[:ntrain]\n",
    "corn_test = corn[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae6493",
   "metadata": {},
   "source": [
    "## Test training with custom loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed9fa253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 2.20885162e+01  1.00000000e+00 -1.21071935e-08  3.35276127e-08]\n",
      "  [ 2.36967678e+01 -1.21071935e-08  1.00000012e+00 -9.12696123e-08]\n",
      "  [ 2.00472736e+01  3.35276127e-08 -9.12696123e-08  1.00000000e+00]]\n",
      "\n",
      " [[ 2.14613533e+01  6.42186217e-03  7.70978332e-02 -2.08936632e-02]\n",
      "  [ 2.34263229e+01  7.70978332e-02  9.94017541e-01  1.62124634e-03]\n",
      "  [ 1.99718361e+01 -2.08936632e-02  1.62124634e-03  9.99560535e-01]]\n",
      "\n",
      " [[ 1.94473038e+01  1.19134076e-01 -3.23084861e-01 -2.36078538e-02]\n",
      "  [ 2.02481308e+01 -3.23084861e-01  8.81498694e-01 -8.65897536e-03]\n",
      "  [ 2.01528721e+01 -2.36078538e-02 -8.65897536e-03  9.99367356e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.98547344e+01  1.27949500e-02 -1.00213535e-01 -5.08771725e-02]\n",
      "  [ 2.23287716e+01 -1.00213535e-01  7.84899771e-01  3.98483872e-01]\n",
      "  [ 2.01498718e+01 -5.08771725e-02  3.98483872e-01  2.02305332e-01]]\n",
      "\n",
      " [[ 2.18559113e+01  3.27398215e-04  1.80485360e-02  1.24150433e-03]\n",
      "  [ 2.12410603e+01  1.80485360e-02  9.94964778e-01  6.84406236e-02]\n",
      "  [ 2.01422958e+01  1.24150433e-03  6.84406236e-02  4.70782351e-03]]\n",
      "\n",
      " [[ 1.99326668e+01  1.32033508e-02 -1.13792866e-01  8.95590987e-03]\n",
      "  [ 2.16622829e+01 -1.13792866e-01  9.80721951e-01 -7.71863610e-02]\n",
      "  [ 2.00066948e+01  8.95590987e-03 -7.71863610e-02  6.07484579e-03]]], shape=(941194, 3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[ 3.20829659e+01  1.00000000e+00 -1.21071935e-08  3.35276127e-08]\n",
      "  [ 3.36992226e+01 -1.21071935e-08  1.00000012e+00 -9.12696123e-08]\n",
      "  [ 3.00295563e+01  3.35276127e-08 -9.12696123e-08  1.00000000e+00]]\n",
      "\n",
      " [[ 3.14558029e+01  6.42186217e-03  7.70978332e-02 -2.08936632e-02]\n",
      "  [ 3.34287796e+01  7.70978332e-02  9.94017541e-01  1.62124634e-03]\n",
      "  [ 2.99541206e+01 -2.08936632e-02  1.62124634e-03  9.99560535e-01]]\n",
      "\n",
      " [[ 2.94417534e+01  1.19134076e-01 -3.23084861e-01 -2.36078538e-02]\n",
      "  [ 3.02505875e+01 -3.23084861e-01  8.81498694e-01 -8.65897536e-03]\n",
      "  [ 3.01351547e+01 -2.36078538e-02 -8.65897536e-03  9.99367356e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.98491821e+01  1.27949500e-02 -1.00213535e-01 -5.08771725e-02]\n",
      "  [ 3.23312263e+01 -1.00213535e-01  7.84899771e-01  3.98483872e-01]\n",
      "  [ 3.01321564e+01 -5.08771725e-02  3.98483872e-01  2.02305332e-01]]\n",
      "\n",
      " [[ 3.18503609e+01  3.27398215e-04  1.80485360e-02  1.24150433e-03]\n",
      "  [ 3.12435150e+01  1.80485360e-02  9.94964778e-01  6.84406236e-02]\n",
      "  [ 3.01245804e+01  1.24150433e-03  6.84406236e-02  4.70782351e-03]]\n",
      "\n",
      " [[ 2.99271164e+01  1.32033508e-02 -1.13792866e-01  8.95590987e-03]\n",
      "  [ 3.16647377e+01 -1.13792866e-01  9.80721951e-01 -7.71863610e-02]\n",
      "  [ 2.99889793e+01  8.95590987e-03 -7.71863610e-02  6.07484579e-03]]], shape=(941194, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_extra)\n",
    "# print(y_train_extra[:,:,0])\n",
    "\n",
    "shift_scale = .01\n",
    "shift = tf.cast(tf.concat([shift_scale*tf.random.normal([3])], axis=0) , tf.float32) + 10 #may need float64\n",
    "# print(shift)\n",
    "\n",
    "y_train_extra = tf.concat([(y_train_extra[:,:,0] + shift)[:,:,None], y_train_extra[:,:,1:] ], axis = -1)\n",
    "print(y_train_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "340cd13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-30 17:14:39.177653: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.10GiB (rounded to 2258865664)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-10-30 17:14:39.177693: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc\n",
      "2022-10-30 17:14:39.177705: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): \tTotal Chunks: 8, Chunks in use: 7. 2.0KiB allocated for chunks. 1.8KiB in use in bin. 44B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177712: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): \tTotal Chunks: 1, Chunks in use: 0. 512B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177720: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177726: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177731: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177737: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177742: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177748: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177753: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177759: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177764: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177772: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 580.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177778: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): \tTotal Chunks: 4, Chunks in use: 4. 6.80MiB allocated for chunks. 6.80MiB in use in bin. 6.80MiB client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177785: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 3.40MiB allocated for chunks. 3.40MiB in use in bin. 2.27MiB client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177790: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177798: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): \tTotal Chunks: 5, Chunks in use: 3. 57.26MiB allocated for chunks. 36.28MiB in use in bin. 32.31MiB client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177803: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177810: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): \tTotal Chunks: 8, Chunks in use: 8. 301.59MiB allocated for chunks. 301.59MiB in use in bin. 301.59MiB client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177817: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 1. 75.40MiB allocated for chunks. 75.40MiB in use in bin. 43.08MiB client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177825: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 167.66MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177833: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): \tTotal Chunks: 6, Chunks in use: 5. 11.40GiB allocated for chunks. 10.52GiB in use in bin. 10.52GiB client-requested in use in bin.\n",
      "2022-10-30 17:14:39.177839: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 2.10GiB was 256.00MiB, Chunk State: \n",
      "2022-10-30 17:14:39.177854: I tensorflow/core/common_runtime/bfc_allocator.cc:1062]   Size: 904.21MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 43.08MiB | Requested Size: 43.08MiB | in_use: 1 | bin_num: -1, next:   Size: 1.2KiB | Requested Size: 1.0KiB | in_use: 1 | bin_num: -1\n",
      "2022-10-30 17:14:39.177858: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 12884901888\n",
      "2022-10-30 17:14:39.177866: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc424000000 of size 1783552 next 7\n",
      "2022-10-30 17:14:39.177871: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc4241b3700 of size 1783552 next 8\n",
      "2022-10-30 17:14:39.177876: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc424366e00 of size 1783552 next 10\n",
      "2022-10-30 17:14:39.177881: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42451a500 of size 256 next 14\n",
      "2022-10-30 17:14:39.177885: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42451a600 of size 256 next 15\n",
      "2022-10-30 17:14:39.177890: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42451a700 of size 256 next 11\n",
      "2022-10-30 17:14:39.177894: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42451a800 of size 256 next 25\n",
      "2022-10-30 17:14:39.177899: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42451a900 of size 256 next 31\n",
      "2022-10-30 17:14:39.177903: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fc42451aa00 of size 256 next 28\n",
      "2022-10-30 17:14:39.177908: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42451ab00 of size 256 next 33\n",
      "2022-10-30 17:14:39.177912: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fc42451ac00 of size 512 next 26\n",
      "2022-10-30 17:14:39.177917: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42451ae00 of size 256 next 37\n",
      "2022-10-30 17:14:39.177921: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fc42451af00 of size 594688 next 16\n",
      "2022-10-30 17:14:39.177926: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc4245ac200 of size 3564800 next 39\n",
      "2022-10-30 17:14:39.177931: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fc424912700 of size 10699776 next 22\n",
      "2022-10-30 17:14:39.177936: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc425346b00 of size 15453952 next 3\n",
      "2022-10-30 17:14:39.177941: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc426203a00 of size 33883136 next 4\n",
      "2022-10-30 17:14:39.177947: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc428253e00 of size 33883136 next 5\n",
      "2022-10-30 17:14:39.177952: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42a2a4200 of size 1783552 next 6\n",
      "2022-10-30 17:14:39.177957: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42a457900 of size 11294464 next 12\n",
      "2022-10-30 17:14:39.177962: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42af1d000 of size 79060224 next 17\n",
      "2022-10-30 17:14:39.177967: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc42fa82d00 of size 45177344 next 19\n",
      "2022-10-30 17:14:39.177971: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fc432598700 of size 948129024 next 1\n",
      "2022-10-30 17:14:39.177977: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc46adcd400 of size 1280 next 2\n",
      "2022-10-30 17:14:39.177982: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc46adcd900 of size 2258865664 next 24\n",
      "2022-10-30 17:14:39.177987: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc4f1806700 of size 2258865664 next 9\n",
      "2022-10-30 17:14:39.177991: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc57823f500 of size 2258865664 next 18\n",
      "2022-10-30 17:14:39.177996: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc5fec78300 of size 2258865664 next 20\n",
      "2022-10-30 17:14:39.178001: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc6856b1100 of size 2258865664 next 21\n",
      "2022-10-30 17:14:39.178005: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc70c0e9f00 of size 45177344 next 23\n",
      "2022-10-30 17:14:39.178010: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc70ebff900 of size 33883136 next 27\n",
      "2022-10-30 17:14:39.178014: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc710c4fd00 of size 11294464 next 29\n",
      "2022-10-30 17:14:39.178019: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc711715400 of size 33883136 next 30\n",
      "2022-10-30 17:14:39.178023: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc713765800 of size 45177344 next 36\n",
      "2022-10-30 17:14:39.178028: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fc71627b200 of size 11294464 next 34\n",
      "2022-10-30 17:14:39.178033: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7fc716d40900 of size 45177344 next 13\n",
      "2022-10-30 17:14:39.178037: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7fc719856300 of size 175807744 next 18446744073709551615\n",
      "2022-10-30 17:14:39.178042: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: \n",
      "2022-10-30 17:14:39.178049: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 7 Chunks of size 256 totalling 1.8KiB\n",
      "2022-10-30 17:14:39.178054: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-10-30 17:14:39.178060: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 1783552 totalling 6.80MiB\n",
      "2022-10-30 17:14:39.178065: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 3564800 totalling 3.40MiB\n",
      "2022-10-30 17:14:39.178070: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 11294464 totalling 21.54MiB\n",
      "2022-10-30 17:14:39.178076: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 15453952 totalling 14.74MiB\n",
      "2022-10-30 17:14:39.178081: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 33883136 totalling 129.25MiB\n",
      "2022-10-30 17:14:39.178086: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 4 Chunks of size 45177344 totalling 172.34MiB\n",
      "2022-10-30 17:14:39.178092: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 79060224 totalling 75.40MiB\n",
      "2022-10-30 17:14:39.178108: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 5 Chunks of size 2258865664 totalling 10.52GiB\n",
      "2022-10-30 17:14:39.178115: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 10.93GiB\n",
      "2022-10-30 17:14:39.178120: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 12884901888 memory_limit_: 12884901888 available bytes: 0 curr_region_allocation_bytes_: 25769803776\n",
      "2022-10-30 17:14:39.178130: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: \n",
      "Limit:                     12884901888\n",
      "InUse:                     11738375424\n",
      "MaxInUse:                  11748483072\n",
      "NumAllocs:                         118\n",
      "MaxAllocSize:               2258865664\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-10-30 17:14:39.178139: W tensorflow/core/common_runtime/bfc_allocator.cc:491] **_______******************************************************************************************_\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m y_test_extra \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([y_test[:, :, \u001b[38;5;28;01mNone\u001b[39;00m], ULUT_test], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# y_test_extra = np.append(y_test[:, :, None], ULUT_test, 2)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_extra\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;28mlen\u001b[39m(x_train))\u001b[38;5;241m.\u001b[39mmap(augment)\u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    738\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4708\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m   4706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4707\u001b[0m   \u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4708\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4709\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m   4710\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:126\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 126\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1629\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1631\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1634\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1635\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1638\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1641\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "def augment(points, gt):\n",
    "    # jitter points\n",
    "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=tf.float32)\n",
    "\n",
    "    #shuffle and randomly translate cloud 2 and ground truth\n",
    "    shift_scale = 0.1 #don't want to make this too large for PC to reach in 1 iter(?)\n",
    "    shift = tf.cast(tf.concat([shift_scale*tf.random.normal([3])], axis=0) , tf.float32) #may need float64\n",
    "    points = tf.concat([tf.random.shuffle(points[:100]), tf.random.shuffle(points[100:]) + shift[:3]], axis = 0)\n",
    "    \n",
    "    #nope\n",
    "#     gt = gt.numpy()\n",
    "#     gt = tf.convert_to_tensor(gt, dtype=tf.float32) #slow?\n",
    "#     print(type(gt))\n",
    "#     gt[:,0] += shift #also nope\n",
    "\n",
    "#     gt = tf.concat([gt[:,:,0] + shift, gt[:,:,1:] ])\n",
    "    gt = tf.concat([(gt[:,:,0] + shift)[:,:,None], gt[:,:,1:] ], axis = -1)\n",
    "    \n",
    "    #no shift\n",
    "#     points = tf.concat([tf.random.shuffle(points[:100]), tf.random.shuffle(points[100:])], axis = 0)\n",
    "    \n",
    "    return points, gt\n",
    "\n",
    "BATCH_SIZE =  1024\n",
    "\n",
    "y_train_extra = tf.concat([y_train[:, :, None], ULUT_train], 2)\n",
    "# y_train_extra = np.append(y_train[:, :, None], ULUT_train, 2)\n",
    "y_test_extra = tf.concat([y_test[:, :, None], ULUT_test], 2)\n",
    "# y_test_extra = np.append(y_test[:, :, None], ULUT_test, 2)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_extra))\n",
    "train_dataset = train_dataset.shuffle(len(x_train)).map(augment).batch(BATCH_SIZE)\n",
    "print(train_dataset)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test_extra))\n",
    "val_dataset = val_dataset.shuffle(len(x_test)).map(augment).batch(BATCH_SIZE)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdaa28e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from network import Net\n",
    "\n",
    "runLen = 8\n",
    "BS = 1024\n",
    "\n",
    "y_train_extra = tf.concat([y_train[:, :, None], ULUT_train], 2)\n",
    "\n",
    "def compact_loss(y_true_extra, y_pred):\n",
    "    #Here, we take in LUT as part of y_true that we disect inside this loss function\n",
    "    y_true = y_true_extra[:,:,0] #ahahahahahah messed up my indexing here!\n",
    "    ULUT = y_true_extra[:,:,1:]\n",
    "    \n",
    "    compact_dnn = tf.matmul(ULUT, y_pred[:,:,None])\n",
    "    compact_true = tf.matmul(ULUT, y_true[:,:,None])\n",
    "    loss_compact = (tf.math.reduce_mean(tf.math.abs(compact_dnn - compact_true), axis = 0)**2 )[:,0]    \n",
    "    loss_compact = tf.math.sqrt(loss_compact)\n",
    "    \n",
    "    print(compact_dnn)\n",
    "    print(y_true[:,:,None])\n",
    "    \n",
    "    #take average of compact loss and total loss to try and balance out training\n",
    "    loss_total = (tf.math.reduce_mean(tf.math.abs(y_pred[:,:,None] - y_true[:,:,None]), axis = 0)**2 )[:,0]\n",
    "    loss_total = tf.math.sqrt(loss_total)\n",
    "    loss = (loss_compact + loss_total) / 2\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def scheduler(epoch, learning_rate):\n",
    "    part1 = runLen//4\n",
    "    part2 = 2*runLen//4\n",
    "    part3 = 3*runLen//4\n",
    "    if epoch < part1:\n",
    "        learning_rate = 0.0005\n",
    "        return learning_rate\n",
    "    if epoch >= part1 and epoch < part2:\n",
    "        learning_rate = 0.0001       \n",
    "        return learning_rate\n",
    "    if epoch >= part2 and epoch < part3:\n",
    "        learning_rate = 0.00005     \n",
    "        return learning_rate\n",
    "    if epoch >= part3:\n",
    "        learning_rate = 0.00001\n",
    "        return learning_rate\n",
    "\n",
    "model = Net() #comment out to re-train existing network\n",
    "model.compile(loss = compact_loss, optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005))\n",
    "scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(\"CP.kmod\", monitor = 'val_loss', save_best_only = True) \n",
    "\n",
    "# trace = model.fit(x = x_train, y = y_train_extra, batch_size = BS, \n",
    "#                   epochs=runLen, verbose=1, validation_split = 0.2,\n",
    "#                   shuffle=True, callbacks = [scheduler, cp])\n",
    "\n",
    "#using tf.data() pipeline instead of simply feeding in np arrays\n",
    "trace = model.fit(train_dataset, epochs=runLen, validation_data = val_dataset, \n",
    "                  verbose=1, callbacks = [scheduler, cp])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, ax0 = plt.subplots()\n",
    "ax0.plot(trace.history['loss'], '-')\n",
    "ax0.plot(trace.history['val_loss'], '-')\n",
    "ax0.legend(['train', 'val'], loc='upper left')\n",
    "ax0.set_xlabel('iteration')\n",
    "ax0.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ModelNet40 shadowed data\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_scan1_320k.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_scan2_320k.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_ground_truth_320k.npy\")\n",
    "\n",
    "#load 100pts KITTI skip3 data\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_scan1_100pts_skip3.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_scan2_100pts_skip3.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_ground_truth_100pts_skip3.npy\")\n",
    "\n",
    "#load 50pts KITTI noskip \n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091v2_scan1_50pts.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091v2_scan2_50pts.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091v2_ground_truth_50pts.npy\")\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071v2_scan1_50pts.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071v2_scan2_50pts.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071v2_ground_truth_50pts.npy\")\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_01_scan1_100pts.npy\")\n",
    "d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_01_scan2_100pts.npy\")\n",
    "gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_01_ground_truth_100pts.npy\")\n",
    "\n",
    "d1_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_03_scan1_100pts.npy\")\n",
    "d2_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_03_scan2_100pts.npy\")\n",
    "gt_2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_CARLA_03_ground_truth_100pts.npy\")\n",
    "d1 = np.append(d1, d1_2, axis = 0)\n",
    "d2 = np.append(d2, d2_2, axis = 0)\n",
    "gt = np.append(gt, gt_2, axis = 0)\n",
    "\n",
    "d1_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_scan1_100pts_skip3.npy\")\n",
    "d2_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_scan2_100pts_skip3.npy\")\n",
    "gt_4 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0091_ground_truth_100pts_skip3.npy\")\n",
    "d1 = np.append(d1, d1_4, axis = 0)\n",
    "d2 = np.append(d2, d2_4, axis = 0)\n",
    "gt = np.append(gt, gt_4, axis = 0)\n",
    "\n",
    "d1_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_scan1_320k.npy\")\n",
    "d2_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_scan2_320k.npy\")\n",
    "gt_5 = np.load(\"/media/derm/06EF-127D1/TrainingData/ModelNet40/100pts_ground_truth_320k.npy\")\n",
    "d1 = np.append(d1, d1_5, axis = 0)\n",
    "d2 = np.append(d2, d2_5, axis = 0)\n",
    "gt = np.append(gt, gt_5, axis = 0)\n",
    "\n",
    "# d1_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071_scan1_100pts.npy\")\n",
    "# d2_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071_scan2_100pts.npy\")\n",
    "# gt_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0071_ground_truth_100pts.npy\")\n",
    "d1_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0095_scan1_100pts_skip3.npy\")\n",
    "d2_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0095_scan2_100pts_skip3.npy\")\n",
    "gt_3 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0095_ground_truth_100pts_skip3.npy\")\n",
    "d1 = np.append(d1, d1_3, axis = 0)\n",
    "d2 = np.append(d2, d2_3, axis = 0)\n",
    "gt = np.append(gt, gt_3, axis = 0)\n",
    "\n",
    "d1_6 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan1_100pts_skip3.npy\")\n",
    "d2_6 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan2_100pts_skip3.npy\")\n",
    "gt_6 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_ground_truth_100pts_skip3.npy\")\n",
    "d1 = np.append(d1, d1_6, axis = 0)\n",
    "d2 = np.append(d2, d2_6, axis = 0)\n",
    "gt = np.append(gt, gt_6, axis = 0)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan1_100pts.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan2_100pts.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_ground_truth_100pts.npy\")\n",
    "# d1 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan1_100pts_skip3.npy\")\n",
    "# d2 = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_scan2_100pts_skip3.npy\")\n",
    "# gt = np.load(\"/media/derm/06EF-127D1/TrainingData/KITTI_0005_ground_truth_100pts_skip3.npy\")\n",
    "\n",
    "points_per_sample = 100 #100          #points sampled from each voxel\n",
    "tsplit = 0.95                   #this fraction goes into training\n",
    "\n",
    "scan1 = np.reshape(d1, [-1, points_per_sample, 3])\n",
    "scan2 = np.reshape(d2, [-1, points_per_sample, 3])\n",
    "ntrain = int(tsplit*tf.shape(scan1)[0].numpy())\n",
    "\n",
    "#randomly shuffle train and test data _______________\n",
    "np.random.seed(10)\n",
    "randy = np.linspace(0,np.shape(gt)[0]-1,np.shape(gt)[0]).astype(int)\n",
    "np.random.shuffle(randy)\n",
    "scan1 = scan1[randy]\n",
    "scan2 = scan2[randy]\n",
    "gt = gt[randy]\n",
    "#____________________________________________________\n",
    "\n",
    "x_train = np.append(scan1[:ntrain], scan2[:ntrain], axis = 1)\n",
    "x_test = np.append(scan1[ntrain:], scan2[ntrain:], axis = 1)\n",
    "print(np.shape(x_train))\n",
    "# print(np.shape(x_test))\n",
    "\n",
    "y_train = gt[:ntrain] #for standard training/ test data\n",
    "y_test = gt[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load uniformly sampled ModelNet40 data\n",
    "points_per_sample = 100\n",
    "# x_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_ModelNet40_x_train.npy')\n",
    "# y_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_ModelNet40_y_train.npy')[:,:3]\n",
    "# x_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_ModelNet40_x_test.npy')\n",
    "# y_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_ModelNet40_y_test.npy')[:,:3]\n",
    "\n",
    "x_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_simple_ModelNet40_x_train.npy')\n",
    "y_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_simple_ModelNet40_y_train.npy')[:,:3]\n",
    "x_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_simple_ModelNet40_x_test.npy')\n",
    "y_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/full_simple_ModelNet40_y_test.npy')[:,:3]\n",
    "\n",
    "# x_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/single_ModelNet40_x_train.npy')\n",
    "# y_train = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/single_ModelNet40_y_train.npy')[:,:3]\n",
    "# x_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/single_ModelNet40_x_test.npy')\n",
    "# y_test = np.load('/media/derm/06EF-127D1/TrainingData/ModelNet40/single_ModelNet40_y_test.npy')[:,:3]\n",
    "\n",
    "print(\"x_test\", np.shape(x_test))\n",
    "print(\"y_test\", np.shape(y_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions to convert between spherical and cartesian coordinate representations\n",
    "def c2s(pts):\n",
    "    \"\"\" converts points from cartesian coordinates to spherical coordinates \"\"\"\n",
    "    r = tf.sqrt(pts[:,0]**2 + pts[:,1]**2 + pts[:,2]**2)\n",
    "    phi = tf.math.acos(pts[:,2]/r)\n",
    "    theta = tf.math.atan2(pts[:,1], pts[:,0])\n",
    "\n",
    "    out = tf.transpose(tf.Variable([r, theta, phi]))\n",
    "    return(out)\n",
    "def s2c(pts):\n",
    "    \"\"\"converts spherical -> cartesian\"\"\"\n",
    "\n",
    "    x = pts[:,0]*tf.math.sin(pts[:,2])*tf.math.cos(pts[:,1])\n",
    "    y = pts[:,0]*tf.math.sin(pts[:,2])*tf.math.sin(pts[:,1]) \n",
    "    z = pts[:,0]*tf.math.cos(pts[:,2])\n",
    "\n",
    "    out = tf.transpose(tf.Variable([x, y, z]))\n",
    "    # out = tf.Variable([x, y, z])\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-burst",
   "metadata": {},
   "source": [
    "### Iterative solution for single test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(np.floor(300*np.random.rand()))\n",
    "# n = 100\n",
    "print(n)\n",
    "\n",
    "c1 = np.array([x_test[n,:points_per_sample,0], x_test[n,:points_per_sample,1], x_test[n,:points_per_sample,2]])\n",
    "c2 = np.array([x_test[n,points_per_sample:,0], x_test[n,points_per_sample:,1], x_test[n,points_per_sample:,2]])\n",
    "\n",
    "inputs = x_test[n][None,:]\n",
    "runlen = 1\n",
    "corr_sum = np.zeros([1,3]) #init var to store correction contributions\n",
    "for i in range(runlen):\n",
    "    correction = model.predict(inputs)[0] #show what the network thinks\n",
    "#     correction = correction*0.1 #for synthetic matab data only??\n",
    "#     correction = y_test[n] #show actual solution\n",
    "    corr_sum += correction\n",
    "    c1 = np.array([c1[0,:] + correction[0], c1[1,:] + correction[1], c1[2,:] + correction[2]])\n",
    "    inputs = np.append(c1, c2, axis = 1).T[None,:,:]\n",
    "\n",
    "print(\"\\n correct soln\", y_test[n])\n",
    "print(\"\\n estiamted soln:\", corr_sum)\n",
    "print(\"\\n error from DNN:\", y_test[n] - corr_sum)\n",
    "mean1 = np.mean(x_test[n,:points_per_sample], axis = 0)\n",
    "mean2 = np.mean(x_test[n,points_per_sample:], axis = 0)\n",
    "print(\"\\n error in means\",  y_test[n] + (mean1 - mean2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Vedo to plot inital and transformed point clouds in 3D \n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "\n",
    "plt1 = Plotter(N = 1, axes = 13, bg = (1, 1, 1), interactive = True)\n",
    "disp = []\n",
    "\n",
    "#draw scan1 \n",
    "# disp.append(Points(x_test[n,:points_per_sample].numpy(), c = 'green', r = 5))\n",
    "disp.append(Points(x_test[n,:points_per_sample], c = 'red', r = 10, alpha = 0.7))\n",
    "\n",
    "#draw initial scan2\n",
    "# disp.append(Points(x_test[n,points_per_sample:].numpy(), c = 'red', r = 5))\n",
    "disp.append(Points(x_test[n,points_per_sample:], c = 'blue', r = 10, alpha = 0.7))\n",
    "\n",
    "#Draw arrow for ground truth soln vec\n",
    "# disp.append(Arrow(mean1 + y_test[n], mean1, c = 'y4', s = 0.002, res = 100)) #arbitrarily start arrow from scan1 center\n",
    "disp.append(Arrow(mean2, mean2 - y_test[n], c = 'y4', s = 0.005, res = 100))\n",
    "\n",
    "#draw ground truth arrow cut short by U and L\n",
    "soln_compact = tf.matmul(LUT[n], y_test[n][:, None])\n",
    "soln_compact_xyz = tf.matmul(U[n], soln_compact)[:,0]\n",
    "soln_compact_xyz = tf.matmul(tf.transpose(U[n]), soln_compact)[:,0]\n",
    "print(\"soln_compact_xyz: \\n\", soln_compact_xyz)\n",
    "disp.append(Arrow( mean2, mean2 - soln_compact_xyz, c = 'p4', s = 0.005, res = 100)) #arbitrarily start arrow from scan1 center\n",
    "\n",
    "#draw the set of 8 points that defined the voxel boundaries for the keyframe scan\n",
    "corn_cart = s2c(corn_test[n])\n",
    "# disp.append(Points(corn_cart, c = 'black', r = 10))\n",
    "\n",
    "#draw box instead of individual points______________________________\n",
    "p1, p2, p3, p4, p5, p6, p7, p8 = corn_cart.numpy()\n",
    "# print(p1)\n",
    "# print(p2)\n",
    "lineWidth = 2\n",
    "c1 = \"black\"\n",
    "\n",
    "arc1 = shapes.Line(p1, p2, c = c1, lw = lineWidth) \n",
    "disp.append(arc1)\n",
    "arc2 = shapes.Line(p3, p4, c = c1, lw = lineWidth) #debug\n",
    "disp.append(arc2)\n",
    "line1 = shapes.Line(p1, p3, c = c1, lw = lineWidth)\n",
    "disp.append(line1)\n",
    "line2 = shapes.Line(p2, p4, c = c1, lw = lineWidth) #problem here\n",
    "disp.append(line2)\n",
    "arc3 = shapes.Line(p5, p6, c = c1, lw = lineWidth) #debug\n",
    "disp.append(arc3)\n",
    "arc4 = shapes.Line(p7, p8, c = c1, lw = lineWidth) #debug\n",
    "disp.append(arc4)\n",
    "line3 = shapes.Line(p5, p7, c = c1, lw = lineWidth)\n",
    "disp.append(line3)\n",
    "line4 = shapes.Line(p6, p8, c = c1, lw = lineWidth)\n",
    "disp.append(line4)\n",
    "disp.append(shapes.Line(p1,p5, c = c1, lw = lineWidth))\n",
    "disp.append(shapes.Line(p2,p6, c = c1, lw = lineWidth))\n",
    "disp.append(shapes.Line(p3,p7, c = c1, lw = lineWidth))\n",
    "disp.append(shapes.Line(p4,p8, c = c1, lw = lineWidth))\n",
    "#_____________________________________________________________________\n",
    "\n",
    "#draw transformed scan2\n",
    "# disp.append(Points(c1, c = 'blue', r = 5))\n",
    "\n",
    "plt1.show(disp, \"Network Performance Test\")\n",
    "ViewInteractiveWidget(plt1.window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-christopher",
   "metadata": {},
   "source": [
    "### Run network on all test data \n",
    "# RUN TESTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range(numToTest):\n",
    "c1 = np.array([x_test[:,:points_per_sample,0], x_test[:,:points_per_sample,1], x_test[:,:points_per_sample,2]])\n",
    "c2 = np.array([x_test[:,points_per_sample:,0], x_test[:,points_per_sample:,1], x_test[:,points_per_sample:,2]])\n",
    "c1 = np.transpose(c1, (1,2,0))\n",
    "c2 = np.transpose(c2, (1,2,0))\n",
    "\n",
    "inputs = x_test\n",
    "# print(\"c1\" , tf.shape(c1))\n",
    "print(\"x_test\" , tf.shape(x_test))\n",
    "print(\"y_test\" , tf.shape(y_test))\n",
    "\n",
    "runlen = 3\n",
    "corr_sum = np.zeros([tf.shape(x_test)[0].numpy(),1,3]) #init var to store correction contributions\n",
    "for i in range(runlen):\n",
    "    correction = model.predict(inputs)[:,None,:] #show what the network thinks\n",
    "#     correction = correction*0.1 #for synthetic matab data only??\n",
    "#     correction = y_test[n] #show actual solution\n",
    "    corr_sum += correction\n",
    "#     print(\"corr_sum\", tf.shape(corr_sum))\n",
    "    c1 += correction\n",
    "#     print(\"after correction\", tf.shape(c1))\n",
    "    inputs = np.append(c1, c2, axis = 1)#.T\n",
    "#     print(\"\\n new inputs\", tf.shape(inputs))\n",
    "    \n",
    "dnn_estimates = corr_sum[:,0,:]\n",
    "print(\"\\n correct soln \\n\", y_test)\n",
    "print(\"\\n estiamted soln: \\n\", dnn_estimates)\n",
    "print(\"\\n error from DNN: \\n\", y_test - dnn_estimates)\n",
    "\n",
    "# print(\"\\n mean raw DNN error: \\n\", np.sqrt(np.sum(np.mean(np.abs(y_test - dnn_estimates), axis = 0)**2)))\n",
    "print(\"\\n mean raw DNN error: \\n\", np.sqrt(np.mean(np.abs(y_test - dnn_estimates), axis = 0)**2))\n",
    "\n",
    "D2D_distance = np.mean(x_test[:,:points_per_sample], axis = 1) - np.mean(x_test[:,points_per_sample:], axis = 1)\n",
    "# print(tf.shape(D2D_distance))\n",
    "# print(tf.shape(y_test))\n",
    "# print(\"\\n mean raw D2D error \\n\", np.sqrt(np.sum(np.mean(np.abs(y_test + D2D_distance), axis = 0)**2 )))\n",
    "print(\"\\n mean raw D2D error \\n\", np.sqrt(np.mean(np.abs(y_test + D2D_distance), axis = 0)**2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c2867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of distribution of errors from DNN (is it even gaussian??)\n",
    "\n",
    "dnn_error = y_test - dnn_estimates\n",
    "D2D_error = y_test + D2D_distance\n",
    "mag_D2D = np.sqrt(D2D_error[:,0]**2 + D2D_error[:,1]**2 + D2D_error[:,2]**2)\n",
    "mag_DNN = np.sqrt(dnn_error[:,0]**2 + dnn_error[:,1]**2 + dnn_error[:,2]**2)\n",
    "\n",
    "#get rid of non-converging DNN solns\n",
    "goodidx = mag_DNN < 1\n",
    "mag_DNN = mag_DNN[goodidx]\n",
    "mag_D2D = mag_D2D[goodidx]\n",
    "print(len(dnn_error) - sum(goodidx), \"of\", len(dnn_error), \"test clouds did not converge\")\n",
    "# print(np.shape(goodidx))\n",
    "\n",
    "num_bins = 100\n",
    "\n",
    "#plot on same axis ---------------------------------------\n",
    "fig, ax = plt.subplots()\n",
    "# ax.set_title(\"x component of registration error\")\n",
    "# R = [-0.5,0.5] #range\n",
    "# ax.hist(dnn_error[:,0], num_bins, R, histtype='step', fill=False, color = (0,0,1), label = 'DNN (ours)')\n",
    "# ax.hist(D2D_error[:,0], num_bins, R, histtype='step', fill=False, color = (1,0,0), label = 'D2D')\n",
    "ax.set_title(\"Registration Error, KITTI data (skip 3)\")\n",
    "# ax.set_title(\"Registration Error, Shadowed ModelNet40 Dataset\")\n",
    "# ax.set_title(\"Registration Error, Uniformly Sampled ModelNet40 Dataset\")\n",
    "R = [0, 0.7]\n",
    "ax.hist(mag_DNN, num_bins, R, histtype='step', fill=False, color = (0,0,1), label = 'DNN (ours)')\n",
    "ax.hist(mag_D2D, num_bins, R, histtype='step', fill=False, color = (1,0,0), label = 'D2D')\n",
    "# ax.set_xlim([0,0.5])\n",
    "ax.set_ylabel('frequency')\n",
    "ax.set_xlabel('magnitude error (m)')\n",
    "ax.legend(loc = 'best')\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# #seprate plots------------------------------------------\n",
    "# fig, ax = plt.subplots(2,1)\n",
    "# ax[0].set_ylabel('frequency')\n",
    "# ax[0].set_xlabel('x component of translation error (m)')\n",
    "# ax[0].set_title('DNN (ours)')\n",
    "# ax[0].hist(dnn_error[:,0], num_bins);\n",
    "# ax[0].set_xlim([-0.5,0.5])\n",
    "\n",
    "# ax[1].set_title('D2D')\n",
    "# ax[1].set_ylabel('frequency')\n",
    "# ax[1].set_xlabel('x component of translation error (m)')\n",
    "# ax[1].hist(D2D_error[:,0], num_bins*3)\n",
    "# ax[1].set_xlim([-0.5,0.5])\n",
    "# #-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d7d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot relationship between error in D2D and error in DNN\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('Magnitude error D2D')\n",
    "# ax.set_xlabel('$\\Delta_{D2D}$') #nope\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylabel('Magnitude error DNN (ours)')\n",
    "ax.set_title(\"KITTI, skip3\")\n",
    "\n",
    "ax.scatter(mag_D2D, mag_DNN, alpha = 1, s = 0.2)\n",
    "\n",
    "c = np.cov(mag_D2D, mag_DNN)\n",
    "\n",
    "print(\"\\n mean magnitude D2D:\", np.mean(mag_D2D))\n",
    "print(\"\\n mean magnitude DNN\", np.mean(mag_DNN))\n",
    "print(\"\\n cov mag D2D:\", np.sqrt(c)[0,0])\n",
    "print(\"\\n cov mag DNN:\", np.sqrt(c)[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstrate using |D2D-DNN| as a monitor statistic\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('Actual D2D Error (unobservable)')\n",
    "ax.set_ylabel('Magnitude Difference Between D2D and DNN')\n",
    "# ax.set_xlabel('$\\Delta_{D2D}$') #nope\n",
    "# ax.set_xlim([0,1])\n",
    "ax.set_title(\"KITTI data\")\n",
    "\n",
    "# print(D2D_distance[:10])\n",
    "# print(dnn_estimates[:10])\n",
    "Diff= dnn_estimates  +  D2D_distance\n",
    "mag_Diff = np.sqrt(Diff[:,0]**2 + Diff[:,1]**2 + Diff[:,2]**2)\n",
    "\n",
    "D2D_error = y_test + D2D_distance\n",
    "mag_D2D = np.sqrt(D2D_error[:,0]**2 + D2D_error[:,1]**2 + D2D_error[:,2]**2)\n",
    "# print(y_test)\n",
    "# print(D2D_distance)\n",
    "# print(D2D_error)\n",
    "\n",
    "ax.scatter(mag_D2D, mag_Diff, alpha = 1, s = 0.2)\n",
    "# ax.plot(np.linspace(0,1,50),np.linspace(0,1,50), color = (1,0,0)) #plot y=x (for debug)\n",
    "\n",
    "\n",
    "c = np.cov(mag_D2D, mag_Diff)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just considering the residuals doesn't tell the whole story \n",
    "#   we need to consider error in COMPACT only directions\n",
    "\n",
    "#use LUT to get compact axis of DNN solution vec for each trial\n",
    "dnn_compact = tf.matmul(LUT, dnn_estimates[:,:,None])\n",
    "dnn_compact_xyz = tf.matmul(U, dnn_compact)\n",
    "truth_compact = tf.matmul(LUT, y_test[:,:,None])\n",
    "truth_compact_xyz = tf.matmul(U, truth_compact)\n",
    "\n",
    "#consider all\n",
    "error_DNN_compact = np.sqrt( np.mean(np.abs(truth_compact_xyz - dnn_compact_xyz), axis = 0)**2 )[:,0]\n",
    "#ignore cases where DNN explodes...\n",
    "# error_DNN_compact = np.sqrt( np.mean(np.abs(truth_compact_xyz[goodidx] - dnn_compact_xyz[goodidx]), axis = 0)**2 )[:,0]\n",
    "print(\"\\n mean compact error DNN: \\n\", error_DNN_compact)\n",
    "DNN_total = np.sqrt(error_DNN_compact[0]**2 + error_DNN_compact[1]**2 + error_DNN_compact[2]**2)\n",
    "print(\"total mag DNN\", DNN_total)\n",
    "\n",
    "# error_D2D_compact = np.sqrt(np.sum( np.mean(np.abs(truth_compact_xyz + d2d_compact_xyz), axis = 0)**2 ))\n",
    "#for distrubution means distance\n",
    "d2d_compact = tf.matmul(LUT, D2D_distance[:,:,None])\n",
    "d2d_compact_xyz = tf.matmul(U, d2d_compact)\n",
    "truth_compact = tf.matmul(LUT, y_test[:,:,None])\n",
    "truth_compact_xyz = tf.matmul(U, truth_compact)\n",
    "error_D2D_compact = np.sqrt(np.mean(np.abs(truth_compact_xyz + d2d_compact_xyz), axis = 0)**2 )[:,0]\n",
    "# error_D2D_compact = np.sqrt(np.mean(np.abs(truth_compact_xyz[goodidx] + d2d_compact_xyz[goodidx]), axis = 0)**2 )[:,0] #ignore DNN exploding\n",
    "print(\"\\n mean compact error D2D: \\n\", error_D2D_compact)\n",
    "D2D_total = np.sqrt(error_D2D_compact[0]**2 + error_D2D_compact[1]**2 + error_D2D_compact[2]**2)\n",
    "print(\"total mag D2D\", D2D_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2c(corn_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 10\n",
    "print(y_train[I])\n",
    "print(ULUT_train[I] @ y_train[I][:,None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceade216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
