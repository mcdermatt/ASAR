{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rental-ordinary",
   "metadata": {},
   "source": [
    "### Monte-Carlo sims for comparing D2D and DNN solution vectors\n",
    "\n",
    "\n",
    "\n",
    "#### TODO:\n",
    "\n",
    "Break test data into \"easy, medium, and hard\" scans with varying degrees of perspective shift\n",
    "\n",
    "Get information on which axis are overly extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bacterial-nerve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import datetime\n",
    "\n",
    "#need to have these two lines to work on my ancient 1060 3gb\n",
    "#  https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "print(tf.__version__) #requires tensorflow 2.3\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "included-thesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5195, 100, 3)\n",
      "(578, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "#Load data directly from numpy files-- KEEP IN RAM (DO NOT CONVERT TO TF Tensor!!!)\n",
    "# #_________________________________________________________________\n",
    "# #load individual data numpy files\n",
    "# d1_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v1_scan1.npy\")\n",
    "# d2_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v1_scan2.npy\")\n",
    "# gt_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v1_ground_truth.npy\")\n",
    "# # d1_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v3_scan1.npy\")\n",
    "# # d2_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v3_scan2.npy\")\n",
    "# # gt_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v3_ground_truth.npy\")\n",
    "\n",
    "# # d1_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/scan1_300k_50_samples.npy\")\n",
    "# # d2_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/scan2_300k_50_samples.npy\")\n",
    "# # gt_1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ground_truth_300k_50_samples.npy\")\n",
    "# # gt_1 = gt_1*0.1 #scale to match real-world data (vel-> pos)\n",
    "\n",
    "# # d1_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_scan1.npy\")\n",
    "# # d2_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_scan2.npy\")\n",
    "# # gt_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_ground_truth.npy\")\n",
    "# d1_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v4_scan1.npy\")\n",
    "# d2_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v4_scan2.npy\")\n",
    "# gt_2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v4_ground_truth.npy\")\n",
    "\n",
    "\n",
    "# d1 = np.append(d1_1, d1_2, axis = 0)\n",
    "# d2 = np.append(d2_1, d2_2, axis = 0)\n",
    "# gt = np.append(gt_1, gt_2, axis = 0)\n",
    "# #_________________________________________________________________\n",
    "\n",
    "#Single datset\n",
    "# d1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_scan1.npy\")\n",
    "# d2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_scan2.npy\")\n",
    "# gt = np.load(\"C:/Users/Derm/Desktop/big/pshift/ICET_Ford_v2_ground_truth.npy\")\n",
    "\n",
    "# d1 = np.load(\"C:/Users/Derm/Desktop/big/pshift/scan1_300k_50_samples.npy\")\n",
    "# d2 = np.load(\"C:/Users/Derm/Desktop/big/pshift/scan2_300k_50_samples.npy\")\n",
    "# gt = np.load(\"C:/Users/Derm/Desktop/big/pshift/ground_truth_300k_50_samples.npy\")\n",
    "# gt = gt*0.1 #scale to match real-world data (vel-> pos)\n",
    "#_________________________________________________________________\n",
    "\n",
    "d1 = np.load(\"training_data/compact_scan1.npy\")\n",
    "d2 = np.load(\"training_data/compact_scan2.npy\")\n",
    "gt = np.load(\"training_data/compact_ground_truth.npy\")\n",
    "LUT = np.load(\"training_data/LUT.npy\")\n",
    "L = np.load(\"training_data/L.npy\")\n",
    "U = np.load(\"training_data/U.npy\")\n",
    "#_________________________________________________________________\n",
    "\n",
    "\n",
    "#reshape but don't convert to tensor\n",
    "points_per_sample = 50          #poitns sammpled from each voxel\n",
    "tsplit = 0.9 #0.95                   #this fraction goes into training\n",
    "\n",
    "scan1 = np.reshape(d1, [-1, points_per_sample, 3])\n",
    "scan2 = np.reshape(d2, [-1, points_per_sample, 3])\n",
    "ntrain = int(tsplit*tf.shape(scan1)[0].numpy())\n",
    "\n",
    "x_train = np.append(scan1[:ntrain], scan2[:ntrain], axis = 1)\n",
    "x_test = np.append(scan1[ntrain:], scan2[ntrain:], axis = 1)\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(x_test))\n",
    "\n",
    "y_train = gt[:ntrain] #for standard training/ test data\n",
    "y_test = gt[ntrain:]\n",
    "# y_train = gt[:ntrain][:,:,0] #when using compact data\n",
    "# y_test = gt[ntrain:][:,:,0]\n",
    "LUT = tf.convert_to_tensor(LUT)[ntrain:]\n",
    "U = tf.convert_to_tensor(U)[ntrain:]\n",
    "L = tf.convert_to_tensor(L)[ntrain:]\n",
    "\n",
    "# print(y_train)\n",
    "# print(np.shape(y_train))\n",
    "# print(np.shape(y_test))\n",
    "\n",
    "#-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "proud-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"FORDNetV3.kmod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "conventional-nashville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([578   3], shape=(2,), dtype=int32)\n",
      "tf.Tensor([578 100   3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([578   3   3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([578   3   3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([578   3   3], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "DNN = model.predict(x_test[:10])\n",
    "# print(DNN)\n",
    "print(tf.shape(y_test))\n",
    "print(tf.shape(x_test))\n",
    "print(tf.shape(LUT))\n",
    "print(tf.shape(U))\n",
    "print(tf.shape(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-tuner",
   "metadata": {},
   "source": [
    "### Iterative solution for single test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "greater-paraguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "\n",
      " correct soln [ 1.087761   -1.2482162  -0.03307071]\n",
      "\n",
      " estiamted soln: [[ 1.08611755 -1.24011327 -0.04212602]]\n",
      "\n",
      " error from DNN: [[ 0.00164349 -0.00810289  0.00905531]]\n",
      "\n",
      " error in means [-0.00739753  0.01978552  0.03824506]\n"
     ]
    }
   ],
   "source": [
    "n = int(np.floor(500*np.random.rand()))\n",
    "# n = 148 #79\n",
    "print(n)\n",
    "\n",
    "c1 = np.array([x_test[n,:points_per_sample,0], x_test[n,:points_per_sample,1], x_test[n,:points_per_sample,2]])\n",
    "c2 = np.array([x_test[n,points_per_sample:,0], x_test[n,points_per_sample:,1], x_test[n,points_per_sample:,2]])\n",
    "\n",
    "inputs = x_test[n][None,:]\n",
    "runlen = 10\n",
    "corr_sum = np.zeros([1,3]) #init var to store correction contributions\n",
    "for i in range(runlen):\n",
    "    correction = model.predict(inputs)[0] #show what the network thinks\n",
    "#     correction = correction*0.1 #for synthetic matab data only??\n",
    "#     correction = y_test[n] #show actual solution\n",
    "    corr_sum += correction\n",
    "    c1 = np.array([c1[0,:] + correction[0], c1[1,:] + correction[1], c1[2,:] + correction[2]])\n",
    "    inputs = np.append(c1, c2, axis = 1).T[None,:,:]\n",
    "\n",
    "print(\"\\n correct soln\", y_test[n])\n",
    "print(\"\\n estiamted soln:\", corr_sum)\n",
    "print(\"\\n error from DNN:\", y_test[n] - corr_sum)\n",
    "mean1 = np.mean(x_test[n,:points_per_sample], axis = 0)\n",
    "mean2 = np.mean(x_test[n,points_per_sample:], axis = 0)\n",
    "print(\"\\n error in means\",  y_test[n] + (mean1 - mean2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "artistic-clerk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1.1184044  -0.02821504  0.00406535], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09b56ae2c9540e09e0982c2e11e8c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=960, layout=Layout(height='auto', width='100%'), width=960)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use Vedo to plot inital and transformed point clouds in 3D \n",
    "from vedo import *\n",
    "from ipyvtklink.viewer import ViewInteractiveWidget\n",
    "\n",
    "plt1 = Plotter(N = 1, axes = 13, bg = (1, 1, 1), interactive = True)\n",
    "disp = []\n",
    "\n",
    "#draw scan1 \n",
    "# disp.append(Points(x_test[n,:points_per_sample].numpy(), c = 'green', r = 5))\n",
    "disp.append(Points(x_test[n,:points_per_sample], c = 'green', r = 5))\n",
    "\n",
    "#draw initial scan2\n",
    "# disp.append(Points(x_test[n,points_per_sample:].numpy(), c = 'red', r = 5))\n",
    "disp.append(Points(x_test[n,points_per_sample:], c = 'red', r = 5))\n",
    "\n",
    "#Draw arrow for ground truth soln vec\n",
    "disp.append(Arrow(mean1, mean1 + y_test[n], c = 'r4', res = 50)) #arbitrarily start arrow from scan1 center\n",
    "\n",
    "#draw ground truth arrow cut short by U and L\n",
    "soln_compact = tf.matmul(LUT[n], y_test[n][:, None])\n",
    "soln_compact_xyz = tf.matmul(U[n], soln_compact)[:,0]\n",
    "print(soln_compact_xyz)\n",
    "disp.append(Arrow(mean1, mean1 + soln_compact_xyz, c = 'p4', res = 50)) #arbitrarily start arrow from scan1 center\n",
    "\n",
    "#draw transformed scan2\n",
    "disp.append(Points(c1, c = 'blue', r = 5))\n",
    "\n",
    "plt1.show(disp, \"Network Performance Test\")\n",
    "ViewInteractiveWidget(plt1.window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-point",
   "metadata": {},
   "source": [
    "### Run network on all test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "altered-perry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test tf.Tensor([578 100   3], shape=(3,), dtype=int32)\n",
      "\n",
      " correct soln \n",
      " [[-0.5912562   0.9231132   0.03430676]\n",
      " [-1.1954186   1.3093736   0.08806767]\n",
      " [ 0.8210644  -2.0195343  -0.0020284 ]\n",
      " ...\n",
      " [-0.58620006  2.0953      0.11858614]\n",
      " [ 0.7578129  -0.5443921  -0.02851354]\n",
      " [-0.9826077  -0.15076858  0.08570372]]\n",
      "\n",
      " estiamted soln: \n",
      " [[-0.60511262  0.94253913  0.02687395]\n",
      " [-1.25293251  1.29066276  0.11121207]\n",
      " [ 0.85414474 -2.02109718 -0.03223034]\n",
      " ...\n",
      " [-0.57487909  2.06275128  0.07774499]\n",
      " [ 0.57990967 -0.67956368 -0.23044539]\n",
      " [-1.04874552 -0.05813899  0.04992488]]\n",
      "\n",
      " error from DNN: \n",
      " [[ 0.01385642 -0.01942591  0.00743281]\n",
      " [ 0.05751391  0.01871086 -0.02314441]\n",
      " [-0.03308033  0.00156283  0.03020194]\n",
      " ...\n",
      " [-0.01132096  0.03254868  0.04084115]\n",
      " [ 0.17790325  0.13517157  0.20193186]\n",
      " [ 0.0661378  -0.09262959  0.03577885]]\n",
      "\n",
      " mean raw DNN error: \n",
      " 0.05303873427534901\n",
      "\n",
      " mean raw D2D error \n",
      " 0.05403466\n"
     ]
    }
   ],
   "source": [
    "# for n in range(numToTest):\n",
    "c1 = np.array([x_test[:,:points_per_sample,0], x_test[:,:points_per_sample,1], x_test[:,:points_per_sample,2]])\n",
    "c2 = np.array([x_test[:,points_per_sample:,0], x_test[:,points_per_sample:,1], x_test[:,points_per_sample:,2]])\n",
    "c1 = np.transpose(c1, (1,2,0))\n",
    "c2 = np.transpose(c2, (1,2,0))\n",
    "\n",
    "inputs = x_test\n",
    "# print(\"c1\" , tf.shape(c1))\n",
    "print(\"x_test\" , tf.shape(x_test))\n",
    "runlen = 10\n",
    "corr_sum = np.zeros([tf.shape(x_test)[0].numpy(),1,3]) #init var to store correction contributions\n",
    "for i in range(runlen):\n",
    "    correction = model.predict(inputs)[:,None,:] #show what the network thinks\n",
    "#     correction = correction*0.1 #for synthetic matab data only??\n",
    "#     correction = y_test[n] #show actual solution\n",
    "    corr_sum += correction\n",
    "#     print(\"corr_sum\", tf.shape(corr_sum))\n",
    "    c1 += correction\n",
    "#     print(\"after correction\", tf.shape(c1))\n",
    "    inputs = np.append(c1, c2, axis = 1)#.T\n",
    "#     print(\"\\n new inputs\", tf.shape(inputs))\n",
    "    \n",
    "dnn_estimates = corr_sum[:,0,:]\n",
    "print(\"\\n correct soln \\n\", y_test)\n",
    "print(\"\\n estiamted soln: \\n\", dnn_estimates)\n",
    "print(\"\\n error from DNN: \\n\", y_test - dnn_estimates)\n",
    "\n",
    "print(\"\\n mean raw DNN error: \\n\", np.sqrt(np.sum(np.mean(np.abs(y_test - dnn_estimates), axis = 0)**2)))\n",
    "\n",
    "D2D_distance = np.mean(x_test[:,:points_per_sample], axis = 1) - np.mean(x_test[:,points_per_sample:], axis = 1)\n",
    "# print(tf.shape(D2D_distance))\n",
    "# print(tf.shape(y_test))\n",
    "print(\"\\n mean raw D2D error \\n\", np.sqrt(np.sum(np.mean(np.abs(y_test + D2D_distance), axis = 0)**2 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "sixth-shopper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " mean compact error DNN: \n",
      " 0.02896793\n",
      "\n",
      " mean compact error D2D: \n",
      " 0.014599225\n"
     ]
    }
   ],
   "source": [
    "#just considering the residuals doesn't tell the whole story \n",
    "#   we need to consider error in COMPACT only directions\n",
    "\n",
    "#use LUT to get compact axis of DNN solution vec for each trial\n",
    "dnn_compact = tf.matmul(LUT, dnn_estimates[:,:,None])\n",
    "dnn_compact_xyz = tf.matmul(U, dnn_compact)\n",
    "\n",
    "#for distrubution means distance\n",
    "d2d_compact = tf.matmul(LUT, D2D_distance[:,:,None])\n",
    "d2d_compact_xyz = tf.matmul(U, d2d_compact)\n",
    "\n",
    "truth_compact = tf.matmul(LUT, y_test[:,:,None])\n",
    "truth_compact_xyz = tf.matmul(U, truth_compact)\n",
    "\n",
    "error_DNN_compact = np.sqrt(np.sum( np.mean(np.abs(truth_compact_xyz - dnn_compact_xyz), axis = 0)**2 ))\n",
    "print(\"\\n mean compact error DNN: \\n\", error_DNN_compact)\n",
    "\n",
    "error_D2D_compact = np.sqrt(np.sum( np.mean(np.abs(truth_compact_xyz + d2d_compact_xyz), axis = 0)**2 ))\n",
    "print(\"\\n mean compact error D2D: \\n\", error_D2D_compact)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-charge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-might",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
